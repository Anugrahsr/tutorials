{
    "docs": [
        {
            "location": "/",
            "text": "Home\n\u00b6\n\n\nWelcome!\n\n\nThis website is a collection of Bioinformatics tutorials that I've accumulated over the years, while teaching a bioinformatics course at the \nSwedish University of Agricultural Sciences\n and during various bioinformatics workshops around the globe.\n\n\nFeel free to follow them online, or to use and modify them for your own teaching.\n\n\nTable of contents\n\u00b6\n\n\n\n\nHome\n\n\nThe command-line\n\n\nFile Formats\n\n\nQuality Control and Trimming\n\n\nMapping and Variant Calling\n\n\nDe-novo Genome Assembly\n\n\nGenome Annotation\n\n\nPan-Genome Analysis\n\n\nMetabarcoding\n\n\nWhole Metagenome Sequencing\n\n\nMetagenome assembly\n\n\nRNA-Seq\n\n\nIntroduction to Nanopore Sequencing\n\n\n\n\nContributing\n\u00b6\n\n\nA typo? Something that irks you? Submit an \nissue\n\nor a pull request.\n\n\nLicense\n\u00b6\n\n\nThis work is licensed under the Creative Commons Attribution 4.0 International License.\nTo view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/ or send a letter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.",
            "title": "Home"
        },
        {
            "location": "/#home",
            "text": "Welcome!  This website is a collection of Bioinformatics tutorials that I've accumulated over the years, while teaching a bioinformatics course at the  Swedish University of Agricultural Sciences  and during various bioinformatics workshops around the globe.  Feel free to follow them online, or to use and modify them for your own teaching.",
            "title": "Home"
        },
        {
            "location": "/#table-of-contents",
            "text": "Home  The command-line  File Formats  Quality Control and Trimming  Mapping and Variant Calling  De-novo Genome Assembly  Genome Annotation  Pan-Genome Analysis  Metabarcoding  Whole Metagenome Sequencing  Metagenome assembly  RNA-Seq  Introduction to Nanopore Sequencing",
            "title": "Table of contents"
        },
        {
            "location": "/#contributing",
            "text": "A typo? Something that irks you? Submit an  issue \nor a pull request.",
            "title": "Contributing"
        },
        {
            "location": "/#license",
            "text": "This work is licensed under the Creative Commons Attribution 4.0 International License.\nTo view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/ or send a letter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.",
            "title": "License"
        },
        {
            "location": "/command_line/",
            "text": "The command-line\n\u00b6\n\n\nThis tutorial is largely inspired of the \nIntroduction to UNIX\n course from the Sanger Institute.\n\n\nThe aim of this module is to introduce Unix and cover some of the basics that will allow you to be more comfortable with the command-line. Several of the programs that you are going to use during this course are useful for bioinformatics analyses. This module is only designed to provide a very brief introduction to some of the features and useful commands of Unix. During this module we will also obtain a genome sequence and examine the basic structure of an EMBL entry.\n\n\nIntroduction\n\u00b6\n\n\nUnix is the standard operating system on most large computer systems in scientific research, in the same way that Microsoft Windows is the dominant operating system on desktop PCs. Unix and MS Windows both perform the important job of managing the computer\u2019s hardware (screen, keyboard, mouse, hard disks, network connections, etc...) on your behalf. They also provide you with tools to manage your files and to run application software. They both offer a graphical user interface (desktop). The desktops look different, call things by different names but they mostly can do the same things. Unix is a powerful, secure, robust and stable operating system that allows dozens of people to run programs on the same computer at the same time. This is why it is the preferred operating system for large-scale scientific computing. It is run on all kind of machines, like mobile phones (Android), desktop PCs, kitchen appliances,... all the way up to supercomputers. Unix powers the majority of the Internet.\n\n\nAims\n\u00b6\n\n\nThe aim of this course is to introduce Unix and cover the basics. The programs that you are going to use during the courses, plus many others that are useful for bioinformatics analyses, are run in Unix. This module is only designed to provide a very brief introduction to some of the features and useful commands of Unix. During this module we will also obtain a genome sequence and examine the basic structure of an EMBL entry.\n\n\nWhy use Unix?\n\u00b6\n\n\n\n\nUnix is a well established, very widespread operating system. You probably have a device running on Unix in your home without realising it (e.g. playstation, TV box, wireless router, android tablets/phones,...\n\n\nCommand line driven, with a huge number of often terse, but powerful commands.\n\n\nIn contrast to Windows, it is designed to allow many users to run their programs simultaneously on the same computer.\n\n\nDesigned to work in computer networks - for example, most of the Internet is Unix based.\n\n\nIt is used on many of the powerful computers at bioinformatics centres and also on many desktops and laptops (MacOS is largely UNIX compatible).\n\n\nThe major difference between Unix and Windows is that it is free (as in freedom) and you can modify it to work however you want. This same principle of freedom is also used in most bioinformatics software.\n\n\nThere are many distributions of Unix such as Ubuntu, RedHat, Fedora, Mint,...). These are all Unix, but they bundle up extra software in a different way or combinations. Some are known for being conservative and reliable; whilst others are know for being cutting-edge (and less reliable).\n\n\nThe MacOSX operating system used by the \neBioKit\n is also based on Unix.\n\n\n\n\nGetting started\n\u00b6\n\n\nFor this course, you will have to connect to the eBiokit using SSH. SSH stands for Secure Shell and is a network protocol used to securely connect to a server. To do so, you will need an SSH client:\n\n\n\n\nOn Linux: it is included by default, named Terminal.\n\n\nOn MacOS: it is included by default, also named Terminal.\n\n\nOn Windows: you'll have to download and install \nMobaXterm\n, a terminal emulator.\n\n\n\n\nOnce you've opened your terminal (or terminal emulator), type\n\n\nssh username@ip_address\n\n\nreplacing \nusername\n and \nip_address\n with your username and the ip address of the server you are connecting to.\nType your password when prompted. As you type, nothing will show on screen. No stars, no dots.\nIt is supposed to be that way. Just type the password and press enter!\n\n\n\n\nYou can type commands directly into the terminal at the \u2018$' prompt. A list of useful commands can be found on the next page. Many of them are two- or three-letter abbreviations. The earliest Unix systems (\ncirca\n 1970) only had slow Teletype terminals, so it was faster to type 'rm' to remove a file than 'delete' or 'erase'. This terseness is a feature of Unix that still survives.\n\n\nThe command line\n\u00b6\n\n\nAll Unix programs may be run by typing commands at the Unix prompt. The command line tells the computer what to do. You may subtly alter these commands by specifying certain options when typing in the command line.\n\n\nCommand line Arguments\n\u00b6\n\n\nTyping any Unix command for example \nls\n, \nmv\n or \ncd\n at the Unix prompt with the appropriate variables such as files names or directories will result in the tasks being performed on pressing the enter key.\n\n\n\n\nThe command is separated from the options and arguments by a space. Additional options and/or arguments can be added to the commands to affect the way the command works. Options usually have one dash and a letter (e.g. -h) or two dashes and a word (--help) with no space between the dash and the letter/word. Arguments are usually filenames or directories.\n\n\nFor example: List the contents of a directory\n\n\nls\n List the contents of a directoryList the contents of a directory with extra information about the files\n\nls \u2013l\n List the contents of a directory with extra information about the files\n\nls \u2013a\n List all contents including hidden files & directories\n\nls -al\n List all contents including hidden files & directories, with extra information about the files\n\nls \u2013l /usr/\n List the contents of the directory /usr/, with extra information about the files\n\n\nFiles and Directories\n\u00b6\n\n\nDirectories are the Unix equivalent of folders on a PC or Mac. They are organised in a hierarchy, so directories can have sub-directories. Directories are very useful for organising your work and keeping your account tidy - for example, if you have more than one project, you can organise the files for each project into different directories to keep them separate. You can think of directories as rooms in a house. You can only be in one room (directory) at a time. When you are in a room you can see everything in that room easily. To see things in other rooms, you have to go to the appropriate door and crane your head around. Unix works in a similar manner, moving from directory to directory to access files. The location or directory that you are in is referred to as the current working directory.\n\n\nDirectory structure example\n\n\n\n\nTherefore if there is a file called genome.seq in the \ndna\n directory its location or full pathname can be expressed as /nfs/dna/genome.seq.\n\n\nGeneral Points\n\u00b6\n\n\nUnix is pretty straightforward, but there are some general points to remember that will make your life easier: most flavors of UNIX are case sensitive - typing \nls\n is generally not the same as typing \nLS\n. You need to put a space between a command and its argument - for example, \nless my_file\n will show you the contents of the file called my_file; \nlessmyfile\n will just give you an error! Unix is not psychic: If you misspell the name of a command or the name of a file, it will not understand you. Many of the commands are only a few letters long; this can be confusing until you start to think logically about why those letters were chosen - ls for list, rm for remove and so on. Often when you have problems with Unix, it is due to a spelling mistake, or perhaps you have omitted a space. If you want to know more about Unix and its commands there are plenty of resources available that provide a more comprehensive guide (including a cheat sheet at the end of this chapter.\n\n\n\n\nhttp://unix.t-a-y-l-o-r.com/\n\n\n\n\nIn what follows, we shall use the following typographical conventions: Characters written in \nbold typewriter font\n are commands to be typed into the computer as they stand. Characters written in \nitalic typewriter font\n indicate non-specific file or directory names. Words inserted within square brackets [Ctrl] indicate keys to be pressed. So, for example,\n\n$ **ls** *any_directory* [Enter]\n means \"at the Unix prompt $, type ls followed by the name of some directory, then press Enter\"\nDon't forget to press the [Enter] key: commands are not sent to the computer until this is done.\n\n\nSome useful Unix commands Command\u00a0and What it does\n\u00b6\n\n\n\n\n\n\n\n\nCommand\n\n\nWhat it does\n\n\n\n\n\n\n\n\n\n\nls\n\n\nLists the contents of the current directory\n\n\n\n\n\n\nmkdir\n\n\nCreates a new directory\n\n\n\n\n\n\nmv\n\n\nMoves or renames a file\n\n\n\n\n\n\ncp\n\n\nCopies a file\n\n\n\n\n\n\nrm\n\n\nRemoves a file\n\n\n\n\n\n\ncat\n\n\nConcatenates files\n\n\n\n\n\n\nless\n\n\nDisplays the contents of a file one page at a time\n\n\n\n\n\n\nhead\n\n\nDisplays the first ten lines of a file\n\n\n\n\n\n\ntail\n\n\nDisplays the last ten lines of a file\n\n\n\n\n\n\ncd\n\n\nChanges current working directory\n\n\n\n\n\n\npwd\n\n\nPrints working directory\n\n\n\n\n\n\nfind\n\n\nFinds files matching an expression\n\n\n\n\n\n\ngrep\n\n\nSearches a file for patterns\n\n\n\n\n\n\nwc\n\n\nCounts the lines, words, characters, and bytes in a file\n\n\n\n\n\n\nkill\n\n\nStops a process\n\n\n\n\n\n\njobs\n\n\nLists the processes that are running\n\n\n\n\n\n\n\n\nFirts steps\n\u00b6\n\n\nThe following exercise introduces a few useful Unix commands and provides examples of how they can be used. Many people panic when they are confronted with an Unix prompt! Don\u2019t! The exercise is designed to be step-by-step, so all the commands you need are provided in the text. If you get lost ask a demonstrator. If you are a person skilled at Unix, be patient it is only a short exercise. Finding where you are and what you\u2019ve got\n\n\npwd\n\u00a0\u00a0\u00a0\u00a0\u00a0\nPrint the working directory As seen previously directories are arranged in a hierarchical structure. To determine where you are in the hierarchy you can use the pwd command to display the name of the current working directory. The current working directory may be thought of as the directory you are in, i.e. your current position in the file-system tree To find out where you are type\n\n\npwd [enter]\n\n\nYou will see that you are in your home directory. We need to move into the ngs_course_data directory. Remember, Unix is case sensitive \nPWD\n is not the same as \npwd\n\n\n\n  \n\n\n\n\n\ncd\n\u00a0\nChange current working directory The cd command will change the current working directory to another, in other words allow you to move up or down in the directory hierarchy. First of all we are going to move into the \"ngs_course_data\" directory below. To do this type:\n\n\ncd ngs_course_data [enter]\n\n\nNow use the pwd command to check your location in the directory hierarchy.\n\n\nChange again the directory to \nModule_Unix\n  \n\n\nls\n\nList the contents of a directory To find out what are the contents of the current directory type \nls\n [enter] The ls command lists the contents of your current directory, this includes files and directories You should see that there are several other directories.\n\n\nNow use the \ncd\n command again to change to the \nModule_Unix\n directory.\n\n\n\n  \n\n\n\n\n\nChanging and moving what you\u2019ve got\n\u00b6\n\n\ncp\n\nCopy a file. \ncp file1 file2\n is the command which makes a copy of file1 in the current working directory and calls it file2! What you are going to do is make a copy of AL513382.embl. This file contains the genome of Salmonella typhi strain CT18 in EMBL format (we'll learn more about file formats later during the course). The new file will be called S_typhi.embl.\n\n\ncp AL513382.embl S_typhi.embl [enter]\n\nIf you use the ls command to check the contents of the current directory you will see that there is an extra file called S_typhi.embl.\n\n\n\n  \u00a0\n\n\n\n\n\nrm\n\nDelete a file. This command removes a file permanently, so be careful! You are now going to remove the old version of the \nS. typhi\n genome file, AL513382.embl\n\nrm AL513382.embl [enter]\n\n\nThe file will be removed. Use the \nls\n command to check the contents of the current directory to see that AL513382.embl has been removed.\n\n\nUnix, as a general rule does exactly what you ask, and does not ask for confirmation. Unfortunately there is no \"recycle bin\" on the command line to recover the file from, so you have to be careful.\n\n\n\n  \u00a0\n\n\n\n\n\ncd\n\nChange current working directory. As before the cd command will change the current working directory to another, in other words allow you to move up or down in the directory hierarchy. First of all we are going to move into the directory above, type:\n\ncd .. [enter]\n\n\nNow use the \npwd\n command to check your location in the directory hierarchy. Next, we are going to move into the Module_Artemis directory. To change to the Module_Artemis directory type:\n\ncd Module_Artemis [enter]\n use the ls command to check the contents of the directory.\n\n\n\n  \n\n\n\n\n\nTips\n\u00b6\n\n\nThere are some short cuts for referring to directories:\n\n\n. \u00a0Current directory (one full stop)  \n.. Directory above (two full stops)  \n~ \u00a0Home directory (tilde)  \n/ \u00a0Root of the file system (like C:\\ in Windows)\n\n\n\n\n\nPressing the tab key twice will try and autocomplete what you\u2019ve started typing or give you a list of all possible completions. This saves a lot of typing and typos. Pressing the up/down arrows will let you scroll through the previous commands. If you highlight some text, middle clicking will paste it on the command line.\n\n\nmv\n\nMove a file. To move a file from one place to another use the \nmv\n command. This moves the file rather than copies it, therefore you end up with only one file rather than two. When using the command the path or pathname is used to tell Unix where to find the file. You refer to files in other directories by using the list of hierarchical names separated by slashes. For example, the file bases in the directory genome has the path genome/bases If no path is specified Unix assumes that the file is in the current working directory. What you are going to do is move the file S_typhi.embl from the Module_Unix directory, to the current working directory.\n\nmv ../Module_Unix/S_typhi.embl . [enter]\n\nUse the ls command to check the contents of the current directory to see that S_typhi.embl has been moved. ../Module_Unix/S_typhi.embl specifies that S_typhi.embl is in the Module_Unix directory. If the file was in the directory above, the path would change to: ../ S_typhi.embl\n\n\n\n  \n\n\n\n\n\nThe command can also be used to rename a file in the current working directory. Previously we used the cp command, but mv provides an alternative without the need to delete the original file. Therefore we could have used: \u00a0\n\n\nmv AL513382.embl S_typhi.embl [enter]\n instead of:\n\n\ncp AL513382.embl S_typhi.embl [enter]\nrm AL513382.embl [enter]\n\n\n\n\n\nViewing what you\u2019ve got\n\u00b6\n\n\nless\n\nDisplay file contents. This command displays the contents of a specified file one screen at a time. You are now going to look at the contents of S_typhi.embl.\n\nless S_typhi.embl [enter]\n\nThe contents of S_typhi.embl will be displayed one screen at a time, to view the next screen press the space bar. less can also scroll backwards if you hit the b key. Another useful feature is the slash key, /, to search for a word in the file. You type the word you are looking for and press enter. The screen will jump to the next occurrence and highlight it. As S_typhi.embl is a large file this will take a while, therefore you may want to escape or exit from this command. To exit press the letter \u2018q\u2019. If you really need to exit from a program and it isn\u2019t responding press \u2018control\u2019 and the letter \u2018c\u2019 at the same time.\n\n\n\n  \u00a0\n\n\n\n\n\nhead\n \n\nDisplay the first ten lines of a file\n\n\ntail\n\u00a0\n\nDisplay the last ten lines of a file  \n\n\nSometimes you may just want to view the text at the beginning or the end of a file, without having to display all of the file. The head and tail commands can be used to do this. You are now going to look at the beginning of S_typhi.embl.\n\n\nhead S_typhi.embl [enter]\n\n\nTo look at the end of S_typhi.embl type: \ntail S_typhi.embl [enter]\n\nThe number of lines that are displayed can be increased by adding extra arguments. To increase the number of lines viewed from 10 to 100 add the \u2013100 argument to the command. For example to view the last 100 lines of S_typhi.embl type:\n\n\ntail -100 S_typhi.embl [enter]\n\n\nDo this for both head and tail commands. What type of information is at the beginning and end of the EMBL format file?\n\n\n\n  \u00a0\n\n\n\n\n\ncat\n\nJoin files together. Having looked at the beginning and end of the S_typhi.embl file you should notice that in EMBL format files the annotation comes first, then the DNA sequence at the end. If you had two separate files containing the annotation and the DNA sequence, both in EMBL format, it is possible to concatenate or join the two together to make a single file like the S_typhi.embl file you have just looked at. The Unix command cat can be used to join two or more files into a single file. The order in which the files are joined is determined by the order in which they appear in the command line. For example, we have two separate files, MAL13P1.dna and MAL13P1.tab, that contain the DNA and annotation, respectively, from the \nP. falciparum\n genome. Return to the Module_Unix directory using the cd command:\n\ncd ../Module_Unix [enter]\n\n\nand type\n\ncat MAL13P1.tab MAL13P1.dna > MAL13P1.embl [enter]\n\nMAL13P1.tab and MAL13P1.dna will be joined together and written to a file called MAL13P1.embl\nThe \n>\n symbol in the command line directs the output of the cat program to the designated file MAL13P1.embl\n\n\n\n  \u00a0\n\n\n\n\n\nwc\n\nCounts the lines, words or characters of files.\nBy typing the command line:\n\nls | wc -l [enter]\n\n\nThe above command uses wc to count the number of files that are listed by ls. The \u2018-l\u2019 option tells wc to return a count of the number of lines. The | symbol (known as the \u2018pipe\u2019 character) in the command line connects the two commands into a single operation for simplicity. You can connect as many commands as you want:\n\n\nls | grep \".embl\" | wc -l\n\nThis command will list out all of the files in the current directory, then send the results to the grep command which searches for all filenames containing the \u2018embl\u2019, then sends the results to wc which counts the number of lines (which corresponds to the number of files).\n\n\n\n  \n\n\n\n\n\ngrep\n\nSearches a file for patterns. \ngrep\n is a powerful tool to search for patterns in a file. In the examples below, we are going to use the file called Malaria.fasta that contains the set of \nP. falciparum\n chromosomes in FASTA format. A FASTA file has the following format:\n\n\n\n\n\n\nSequence Header\nCTAAACCTAAACCTAAACCCTGAACCCTAA...\n\n\n\n\n\n\nTherefore if we want to get the sequence headers, we can extract the lines that match the \u2018>\u2019 symbol:\n\n\ngrep \u2018>\u2019 Malaria.fasta [enter]\n\n\nBy typing the command line:\n\n\ngrep -B 1 -A 1 'aagtagggttca' Malaria.fasta [enter]\n\n\nThis command will search for a nucliotide sequence and print 1 line before and after any match. It won\u2019t find the pattern if it spans more than 1 line.\n\n\n\n  \n\n\n\n\n\nfind\n\nFinds files matching an expression. The find command is similar to ls but in many ways it is more powerful. It can be used to recursively search the directory tree for a specified path name, seeking files that match a given Boolean expression (a test which returns true or false)\n\n\nfind . -name \u201c*.embl\u201d\n This command will return the files which name has the .embl suffix.\n\n\nmkdir test_directory\n\n\nfind . -type d\n\n\nThis command will return all the subdirectories contained in the current directory. These are just two basic examples but it is possible to search in many other ways: \n-mtime\n search files by modifying date \n-atime\n search files by last access date \n-size\n search files by file size \n-user\n search files by user they belong to.\n\n\nTips\n\u00b6\n\n\nYou need to be careful with quoting when using wildcards!\n\n\nThe wildcard * symbol represents a string of any character and of any length.\n\n\n\n  \n\n\nFor more information on Unix command see EMBNet UNIX Quick Guide. \u00a0\n\n\n\n  End of the module\n\n\n# Introduction to Unix (continued)\n\nIn this part of the Unix tutorial, you will learn to download files, compress and decompress them, and combine commands.\n\n## Download files\n\n`wget` can be used to download files from the internet and store them.\n\n`wget https://raw.githubusercontent.com/HadrienG/tutorials/master/LICENSE`\n\nwill download the file that is located at the above URL on the internet, and put it **in the current directory**. This is the license under which this course is released. Open it and read it if you like!\n\nThe `-O` option can be used to change the output file name.\n\n`wget -O GNU_FDL.txt https://raw.githubusercontent.com/HadrienG/tutorials/master/LICENSE`\n\nYou can also use wget to download a file list using -i option and giving a text file containing file URLs. The following\n\n\n\u0002wzxhzdk:2\u0003\n\n\n`wget -i download-file-list.txt`\n\n## Compressing and decompressing files\n\n### Compressing files with gzip\n\ngzip is a utility for compressing and decompressing individual files. To compress files, use:\n\n`gzip filename`\n\nThe filename will be deleted and replaced by a compressed file called filename.gz. To reverse the compression process, use:\n\n`gzip -d filename.gz`\n\nTry it on the License you just downloaded!\n\n### Tar archives\n\nQuite often, you don't want to compress just one file, but rather a bunch of them, or a directory.\n\ntar backs up entire directories and files as an archive. An archive is a file that contains other files plus information about them, such as their filename, owner, timestamps, and access permissions. tar does not perform any compression by default.\n\nTo create a gzipped disk file tar archive, use\n\n`tar -czvf archivename filenames`\n\nwhere archivename will usually have a .tar.gz extension\n\nThe c option means create, the v option means verbose (output filenames as they are archived), option f means file, and z means that the tar archive should be gzip compressed.\n\nTo list the contents of a gzipped tar archive, use\n\n`tar -tzvf archivename`\n\nTo unpack files from a tar archive, use\n\n`tar -xzvf archivename`\n\nTry to archive the folder `Module_Unix` from the previous exercise!\n\nYou will notice a file called tutorials.tar.bz2 in your home directory. This is also a compressed archive, but compressed in the bzip format. Read the tar manual and find a way to decompress it.\n\nHint: you can read the manual for any command using `man`\n\n`man tar`\n\n### Redirection\n\nSome commands give you an output to your screen, but you would have preferred it to go into another program or into a file. For those cases you have some redirection characters.\n\n#### Output redirection\n\nThe output from a command normally intended for standard output (that is, your screen) can be easily diverted to a file instead. This capability is known as output redirection:\n\nIf the notation `> file` is appended to any command that normally writes its output to standard output, the output of that command will be written to file instead of your terminal.\n\nI.e, the following who command:\n\n`who > users.txt`\n\nNo output appears at the terminal. This is because the output has been redirected into the specified file.\n\n`less users.txt`\n\nBe careful, if a command has its output redirected to a file and the file already contains some data, that data will be lost. Consider this example:\n\n`echo Hello > users.txt`\n\n`less users.txt`\n\nYou can use the `>>` operator to append the output in an existing file as follows:\n\n\n\u0002wzxhzdk:3\u0003\n\n\n`less users.txt`\n\n#### Piping\n\nYou can connect two commands together so that the output from one program becomes the input of the next program. Two or more commands connected in this way form a pipe.\n\nTo make a pipe, put a vertical bar `|` on the command line between two commands.\n\nRemember the command `grep`? We can pipe other commands to it, to refine searches per example:\n\n`ls -l ngs_course_data | grep \"Jan\"`\n\nwill only give you the files and directories created in January.\n\nTip: There are various options you can use with the grep command, look at the manual!\n\nPipes are extremely useful to connect various bioinformatics software together. We'll use them extensively later.\n# Introduction to Unix (continued)\n\nIn this part of the tutorial, we'll learn how to install programs in a Unix system\n\n## Using a package manager\n\nThis is the most straight-forward way, and the way used by most of the people using unix at home,\nor administrating their own machine.\n\nThis course is aimed at giving you a working knowledge of linux for bioinformatics, and in that setting, you will rarely, if ever, be the administrator of your own machine. The methods below are here as an information\n\n### On Ubuntu and Debian: Apt\n\nTo install a software:\n\n`apt-get install name_of_the_software`\n\nto uninstall:\n\n`apt-get remove name_of_the_software`\n\nto update all installed softwares:\n\n\n\u0002wzxhzdk:4\u0003\n\n\n### On Fedora, CentOS and RedHat: yum\n\nTo install a software:\n\n`yum install name_of_the_software`\n\nto uninstall:\n\n`yum remove name_of_the_software`\n\nto update:\n\n`yum update`\n\n### MacOS: brew\n\nAlthough there are no official package managers on MacOS, two popular, community-driven alternatives exist: macports and brew.\n\nBrew is particularly pupular within the bioinformatics community, and allows easy installation of many bioinformatics softwares on MacOS\n\nTo install brew on your mac:\n\n`/usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"`\n\nTo install a software:\n\n`brew install name_of_the_software`\n\nTo uninstall:\n\n`brew uninstall name_of_the_software`\n\nTo update all brew-installed softwares:\n\n\n\u0002wzxhzdk:5\u0003\n\n\nMore info on [brew.sh](http://brew.sh) and [brew.sh/homebrew-science/](http://brew.sh/homebrew-science/)\n\n## Downloading binaries\n\nIn a university setting, you will rarely by administrator of your own machine. This is a very good thing for one reason: it's harder for you to break something!\n\nThe downside is that it makes installing softwares more complicated. We'll start wit simply downloading the software and executing it, then we'll learn how to obtain packages from source code.\n\nfor example, we'll install the blast binaries:\n\nFirst, download the archive:\n`wget ftp://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/ncbi-blast-2.6.0+-x64-linux.tar.gz`\n\nthen unpack it and go to the newly created directory\n\n\n\u0002wzxhzdk:6\u0003\n\n\nyou should have a `bin` directory, go inside and look at the files. You have a bunch of executable files.\n\n### Execute a file\n\n\nMost of the lunix commands that you execute on a regular basis (ls, cp, mkdir) are located in `/usr/bin`, but you don't have to invoke them with their full path: i.e. you dont type `/usr/bin/ls` but just `ls`. This is because `/usr/bin/ls` is in your $PATH.\n\nto execute a file that you just downloaded, and is therefore not in your path, you have to type the absolute or relative path to that file. Meaning, for the blast program suite that we just downloaded:\n\n`bin/blastn -help`\n\nor\n\n\n\u0002wzxhzdk:7\u0003\n\n\nand that's it!\nBut it is not very convenient. You want to be able to execute blast without having to remember where it is. If you have administrator rights (sudo), you can move the software in `/usr/bin`. If you don't you can modify your $PATH in a configuration file called `.bash_profile` that is located in your home.\n\nMore information on how to correctly modify your PATH [here](http://unix.stackexchange.com/a/26059)\n\n## Compiling from source\n\nSometimes pre-compiled binaries are not available. You then have to compile from source: transforming the human-readable code (written in one or another programming language) into machine-readable code (binary)\n\nThe most common way to do so, if a software package has its source coud available online is\n\n\n\u0002wzxhzdk:8\u0003\n\n\nIf you don't have the administrator rights, you'll often have to pass an extra argument to ./configure:\n\n\n\u0002wzxhzdk:9\u0003\n\n\nMost of the softwares come with instructions on how to install them. Always read the file called README or INSTALL in the package directory before installing!\n\n### Exercice\n\nThe most popular unix distributions come with a version of python (a programming language) that is not the most recent one. Install from source the most recent version of python in a folder called `bin` in your home directory.\n\nYou can download the python source code at https://www.python.org/ftp/python/3.6.0/Python-3.6.0.tgz\n\n## Install python packages\n\nPython is a really popular programming language in the world of bioinformatics. Python has a package manager called `pip` that you can use to install softwares written in python.\n\nPlease us the python executable you installed in the above exercise!\n\nFirstly, get pip:\n\n`wget https://bootstrap.pypa.io/get-pip.py`\n\nthen execute the script\n\n`python get-pip.py`\n\nThenm you can use pip to install package, either globally (if you're an administrator):\n\n`pip install youtube_dl`\n\nor just for you:\n\n`pip install --user youtube_dl`\n\n## Final exercise\n\nOne of the oldest and most famous bioinformatics package is called EMBOSS.\nInstall EMBOSS in the bin directory of your home. Good luck!\n# Introduction to UNIX (continued)\n\nIn the 4th and last module of your unix course, we'll how to write small programs, or scripts.\n\nShell scripts allow us to program commands in chains and have the system execute them as a scripted chain of events. They also allow for far more useful functions, such as command substitution. You can invoke a command, like date, and use it\u2019s output as part of a file-naming scheme. You can automate backups and each copied file can have the current date appended to the end of its name. You can automate a bioinformatics analysis pipeline.\n\nBefore we begin our scripting tutorial, let\u2019s cover some basic information. We\u2019ll be using the bash shell, which most Linux distributions use natively. Bash is available for Mac OS users and Cygwin on Windows (which you are using with MobaXterm). Since it\u2019s so universal, you should be able to script regardless of your platform.\n\nAt their core, scripts are just plain text files. You can use nano (or any other text editor) to write them.\n\n## Permissions\n\nScripts are executed like programs. For this to happen, you need to have the proper permissions.\nYou can make the script executable for you by running the following command:\n\n`chmod u+x my_script.sh`\n\nby convention, bash script are saved with the .sh extension. Linux doesn't really care about file extension, but it is easier for the user to use the \"proper\" extensions!\n\n## executing a script\n\nYou have to cd in the proper directory, then run the script like this:\n\n`./my_script.sh`\n\nTo make things more convenient, you can place scripts in a \u201cbin\u201d folder in your home directory and add it to your path\n\n`mkdir -p ~/bin`\n\nMore information on how to correctly modify your PATH [here](http://unix.stackexchange.com/a/26059)\n\n## Getting started\n\nAs previously said, every script is a text file. Still, there are rules and conventions to follow in order of you file being recognized as a script\n\nIf you juste write a few command and try to execute it as is, with `./my_script`, it will not work. You can invoke `sh my_script`, but it is not very convenient.\n`./` tries to find out which interpreter to use (e.g. which programming language and how to execute your script). It does so by looking at the first line:\n\nThe first line of your bash scripts should be:\n\n`#!/bin/bash` or `#!/usr/bin/env bash`\n\nThe second version being better and more portable. Ask your teacher why!\n\nThis line will have the same syntax for every interpreted language. If you are programming in python:\n\n`#!/usr/bin/env python`\n\n### New line = new command\n\nAfter the firstline, every line of your script will be a new command. Your first scripts will essentially be a succession of terminal commands. We'll learn about flow control (if, for, while, ...) later on.\n\n### Comments\n\nIt is good practise to comment your scripts, i.e give some explanation of what is does, and explain a particularly arcane method that you wrote.\n\nComments start with a `#` and are snippets of texts that are ignored by the interpreter.\n\n### Your first script\n\nLet's start with a simple script, that copy files and append today's date to the end of the file name. We'll call it `datecp.sh`\n\nIn your `~/bin` folder:\n\n\n\u0002wzxhzdk:10\u0003\n\n\nand let's start writing our script\n\n`nano datecp.sh`\n\n\n\u0002wzxhzdk:11\u0003\n\n\nNext, we need to declare a variable. A variable allows us to store and reuse information (characters, the date or the command `date`). Variables have a name, but can **expend** to their content when referenced if they contain a command.\n\nVariables can hold strings and characers, like this:\n\n`my_variable=\"hippopotamus\"`\n\nor a command. In bash, the correct way to store a command in a variable is within the syntax `$()`:\n\n`variable=$(command \u2013options arguments)`\n\nStore the date and time in a variable. Test the date command first in your terminal, then when you got the right format, store it in a variable in your script.\n\nIt is generally bad practice to put spaces in file names in unix, so we'll want the following date format:\n\n`date +%m_%d_%y-%H.%M.%S`\n\nand for putting it into a variable:\n\ndate_formatted=$(date +%m_%d_%y-%H.%M.%S)\n\nYour script now can print thedate without too much more coding:\n\n\n\u0002wzxhzdk:12\u0003\n\n\nNow we need to add the copying part:\n\n`cp \u2013iv $1 $2.$date_formatted`\n\nThis will invoke the copy command, with two options: -i for asking for permission before overwriting a file, and -v for verbose.\n\nYou can also notice two variables: $1 and $2. When scripting in bash, a dollar sign ($) followed by a number will denote an argument of the script. For example in the following command:\n\n`cp \u2013iv a_file a_file_copy` the first argument ($1) is `a_file` and the second argument ($2) is `a_file_copy`\n\nWhat our script will do is a simple copy of a file, but with adding the date to the end of the file name. Save it and try it out!\n\n### Exercise\n\nWrite a script that backs itself up, that is, copies itself to a file named backup.sh.\n\nHint: Use the cat command and the appropriate positional parameter.",
            "title": "The command-line"
        },
        {
            "location": "/command_line/#the-command-line",
            "text": "This tutorial is largely inspired of the  Introduction to UNIX  course from the Sanger Institute.  The aim of this module is to introduce Unix and cover some of the basics that will allow you to be more comfortable with the command-line. Several of the programs that you are going to use during this course are useful for bioinformatics analyses. This module is only designed to provide a very brief introduction to some of the features and useful commands of Unix. During this module we will also obtain a genome sequence and examine the basic structure of an EMBL entry.",
            "title": "The command-line"
        },
        {
            "location": "/command_line/#introduction",
            "text": "Unix is the standard operating system on most large computer systems in scientific research, in the same way that Microsoft Windows is the dominant operating system on desktop PCs. Unix and MS Windows both perform the important job of managing the computer\u2019s hardware (screen, keyboard, mouse, hard disks, network connections, etc...) on your behalf. They also provide you with tools to manage your files and to run application software. They both offer a graphical user interface (desktop). The desktops look different, call things by different names but they mostly can do the same things. Unix is a powerful, secure, robust and stable operating system that allows dozens of people to run programs on the same computer at the same time. This is why it is the preferred operating system for large-scale scientific computing. It is run on all kind of machines, like mobile phones (Android), desktop PCs, kitchen appliances,... all the way up to supercomputers. Unix powers the majority of the Internet.",
            "title": "Introduction"
        },
        {
            "location": "/command_line/#aims",
            "text": "The aim of this course is to introduce Unix and cover the basics. The programs that you are going to use during the courses, plus many others that are useful for bioinformatics analyses, are run in Unix. This module is only designed to provide a very brief introduction to some of the features and useful commands of Unix. During this module we will also obtain a genome sequence and examine the basic structure of an EMBL entry.",
            "title": "Aims"
        },
        {
            "location": "/command_line/#why-use-unix",
            "text": "Unix is a well established, very widespread operating system. You probably have a device running on Unix in your home without realising it (e.g. playstation, TV box, wireless router, android tablets/phones,...  Command line driven, with a huge number of often terse, but powerful commands.  In contrast to Windows, it is designed to allow many users to run their programs simultaneously on the same computer.  Designed to work in computer networks - for example, most of the Internet is Unix based.  It is used on many of the powerful computers at bioinformatics centres and also on many desktops and laptops (MacOS is largely UNIX compatible).  The major difference between Unix and Windows is that it is free (as in freedom) and you can modify it to work however you want. This same principle of freedom is also used in most bioinformatics software.  There are many distributions of Unix such as Ubuntu, RedHat, Fedora, Mint,...). These are all Unix, but they bundle up extra software in a different way or combinations. Some are known for being conservative and reliable; whilst others are know for being cutting-edge (and less reliable).  The MacOSX operating system used by the  eBioKit  is also based on Unix.",
            "title": "Why use Unix?"
        },
        {
            "location": "/command_line/#getting-started",
            "text": "For this course, you will have to connect to the eBiokit using SSH. SSH stands for Secure Shell and is a network protocol used to securely connect to a server. To do so, you will need an SSH client:   On Linux: it is included by default, named Terminal.  On MacOS: it is included by default, also named Terminal.  On Windows: you'll have to download and install  MobaXterm , a terminal emulator.   Once you've opened your terminal (or terminal emulator), type  ssh username@ip_address  replacing  username  and  ip_address  with your username and the ip address of the server you are connecting to.\nType your password when prompted. As you type, nothing will show on screen. No stars, no dots.\nIt is supposed to be that way. Just type the password and press enter!   You can type commands directly into the terminal at the \u2018$' prompt. A list of useful commands can be found on the next page. Many of them are two- or three-letter abbreviations. The earliest Unix systems ( circa  1970) only had slow Teletype terminals, so it was faster to type 'rm' to remove a file than 'delete' or 'erase'. This terseness is a feature of Unix that still survives.",
            "title": "Getting started"
        },
        {
            "location": "/command_line/#the-command-line_1",
            "text": "All Unix programs may be run by typing commands at the Unix prompt. The command line tells the computer what to do. You may subtly alter these commands by specifying certain options when typing in the command line.",
            "title": "The command line"
        },
        {
            "location": "/command_line/#command-line-arguments",
            "text": "Typing any Unix command for example  ls ,  mv  or  cd  at the Unix prompt with the appropriate variables such as files names or directories will result in the tasks being performed on pressing the enter key.   The command is separated from the options and arguments by a space. Additional options and/or arguments can be added to the commands to affect the way the command works. Options usually have one dash and a letter (e.g. -h) or two dashes and a word (--help) with no space between the dash and the letter/word. Arguments are usually filenames or directories.  For example: List the contents of a directory  ls  List the contents of a directoryList the contents of a directory with extra information about the files ls \u2013l  List the contents of a directory with extra information about the files ls \u2013a  List all contents including hidden files & directories ls -al  List all contents including hidden files & directories, with extra information about the files ls \u2013l /usr/  List the contents of the directory /usr/, with extra information about the files",
            "title": "Command line Arguments"
        },
        {
            "location": "/command_line/#files-and-directories",
            "text": "Directories are the Unix equivalent of folders on a PC or Mac. They are organised in a hierarchy, so directories can have sub-directories. Directories are very useful for organising your work and keeping your account tidy - for example, if you have more than one project, you can organise the files for each project into different directories to keep them separate. You can think of directories as rooms in a house. You can only be in one room (directory) at a time. When you are in a room you can see everything in that room easily. To see things in other rooms, you have to go to the appropriate door and crane your head around. Unix works in a similar manner, moving from directory to directory to access files. The location or directory that you are in is referred to as the current working directory.  Directory structure example   Therefore if there is a file called genome.seq in the  dna  directory its location or full pathname can be expressed as /nfs/dna/genome.seq.",
            "title": "Files and Directories"
        },
        {
            "location": "/command_line/#general-points",
            "text": "Unix is pretty straightforward, but there are some general points to remember that will make your life easier: most flavors of UNIX are case sensitive - typing  ls  is generally not the same as typing  LS . You need to put a space between a command and its argument - for example,  less my_file  will show you the contents of the file called my_file;  lessmyfile  will just give you an error! Unix is not psychic: If you misspell the name of a command or the name of a file, it will not understand you. Many of the commands are only a few letters long; this can be confusing until you start to think logically about why those letters were chosen - ls for list, rm for remove and so on. Often when you have problems with Unix, it is due to a spelling mistake, or perhaps you have omitted a space. If you want to know more about Unix and its commands there are plenty of resources available that provide a more comprehensive guide (including a cheat sheet at the end of this chapter.   http://unix.t-a-y-l-o-r.com/   In what follows, we shall use the following typographical conventions: Characters written in  bold typewriter font  are commands to be typed into the computer as they stand. Characters written in  italic typewriter font  indicate non-specific file or directory names. Words inserted within square brackets [Ctrl] indicate keys to be pressed. So, for example, $ **ls** *any_directory* [Enter]  means \"at the Unix prompt $, type ls followed by the name of some directory, then press Enter\"\nDon't forget to press the [Enter] key: commands are not sent to the computer until this is done.",
            "title": "General Points"
        },
        {
            "location": "/command_line/#some-useful-unix-commands-command-and-what-it-does",
            "text": "Command  What it does      ls  Lists the contents of the current directory    mkdir  Creates a new directory    mv  Moves or renames a file    cp  Copies a file    rm  Removes a file    cat  Concatenates files    less  Displays the contents of a file one page at a time    head  Displays the first ten lines of a file    tail  Displays the last ten lines of a file    cd  Changes current working directory    pwd  Prints working directory    find  Finds files matching an expression    grep  Searches a file for patterns    wc  Counts the lines, words, characters, and bytes in a file    kill  Stops a process    jobs  Lists the processes that are running",
            "title": "Some useful Unix commands Command\u00a0and What it does"
        },
        {
            "location": "/command_line/#firts-steps",
            "text": "The following exercise introduces a few useful Unix commands and provides examples of how they can be used. Many people panic when they are confronted with an Unix prompt! Don\u2019t! The exercise is designed to be step-by-step, so all the commands you need are provided in the text. If you get lost ask a demonstrator. If you are a person skilled at Unix, be patient it is only a short exercise. Finding where you are and what you\u2019ve got  pwd \u00a0\u00a0\u00a0\u00a0\u00a0\nPrint the working directory As seen previously directories are arranged in a hierarchical structure. To determine where you are in the hierarchy you can use the pwd command to display the name of the current working directory. The current working directory may be thought of as the directory you are in, i.e. your current position in the file-system tree To find out where you are type  pwd [enter]  You will see that you are in your home directory. We need to move into the ngs_course_data directory. Remember, Unix is case sensitive  PWD  is not the same as  pwd  \n     cd \u00a0\nChange current working directory The cd command will change the current working directory to another, in other words allow you to move up or down in the directory hierarchy. First of all we are going to move into the \"ngs_course_data\" directory below. To do this type:  cd ngs_course_data [enter]  Now use the pwd command to check your location in the directory hierarchy.  Change again the directory to  Module_Unix     ls \nList the contents of a directory To find out what are the contents of the current directory type  ls  [enter] The ls command lists the contents of your current directory, this includes files and directories You should see that there are several other directories.  Now use the  cd  command again to change to the  Module_Unix  directory.",
            "title": "Firts steps"
        },
        {
            "location": "/command_line/#changing-and-moving-what-youve-got",
            "text": "cp \nCopy a file.  cp file1 file2  is the command which makes a copy of file1 in the current working directory and calls it file2! What you are going to do is make a copy of AL513382.embl. This file contains the genome of Salmonella typhi strain CT18 in EMBL format (we'll learn more about file formats later during the course). The new file will be called S_typhi.embl.  cp AL513382.embl S_typhi.embl [enter] \nIf you use the ls command to check the contents of the current directory you will see that there is an extra file called S_typhi.embl.  \n  \u00a0   rm \nDelete a file. This command removes a file permanently, so be careful! You are now going to remove the old version of the  S. typhi  genome file, AL513382.embl rm AL513382.embl [enter]  The file will be removed. Use the  ls  command to check the contents of the current directory to see that AL513382.embl has been removed.  Unix, as a general rule does exactly what you ask, and does not ask for confirmation. Unfortunately there is no \"recycle bin\" on the command line to recover the file from, so you have to be careful.  \n  \u00a0   cd \nChange current working directory. As before the cd command will change the current working directory to another, in other words allow you to move up or down in the directory hierarchy. First of all we are going to move into the directory above, type: cd .. [enter]  Now use the  pwd  command to check your location in the directory hierarchy. Next, we are going to move into the Module_Artemis directory. To change to the Module_Artemis directory type: cd Module_Artemis [enter]  use the ls command to check the contents of the directory.",
            "title": "Changing and moving what you\u2019ve got"
        },
        {
            "location": "/command_line/#tips",
            "text": "There are some short cuts for referring to directories:  . \u00a0Current directory (one full stop)  \n.. Directory above (two full stops)  \n~ \u00a0Home directory (tilde)  \n/ \u00a0Root of the file system (like C:\\ in Windows)  Pressing the tab key twice will try and autocomplete what you\u2019ve started typing or give you a list of all possible completions. This saves a lot of typing and typos. Pressing the up/down arrows will let you scroll through the previous commands. If you highlight some text, middle clicking will paste it on the command line.  mv \nMove a file. To move a file from one place to another use the  mv  command. This moves the file rather than copies it, therefore you end up with only one file rather than two. When using the command the path or pathname is used to tell Unix where to find the file. You refer to files in other directories by using the list of hierarchical names separated by slashes. For example, the file bases in the directory genome has the path genome/bases If no path is specified Unix assumes that the file is in the current working directory. What you are going to do is move the file S_typhi.embl from the Module_Unix directory, to the current working directory. mv ../Module_Unix/S_typhi.embl . [enter] \nUse the ls command to check the contents of the current directory to see that S_typhi.embl has been moved. ../Module_Unix/S_typhi.embl specifies that S_typhi.embl is in the Module_Unix directory. If the file was in the directory above, the path would change to: ../ S_typhi.embl  \n     The command can also be used to rename a file in the current working directory. Previously we used the cp command, but mv provides an alternative without the need to delete the original file. Therefore we could have used: \u00a0  mv AL513382.embl S_typhi.embl [enter]  instead of:  cp AL513382.embl S_typhi.embl [enter]\nrm AL513382.embl [enter]",
            "title": "Tips"
        },
        {
            "location": "/command_line/#viewing-what-youve-got",
            "text": "less \nDisplay file contents. This command displays the contents of a specified file one screen at a time. You are now going to look at the contents of S_typhi.embl. less S_typhi.embl [enter] \nThe contents of S_typhi.embl will be displayed one screen at a time, to view the next screen press the space bar. less can also scroll backwards if you hit the b key. Another useful feature is the slash key, /, to search for a word in the file. You type the word you are looking for and press enter. The screen will jump to the next occurrence and highlight it. As S_typhi.embl is a large file this will take a while, therefore you may want to escape or exit from this command. To exit press the letter \u2018q\u2019. If you really need to exit from a program and it isn\u2019t responding press \u2018control\u2019 and the letter \u2018c\u2019 at the same time.  \n  \u00a0   head   \nDisplay the first ten lines of a file  tail \u00a0 \nDisplay the last ten lines of a file    Sometimes you may just want to view the text at the beginning or the end of a file, without having to display all of the file. The head and tail commands can be used to do this. You are now going to look at the beginning of S_typhi.embl.  head S_typhi.embl [enter]  To look at the end of S_typhi.embl type:  tail S_typhi.embl [enter] \nThe number of lines that are displayed can be increased by adding extra arguments. To increase the number of lines viewed from 10 to 100 add the \u2013100 argument to the command. For example to view the last 100 lines of S_typhi.embl type:  tail -100 S_typhi.embl [enter]  Do this for both head and tail commands. What type of information is at the beginning and end of the EMBL format file?  \n  \u00a0   cat \nJoin files together. Having looked at the beginning and end of the S_typhi.embl file you should notice that in EMBL format files the annotation comes first, then the DNA sequence at the end. If you had two separate files containing the annotation and the DNA sequence, both in EMBL format, it is possible to concatenate or join the two together to make a single file like the S_typhi.embl file you have just looked at. The Unix command cat can be used to join two or more files into a single file. The order in which the files are joined is determined by the order in which they appear in the command line. For example, we have two separate files, MAL13P1.dna and MAL13P1.tab, that contain the DNA and annotation, respectively, from the  P. falciparum  genome. Return to the Module_Unix directory using the cd command: cd ../Module_Unix [enter]  and type cat MAL13P1.tab MAL13P1.dna > MAL13P1.embl [enter] \nMAL13P1.tab and MAL13P1.dna will be joined together and written to a file called MAL13P1.embl\nThe  >  symbol in the command line directs the output of the cat program to the designated file MAL13P1.embl  \n  \u00a0   wc \nCounts the lines, words or characters of files.\nBy typing the command line: ls | wc -l [enter]  The above command uses wc to count the number of files that are listed by ls. The \u2018-l\u2019 option tells wc to return a count of the number of lines. The | symbol (known as the \u2018pipe\u2019 character) in the command line connects the two commands into a single operation for simplicity. You can connect as many commands as you want:  ls | grep \".embl\" | wc -l \nThis command will list out all of the files in the current directory, then send the results to the grep command which searches for all filenames containing the \u2018embl\u2019, then sends the results to wc which counts the number of lines (which corresponds to the number of files).  \n     grep \nSearches a file for patterns.  grep  is a powerful tool to search for patterns in a file. In the examples below, we are going to use the file called Malaria.fasta that contains the set of  P. falciparum  chromosomes in FASTA format. A FASTA file has the following format:    Sequence Header\nCTAAACCTAAACCTAAACCCTGAACCCTAA...    Therefore if we want to get the sequence headers, we can extract the lines that match the \u2018>\u2019 symbol:  grep \u2018>\u2019 Malaria.fasta [enter]  By typing the command line:  grep -B 1 -A 1 'aagtagggttca' Malaria.fasta [enter]  This command will search for a nucliotide sequence and print 1 line before and after any match. It won\u2019t find the pattern if it spans more than 1 line.  \n     find \nFinds files matching an expression. The find command is similar to ls but in many ways it is more powerful. It can be used to recursively search the directory tree for a specified path name, seeking files that match a given Boolean expression (a test which returns true or false)  find . -name \u201c*.embl\u201d  This command will return the files which name has the .embl suffix.  mkdir test_directory  find . -type d  This command will return all the subdirectories contained in the current directory. These are just two basic examples but it is possible to search in many other ways:  -mtime  search files by modifying date  -atime  search files by last access date  -size  search files by file size  -user  search files by user they belong to.",
            "title": "Viewing what you\u2019ve got"
        },
        {
            "location": "/command_line/#tips_1",
            "text": "You need to be careful with quoting when using wildcards!  The wildcard * symbol represents a string of any character and of any length.  \n   \n\nFor more information on Unix command see EMBNet UNIX Quick Guide.",
            "title": "Tips"
        },
        {
            "location": "/file_formats/",
            "text": "File Formats\n\u00b6\n\n\nThis lecture is aimed at making you discover the most popular file formats used in bioinformatics. You're expected to have basic working knowledge of Linux to be able to follow the lesson.\n\n\nTable of Contents\n\u00b6\n\n\n\n\nThe fasta format\n\n\nThe fastq format\n\n\nThe sam/bam format\n\n\nThe vcf format\n\n\nThe gff format\n\n\n\n\nThe fasta format\n\u00b6\n\n\nThe fasta format was invented in 1988 and designed to represent nucleotide or peptide sequences. It originates from the \nFASTA\n software package, but is now a standard in the world of bioinformatics.\n\n\nThe first line in a FASTA file starts with a \">\" (greater-than) symbol followed by the description or identifier of the sequence. Following the initial line (used for a unique description of the sequence) is the actual sequence itself in standard one-letter code.\n\n\nA few sample sequences:\n\n\n>KX580312.1 Homo sapiens truncated breast cancer 1 (BRCA1) gene, exon 15 and partial cds\nGTCATCCCCTTCTAAATGCCCATCATTAGATGATAGGTGGTACATGCACAGTTGCTCTGGGAGTCTTCAG\nAATAGAAACTACCCATCTCAAGAGGAGCTCATTAAGGTTGTTGATGTGGAGGAGTAACAGCTGGAAGAGT\nCTGGGCCACACGATTTGACGGAAACATCTTACTTGCCAAGGCAAGATCTAG\n\n\n\n\n\n>KRN06561.1 heat shock [Lactobacillus sucicola DSM 21376 = JCM 15457]\nMSLVMANELTNRFNNWMKQDDFFGNLGRSFFDLDNSVNRALKTDVKETDKAYEVRIDVPGIDKKDITVDY\nHDGVLSVNAKRDSFNDESDSEGNVIASERSYGRFARQYSLPNVDESGIKAKCEDGVLKLTLPKLAEEKIN\nGNHIEIE\n\n\n\n\n\nA fasta file can contain multiple sequence. Each sequence will be separated by their \"header\" line, starting by \">\".\n\n\nExample:\n\n\n>KRN06561.1 heat shock [Lactobacillus sucicola DSM 21376 = JCM 15457]\nMSLVMANELTNRFNNWMKQDDFFGNLGRSFFDLDNSVNRALKTDVKETDKAYEVRIDVPGIDKKDITVDY\nHDGVLSVNAKRDSFNDESDSEGNVIASERSYGRFARQYSLPNVDESGIKAKCEDGVLKLTLPKLAEEKIN\nGNHIEIE\n>3HHU_A Chain A, Human Heat-Shock Protein 90 (Hsp90)\nMPEETQTQDQPMEEEEVETFAFQAEIAQLMSLIINTFYSNKEIFLRELISNSSDALDKIRYESLTDPSKL\nDSGKELHINLIPNKQDRTLTIVDTGIGMTKADLINNLGTIAKSGTKAFMEALQAGADISMIGQFGVGFYS\nAYLVAEKVTVITKHNDDEQYAWESSAGGSFTVRTDTGEPMGRGTKVILHLKEDQTEYLEERRIKEIVKKH\nSQFIGYPITLFVEK\n\n\n\n\n\nThe fastq format\n\u00b6\n\n\nThe fastq format is also a text based format to represent nucleotide sequences, but also contains the corresponding quality of each nucleotide. It is the standard for storing the output of high-throughput sequencing instruments such as the Illumina machines.\n\n\nA fastq file uses four lines per sequence:\n\n\n\n\nLine 1 begins with a '@' character and is followed by a sequence identifier and an optional description (like a FASTA title line).\n\n\nLine 2 is the raw sequence letters.\n\n\nLine 3 begins with a '+' character and is optionally followed by the same sequence identifier (and any description) again.\n\n\nLine 4 encodes the quality values for the sequence in Line 2, and must contain the same number of symbols as letters in the sequence.\n\n\n\n\nAn example sequence in fastq format:\n\n\n@SEQ_ID\nGATTTGGGGTTCAAAGCAGTATCGATCAAATAGTAAATCCATTTGTTCAACTCACAGTTT\n+\n!''*((((***+))%%%++)(%%%%).1***-+*''))**55CCF>>>>>>CCCCCCC65\n\n\n\n\n\nQuality\n\u00b6\n\n\nThe quality, also called phred score, is the probability that the corresponding basecall is incorrect.\n\n\nPhred scores use a logarithmic scale, and are represented by ASCII characters, mapping to a quality usually going from 0 to 40.\n\n\n\n\n\n\n\n\nPhred Quality Score\n\n\nProbability of incorrect base call\n\n\nBase call accuracy\n\n\n\n\n\n\n\n\n\n\n10\n\n\n1 in 10\n\n\n90%\n\n\n\n\n\n\n20\n\n\n1 in 100\n\n\n99%\n\n\n\n\n\n\n30\n\n\n1 in 1000\n\n\n99.9%\n\n\n\n\n\n\n40\n\n\n1 in 10,000\n\n\n99.99%\n\n\n\n\n\n\n50\n\n\n1 in 100,000\n\n\n99.999%\n\n\n\n\n\n\n60\n\n\n1 in 1,000,000\n\n\n99.9999%\n\n\n\n\n\n\n\n\nthe sam/bam format\n\u00b6\n\n\nFrom \nWikipedia\n:\n\n\nSAM (file format) is a text-based format for storing biological sequences aligned to a reference sequence developed by Heng Li. The acronym SAM stands for Sequence Alignment/Map. It is widely used for storing data, such as nucleotide sequences, generated by Next generation sequencing technologies and usually mapped to a reference.\n\n\nThe SAM format consists of a header and an alignment section. The binary representation of a SAM file is a BAM file, which is a compressed SAM file.[1] SAM files can be analysed and edited with the software SAMtools.\n\n\nThe SAM format has a really extensive and complex specification that you can find \nhere\n.\n\n\nIn brief it consists of a header section and reads (with other information) in tab delimited format.\n\n\nExample header section\n\u00b6\n\n\n@HD VN:1.0                  SO:unsorted\n@SQ SN:O_volvulusOVOC_OM1a  LN:2816604\n@SQ SN:O_volvulusOVOC_OM1b  LN:28345163\n@SQ SN:O_volvulusOVOC_OM2   LN:25485961\n\n\n\n\n\nExample read\n\u00b6\n\n\nM01137\n:\n130\n:\n00\n-\nA\n:\n17009\n:\n1352\n/\n14\n \n*\n \n0\n \n0\n \n*\n \n*\n \n0\n \n0\n \nAGCAAAATACAACGATCTGGATGGTAGCATTAGCGATGCGACACTGCTTGAACCGTCAAAG\n \nFGGFGCFGFFGC8\n,,\n@\nD\n?\nE6EFCF\n,=\nAEFFGGDGGGADFGG\n@\n>\nFFEGGG\n:+<\n7\nD\n>\nAFCFGG\n \nYT\n:\nZ\n:\nUU\n\n\n\n\n\n\nthe vcf format\n\u00b6\n\n\nThe vcf format is also a text-based file format. VCF stands for Variant Call Format and is used to store gene sequence variations (SNVs, indels). The format has been developped for genotyping projects, and is the standard to represent variations in the genome of a species.\n\n\nA vcf is a tab-delimited file, described \nhere\n.\n\n\nVCF Example\n\u00b6\n\n\n##fileformat=VCFv4.0\n##fileDate=20110705\n##reference=1000GenomesPilot-NCBI37\n##phasing=partial\n##INFO=<ID=NS,Number=1,Type=Integer,Description=\"Number of Samples With Data\">\n##INFO=<ID=DP,Number=1,Type=Integer,Description=\"Total Depth\">\n##INFO=<ID=AF,Number=.,Type=Float,Description=\"Allele Frequency\">\n##INFO=<ID=AA,Number=1,Type=String,Description=\"Ancestral Allele\">\n##INFO=<ID=DB,Number=0,Type=Flag,Description=\"dbSNP membership, build 129\">\n##INFO=<ID=H2,Number=0,Type=Flag,Description=\"HapMap2 membership\">\n##FILTER=<ID=q10,Description=\"Quality below 10\">\n##FILTER=<ID=s50,Description=\"Less than 50% of samples have data\">\n##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\"Genotype Quality\">\n##FORMAT=<ID=GT,Number=1,Type=String,Description=\"Genotype\">\n##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\"Read Depth\">\n##FORMAT=<ID=HQ,Number=2,Type=Integer,Description=\"Haplotype Quality\">\n#CHROM POS    ID        REF  ALT     QUAL FILTER INFO                              FORMAT      Sample1        Sample2        Sample3\n2      4370   rs6057    G    A       29   .      NS=2;DP=13;AF=0.5;DB;H2           GT:GQ:DP:HQ 0|0:48:1:52,51 1|0:48:8:51,51 1/1:43:5:.,.\n2      7330   .         T    A       3    q10    NS=5;DP=12;AF=0.017               GT:GQ:DP:HQ 0|0:46:3:58,50 0|1:3:5:65,3   0/0:41:3\n2      110696 rs6055    A    G,T     67   PASS   NS=2;DP=10;AF=0.333,0.667;AA=T;DB GT:GQ:DP:HQ 1|2:21:6:23,27 2|1:2:0:18,2   2/2:35:4\n2      130237 .         T    .       47   .      NS=2;DP=16;AA=T                   GT:GQ:DP:HQ 0|0:54:7:56,60 0|0:48:4:56,51 0/0:61:2\n2      134567 microsat1 GTCT G,GTACT 50   PASS   NS=2;DP=9;AA=G                    GT:GQ:DP    0/1:35:4       0/2:17:2       1/1:40:3\nchr1    45796269        .       G       C\nchr1    45797505        .       C       G\nchr1    45798555        .       T       C\nchr1    45798901        .       C       T\nchr1    45805566        .       G       C\nchr2    47703379        .       C       T\nchr2    48010488        .       G       A\nchr2    48030838        .       A       T\nchr2    48032875        .       CTAT    -\nchr2    48032937        .       T       C\nchr2    48033273        .       TTTTTGTTTTAATTCCT       -\nchr2    48033551        .       C       G\nchr2    48033910        .       A       T\nchr2    215632048       .       G       T\nchr2    215632125       .       TT      -\nchr2    215632155       .       T       C\nchr2    215632192       .       G       A\nchr2    215632255       .       CA      TG\nchr2    215634055       .       C       T\n\n\n\n\n\nthe gff format\n\u00b6\n\n\nThe general feature format (gff) is another text file format, used for describing genes and other features of DNA, RNA and protein sequences. It is the standard for annotation of genomes.\n\n\nA gff file should contain 9 columns, described \nhere\n\n\nExample gff\n\u00b6\n\n\n##description: evidence-based annotation of the human genome (GRCh38), version 25 (Ensembl 85)\n##provider: GENCODE\n##contact: gencode-help@sanger.ac.uk\n##format: gtf\n##date: 2016-07-15\nchr1    HAVANA  gene    11869   14409   .   +   .   gene_id \"ENSG00000223972.5\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; level 2; havana_gene \"OTTHUMG00000000961.2\";\nchr1    HAVANA  transcript  11869   14409   .   +   .   gene_id \"ENSG00000223972.5\"; transcript_id \"ENST00000456328.2\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; transcript_type \"processed_transcript\"; transcript_status \"KNOWN\"; transcript_name \"DDX11L1-002\"; level 2; transcript_support_level \"1\"; tag \"basic\"; havana_gene \"OTTHUMG00000000961.2\"; havana_transcript \"OTTHUMT00000362751.1\";\nchr1    HAVANA  exon    11869   12227   .   +   .   gene_id \"ENSG00000223972.5\"; transcript_id \"ENST00000456328.2\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; transcript_type \"processed_transcript\"; transcript_status \"KNOWN\"; transcript_name \"DDX11L1-002\"; exon_number 1; exon_id \"ENSE00002234944.1\"; level 2; transcript_support_level \"1\"; tag \"basic\"; havana_gene \"OTTHUMG00000000961.2\"; havana_transcript \"OTTHUMT00000362751.1\";\nchr1    HAVANA  exon    12613   12721   .   +   .   gene_id \"ENSG00000223972.5\"; transcript_id \"ENST00000456328.2\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; transcript_type \"processed_transcript\"; transcript_status \"KNOWN\"; transcript_name \"DDX11L1-002\"; exon_number 2; exon_id \"ENSE00003582793.1\"; level 2; transcript_support_level \"1\"; tag \"basic\"; havana_gene \"OTTHUMG00000000961.2\"; havana_transcript \"OTTHUMT00000362751.1\";\nchr1    HAVANA  exon    13221   14409   .   +   .   gene_id \"ENSG00000223972.5\"; transcript_id \"ENST00000456328.2\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; transcript_type \"processed_transcript\"; transcript_status \"KNOWN\"; transcript_name \"DDX11L1-002\"; exon_number 3; exon_id \"ENSE00002312635.1\"; level 2; transcript_support_level \"1\"; tag \"basic\"; havana_gene \"OTTHUMG00000000961.2\"; havana_transcript \"OTTHUMT00000362751.1\";",
            "title": "File Formats"
        },
        {
            "location": "/file_formats/#file-formats",
            "text": "This lecture is aimed at making you discover the most popular file formats used in bioinformatics. You're expected to have basic working knowledge of Linux to be able to follow the lesson.",
            "title": "File Formats"
        },
        {
            "location": "/file_formats/#table-of-contents",
            "text": "The fasta format  The fastq format  The sam/bam format  The vcf format  The gff format",
            "title": "Table of Contents"
        },
        {
            "location": "/file_formats/#the-fasta-format",
            "text": "The fasta format was invented in 1988 and designed to represent nucleotide or peptide sequences. It originates from the  FASTA  software package, but is now a standard in the world of bioinformatics.  The first line in a FASTA file starts with a \">\" (greater-than) symbol followed by the description or identifier of the sequence. Following the initial line (used for a unique description of the sequence) is the actual sequence itself in standard one-letter code.  A few sample sequences:  >KX580312.1 Homo sapiens truncated breast cancer 1 (BRCA1) gene, exon 15 and partial cds\nGTCATCCCCTTCTAAATGCCCATCATTAGATGATAGGTGGTACATGCACAGTTGCTCTGGGAGTCTTCAG\nAATAGAAACTACCCATCTCAAGAGGAGCTCATTAAGGTTGTTGATGTGGAGGAGTAACAGCTGGAAGAGT\nCTGGGCCACACGATTTGACGGAAACATCTTACTTGCCAAGGCAAGATCTAG  >KRN06561.1 heat shock [Lactobacillus sucicola DSM 21376 = JCM 15457]\nMSLVMANELTNRFNNWMKQDDFFGNLGRSFFDLDNSVNRALKTDVKETDKAYEVRIDVPGIDKKDITVDY\nHDGVLSVNAKRDSFNDESDSEGNVIASERSYGRFARQYSLPNVDESGIKAKCEDGVLKLTLPKLAEEKIN\nGNHIEIE  A fasta file can contain multiple sequence. Each sequence will be separated by their \"header\" line, starting by \">\".  Example:  >KRN06561.1 heat shock [Lactobacillus sucicola DSM 21376 = JCM 15457]\nMSLVMANELTNRFNNWMKQDDFFGNLGRSFFDLDNSVNRALKTDVKETDKAYEVRIDVPGIDKKDITVDY\nHDGVLSVNAKRDSFNDESDSEGNVIASERSYGRFARQYSLPNVDESGIKAKCEDGVLKLTLPKLAEEKIN\nGNHIEIE\n>3HHU_A Chain A, Human Heat-Shock Protein 90 (Hsp90)\nMPEETQTQDQPMEEEEVETFAFQAEIAQLMSLIINTFYSNKEIFLRELISNSSDALDKIRYESLTDPSKL\nDSGKELHINLIPNKQDRTLTIVDTGIGMTKADLINNLGTIAKSGTKAFMEALQAGADISMIGQFGVGFYS\nAYLVAEKVTVITKHNDDEQYAWESSAGGSFTVRTDTGEPMGRGTKVILHLKEDQTEYLEERRIKEIVKKH\nSQFIGYPITLFVEK",
            "title": "The fasta format"
        },
        {
            "location": "/file_formats/#the-fastq-format",
            "text": "The fastq format is also a text based format to represent nucleotide sequences, but also contains the corresponding quality of each nucleotide. It is the standard for storing the output of high-throughput sequencing instruments such as the Illumina machines.  A fastq file uses four lines per sequence:   Line 1 begins with a '@' character and is followed by a sequence identifier and an optional description (like a FASTA title line).  Line 2 is the raw sequence letters.  Line 3 begins with a '+' character and is optionally followed by the same sequence identifier (and any description) again.  Line 4 encodes the quality values for the sequence in Line 2, and must contain the same number of symbols as letters in the sequence.   An example sequence in fastq format:  @SEQ_ID\nGATTTGGGGTTCAAAGCAGTATCGATCAAATAGTAAATCCATTTGTTCAACTCACAGTTT\n+\n!''*((((***+))%%%++)(%%%%).1***-+*''))**55CCF>>>>>>CCCCCCC65",
            "title": "The fastq format"
        },
        {
            "location": "/file_formats/#quality",
            "text": "The quality, also called phred score, is the probability that the corresponding basecall is incorrect.  Phred scores use a logarithmic scale, and are represented by ASCII characters, mapping to a quality usually going from 0 to 40.     Phred Quality Score  Probability of incorrect base call  Base call accuracy      10  1 in 10  90%    20  1 in 100  99%    30  1 in 1000  99.9%    40  1 in 10,000  99.99%    50  1 in 100,000  99.999%    60  1 in 1,000,000  99.9999%",
            "title": "Quality"
        },
        {
            "location": "/file_formats/#the-sambam-format",
            "text": "From  Wikipedia :  SAM (file format) is a text-based format for storing biological sequences aligned to a reference sequence developed by Heng Li. The acronym SAM stands for Sequence Alignment/Map. It is widely used for storing data, such as nucleotide sequences, generated by Next generation sequencing technologies and usually mapped to a reference.  The SAM format consists of a header and an alignment section. The binary representation of a SAM file is a BAM file, which is a compressed SAM file.[1] SAM files can be analysed and edited with the software SAMtools.  The SAM format has a really extensive and complex specification that you can find  here .  In brief it consists of a header section and reads (with other information) in tab delimited format.",
            "title": "the sam/bam format"
        },
        {
            "location": "/file_formats/#example-header-section",
            "text": "@HD VN:1.0                  SO:unsorted\n@SQ SN:O_volvulusOVOC_OM1a  LN:2816604\n@SQ SN:O_volvulusOVOC_OM1b  LN:28345163\n@SQ SN:O_volvulusOVOC_OM2   LN:25485961",
            "title": "Example header section"
        },
        {
            "location": "/file_formats/#example-read",
            "text": "M01137 : 130 : 00 - A : 17009 : 1352 / 14   *   0   0   *   *   0   0   AGCAAAATACAACGATCTGGATGGTAGCATTAGCGATGCGACACTGCTTGAACCGTCAAAG   FGGFGCFGFFGC8 ,, @ D ? E6EFCF ,= AEFFGGDGGGADFGG @ > FFEGGG :+< 7 D > AFCFGG   YT : Z : UU",
            "title": "Example read"
        },
        {
            "location": "/file_formats/#the-vcf-format",
            "text": "The vcf format is also a text-based file format. VCF stands for Variant Call Format and is used to store gene sequence variations (SNVs, indels). The format has been developped for genotyping projects, and is the standard to represent variations in the genome of a species.  A vcf is a tab-delimited file, described  here .",
            "title": "the vcf format"
        },
        {
            "location": "/file_formats/#vcf-example",
            "text": "##fileformat=VCFv4.0\n##fileDate=20110705\n##reference=1000GenomesPilot-NCBI37\n##phasing=partial\n##INFO=<ID=NS,Number=1,Type=Integer,Description=\"Number of Samples With Data\">\n##INFO=<ID=DP,Number=1,Type=Integer,Description=\"Total Depth\">\n##INFO=<ID=AF,Number=.,Type=Float,Description=\"Allele Frequency\">\n##INFO=<ID=AA,Number=1,Type=String,Description=\"Ancestral Allele\">\n##INFO=<ID=DB,Number=0,Type=Flag,Description=\"dbSNP membership, build 129\">\n##INFO=<ID=H2,Number=0,Type=Flag,Description=\"HapMap2 membership\">\n##FILTER=<ID=q10,Description=\"Quality below 10\">\n##FILTER=<ID=s50,Description=\"Less than 50% of samples have data\">\n##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\"Genotype Quality\">\n##FORMAT=<ID=GT,Number=1,Type=String,Description=\"Genotype\">\n##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\"Read Depth\">\n##FORMAT=<ID=HQ,Number=2,Type=Integer,Description=\"Haplotype Quality\">\n#CHROM POS    ID        REF  ALT     QUAL FILTER INFO                              FORMAT      Sample1        Sample2        Sample3\n2      4370   rs6057    G    A       29   .      NS=2;DP=13;AF=0.5;DB;H2           GT:GQ:DP:HQ 0|0:48:1:52,51 1|0:48:8:51,51 1/1:43:5:.,.\n2      7330   .         T    A       3    q10    NS=5;DP=12;AF=0.017               GT:GQ:DP:HQ 0|0:46:3:58,50 0|1:3:5:65,3   0/0:41:3\n2      110696 rs6055    A    G,T     67   PASS   NS=2;DP=10;AF=0.333,0.667;AA=T;DB GT:GQ:DP:HQ 1|2:21:6:23,27 2|1:2:0:18,2   2/2:35:4\n2      130237 .         T    .       47   .      NS=2;DP=16;AA=T                   GT:GQ:DP:HQ 0|0:54:7:56,60 0|0:48:4:56,51 0/0:61:2\n2      134567 microsat1 GTCT G,GTACT 50   PASS   NS=2;DP=9;AA=G                    GT:GQ:DP    0/1:35:4       0/2:17:2       1/1:40:3\nchr1    45796269        .       G       C\nchr1    45797505        .       C       G\nchr1    45798555        .       T       C\nchr1    45798901        .       C       T\nchr1    45805566        .       G       C\nchr2    47703379        .       C       T\nchr2    48010488        .       G       A\nchr2    48030838        .       A       T\nchr2    48032875        .       CTAT    -\nchr2    48032937        .       T       C\nchr2    48033273        .       TTTTTGTTTTAATTCCT       -\nchr2    48033551        .       C       G\nchr2    48033910        .       A       T\nchr2    215632048       .       G       T\nchr2    215632125       .       TT      -\nchr2    215632155       .       T       C\nchr2    215632192       .       G       A\nchr2    215632255       .       CA      TG\nchr2    215634055       .       C       T",
            "title": "VCF Example"
        },
        {
            "location": "/file_formats/#the-gff-format",
            "text": "The general feature format (gff) is another text file format, used for describing genes and other features of DNA, RNA and protein sequences. It is the standard for annotation of genomes.  A gff file should contain 9 columns, described  here",
            "title": "the gff format"
        },
        {
            "location": "/file_formats/#example-gff",
            "text": "##description: evidence-based annotation of the human genome (GRCh38), version 25 (Ensembl 85)\n##provider: GENCODE\n##contact: gencode-help@sanger.ac.uk\n##format: gtf\n##date: 2016-07-15\nchr1    HAVANA  gene    11869   14409   .   +   .   gene_id \"ENSG00000223972.5\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; level 2; havana_gene \"OTTHUMG00000000961.2\";\nchr1    HAVANA  transcript  11869   14409   .   +   .   gene_id \"ENSG00000223972.5\"; transcript_id \"ENST00000456328.2\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; transcript_type \"processed_transcript\"; transcript_status \"KNOWN\"; transcript_name \"DDX11L1-002\"; level 2; transcript_support_level \"1\"; tag \"basic\"; havana_gene \"OTTHUMG00000000961.2\"; havana_transcript \"OTTHUMT00000362751.1\";\nchr1    HAVANA  exon    11869   12227   .   +   .   gene_id \"ENSG00000223972.5\"; transcript_id \"ENST00000456328.2\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; transcript_type \"processed_transcript\"; transcript_status \"KNOWN\"; transcript_name \"DDX11L1-002\"; exon_number 1; exon_id \"ENSE00002234944.1\"; level 2; transcript_support_level \"1\"; tag \"basic\"; havana_gene \"OTTHUMG00000000961.2\"; havana_transcript \"OTTHUMT00000362751.1\";\nchr1    HAVANA  exon    12613   12721   .   +   .   gene_id \"ENSG00000223972.5\"; transcript_id \"ENST00000456328.2\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; transcript_type \"processed_transcript\"; transcript_status \"KNOWN\"; transcript_name \"DDX11L1-002\"; exon_number 2; exon_id \"ENSE00003582793.1\"; level 2; transcript_support_level \"1\"; tag \"basic\"; havana_gene \"OTTHUMG00000000961.2\"; havana_transcript \"OTTHUMT00000362751.1\";\nchr1    HAVANA  exon    13221   14409   .   +   .   gene_id \"ENSG00000223972.5\"; transcript_id \"ENST00000456328.2\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; transcript_type \"processed_transcript\"; transcript_status \"KNOWN\"; transcript_name \"DDX11L1-002\"; exon_number 3; exon_id \"ENSE00002312635.1\"; level 2; transcript_support_level \"1\"; tag \"basic\"; havana_gene \"OTTHUMG00000000961.2\"; havana_transcript \"OTTHUMT00000362751.1\";",
            "title": "Example gff"
        },
        {
            "location": "/qc/",
            "text": "Quality Control and Trimming\n\u00b6\n\n\nIn this practical you will learn to import, view and check the quality of raw high thoughput sequencing sequencing data.\n\n\nThe first dataset you will be working with is from an Illumina MiSeq dataset.\nThe sequenced organism is an enterohaemorrhagic E. coli (EHEC) of the serotype O157, a potentially fatal gastrointestinal pathogen.\nThe sequenced bacterium was part of an outbreak investigation in the St. Louis area, USA in 2011.\nThe sequencing was done as paired-end 2x150bp.\n\n\nDownloading the data\n\u00b6\n\n\nThe raw data were deposited at the European Nucleotide Archive, under the accession number SRR957824.\nYou could go to the ENA \nwebsite\n and search for the run with the accession SRR957824.\n\n\nHowever these files contain about 3 million reads and are therefore quite big.\nWe are only gonna use a subset of the original dataset for this tutorial.\n\n\nFirst create a \ndata/\n directory in your home folder\n\n\nmkdir ~/data\n\n\n\n\n\nnow let's download the subset\n\n\ncd\n ~/data\ncurl -O -J -L https://osf.io/shqpv/download\ncurl -O -J -L https://osf.io/9m3ch/download\n\n\n\n\n\nLet\u2019s make sure we downloaded all of our data using md5sum.\n\n\nmd5sum SRR957824_500K_R1.fastq.gz SRR957824_500K_R2.fastq.gz\n\n\n\n\n\nyou should see this\n\n\n1e8cf249e3217a5a0bcc0d8a654585fb  SRR957824_500K_R1.fastq.gz\n70c726a31f05f856fe942d727613adb7  SRR957824_500K_R2.fastq.gz\n\n\n\n\n\nand now look at the file names and their size\n\n\nls -l\n\n\n\n\n\ntotal 97M\n-rw-r--r-- 1 hadrien 48M Nov 19 18:44 SRR957824_500K_R1.fastq.gz\n-rw-r--r-- 1 hadrien 50M Nov 19 18:53 SRR957824_500K_R2.fastq.gz\n\n\n\n\n\nThere are 500 000 paired-end reads taken randomly from the original data\n\n\nOne last thing before we get to the quality control: those files are writeable.\nBy default, UNIX makes things writeable by the file owner.\nThis poses an issue with creating typos or errors in raw data.\nWe fix that before going further\n\n\nchmod u-w *\n\n\n\n\n\nWorking Directory\n\u00b6\n\n\nFirst we make a work directory: a directory where we can play around with a copy of the data without messing with the original\n\n\nmkdir ~/work\n\ncd\n ~/work\n\n\n\n\n\nNow we make a link of the data in our working directory\n\n\nln -s ~/data/* .\n\n\n\n\n\nThe files that we've downloaded are FASTQ files. Take a look at one of them with\n\n\nzless SRR957824_500K_R1.fastq.gz\n\n\n\n\n\n\n\nTip\n\n\nUse the spacebar to scroll down, and type \u2018q\u2019 to exit \u2018less\u2019\n\n\n\n\nYou can read more on the FASTQ format in the \nFile Formats\n lesson.\n\n\n\n\nQuestion\n\n\nWhere does the filename come from?\n\n\n\n\n\n\nQuestion\n\n\nWhy are there 1 and 2 in the file names?    \n\n\n\n\nFastQC\n\u00b6\n\n\nTo check the quality of the sequence data we will use a tool called FastQC.\n\n\nFastQC has a graphical interface and can be downloaded and run on a Windows or Linux computer without installation.\nIt is available \nhere\n.\n\n\nHowever, FastQC is also available as a command line utility on the training server you are using.\nTo run FastQC on our two files\n\n\nfastqc SRR957824_500K_R1.fastq.gz SRR957824_500K_R2.fastq.gz\n\n\n\n\n\nand look what FastQC has produced\n\n\nls *fastqc*\n\n\n\n\n\nFor each file, FastQC has produced both a .zip archive containing all the plots, and a html report.\n\n\nDownload and open the html files with your favourite web browser.\n\n\nAlternatively you can look a these copies of them:\n\n\n\n\nSRR957824_500K_R1_fastqc.html\n\n\nSRR957824_500K_R2_fastqc.html\n\n\n\n\n\n\nQuestion\n\n\nWhat should you pay attention to in the FastQC report?\n\n\n\n\n\n\nQuestion\n\n\nWhich file is of better quality?\n\n\n\n\nPay special attention to the per base sequence quality and sequence length distribution.\nExplanations for the various quality modules can be found \nhere\n.\nAlso, have a look at examples of a \ngood\n and a \nbad\n illumina read set for comparison.\n\n\nYou will note that the reads in your uploaded dataset have fairly poor quality (<20) towards the end. There are also outlier reads that have very poor quality for most of the second half of the reads.\n\n\nScythe\n\u00b6\n\n\nNow we'll do some trimming!\n\n\nScythe uses a Naive Bayesian approach to classify contaminant substrings in sequence reads.\nIt considers quality information, which can make it robust in picking out 3'-end adapters, which often include poor quality bases.\n\n\nThe first thing we need is the adapters to trim off\n\n\ncurl -O -J -L https://osf.io/v24pt/download\n\n\n\n\n\nNow we run scythe on both our read files\n\n\nscythe -a adapters.fasta -o SRR957824_adapt_R1.fastq SRR957824_500K_R1.fastq.gz\nscythe -a adapters.fasta -o SRR957824_adapt_R2.fastq SRR957824_500K_R2.fastq.gz\n\n\n\n\n\n\n\nQuestion\n\n\nWhat adapters do you use?\n\n\n\n\nSickle\n\u00b6\n\n\nMost modern sequencing technologies produce reads that have deteriorating quality towards the 3'-end and some towards the 5'-end as well.\nIncorrectly called bases in both regions negatively impact assembles, mapping, and downstream bioinformatics analyses.\n\n\nWe will trim each read individually down to the good quality part to keep the bad part from interfering with downstream applications.\n\n\nTo do so, we will use sickle. Sickle is a tool that uses sliding windows along with quality and length thresholds to determine when quality is sufficiently low to trim the 3'-end of reads and also determines when the quality is sufficiently high enough to trim the 5'-end of reads. It will also discard reads based upon a length threshold.\n\n\nTo run sickle\n\n\nsickle pe -f SRR957824_adapt_R1.fastq -r SRR957824_adapt_R2.fastq \n\\\n\n    -t sanger -o SRR957824_trimmed_R1.fastq -p SRR957824_trimmed_R2.fastq \n\\\n\n    -s /dev/null -q \n25\n\n\n\n\n\n\nwhich should output something like\n\n\nPE forward file: SRR957824_trimmed_R1.fastq\nPE reverse file: SRR957824_trimmed_R2.fastq\n\nTotal input FastQ records: 1000000 (500000 pairs)\n\nFastQ paired records kept: 834570 (417285 pairs)\nFastQ single records kept: 13263 (from PE1: 11094, from PE2: 2169)\nFastQ paired records discarded: 138904 (69452 pairs)\nFastQ single records discarded: 13263 (from PE1: 2169, from PE2: 11094)\n\n\n\n\n\nFastQC again\n\u00b6\n\n\nRun fastqc again on the filtered reads\n\n\nfastqc SRR957824_trimmed_R1.fastq SRR957824_trimmed_R2.fastq\n\n\n\n\n\nand look at the reports\n\n\n\n\nSRR957824_trimmed_R1_fastqc.html\n\n\nSRR957824_trimmed_R2_fastqc.html\n\n\n\n\nMultiQC\n\u00b6\n\n\nMultiQC\n is a tool that aggreagtes results from several popular QC bioinformatics software into one html report.\n\n\nLet's run MultiQC in our current directory\n\n\nmultiqc .\n\n\n\n\n\nYou can download the report or view it by clickinh on the link below\n\n\n\n\nmultiqc_report.html\n\n\n\n\n\n\nQuestion\n\n\nWhat did the trimming do to the per-base sequence quality, the per sequence quality scores and the sequence length distribution?",
            "title": "Quality Control and Trimming"
        },
        {
            "location": "/qc/#quality-control-and-trimming",
            "text": "In this practical you will learn to import, view and check the quality of raw high thoughput sequencing sequencing data.  The first dataset you will be working with is from an Illumina MiSeq dataset.\nThe sequenced organism is an enterohaemorrhagic E. coli (EHEC) of the serotype O157, a potentially fatal gastrointestinal pathogen.\nThe sequenced bacterium was part of an outbreak investigation in the St. Louis area, USA in 2011.\nThe sequencing was done as paired-end 2x150bp.",
            "title": "Quality Control and Trimming"
        },
        {
            "location": "/qc/#downloading-the-data",
            "text": "The raw data were deposited at the European Nucleotide Archive, under the accession number SRR957824.\nYou could go to the ENA  website  and search for the run with the accession SRR957824.  However these files contain about 3 million reads and are therefore quite big.\nWe are only gonna use a subset of the original dataset for this tutorial.  First create a  data/  directory in your home folder  mkdir ~/data  now let's download the subset  cd  ~/data\ncurl -O -J -L https://osf.io/shqpv/download\ncurl -O -J -L https://osf.io/9m3ch/download  Let\u2019s make sure we downloaded all of our data using md5sum.  md5sum SRR957824_500K_R1.fastq.gz SRR957824_500K_R2.fastq.gz  you should see this  1e8cf249e3217a5a0bcc0d8a654585fb  SRR957824_500K_R1.fastq.gz\n70c726a31f05f856fe942d727613adb7  SRR957824_500K_R2.fastq.gz  and now look at the file names and their size  ls -l  total 97M\n-rw-r--r-- 1 hadrien 48M Nov 19 18:44 SRR957824_500K_R1.fastq.gz\n-rw-r--r-- 1 hadrien 50M Nov 19 18:53 SRR957824_500K_R2.fastq.gz  There are 500 000 paired-end reads taken randomly from the original data  One last thing before we get to the quality control: those files are writeable.\nBy default, UNIX makes things writeable by the file owner.\nThis poses an issue with creating typos or errors in raw data.\nWe fix that before going further  chmod u-w *",
            "title": "Downloading the data"
        },
        {
            "location": "/qc/#working-directory",
            "text": "First we make a work directory: a directory where we can play around with a copy of the data without messing with the original  mkdir ~/work cd  ~/work  Now we make a link of the data in our working directory  ln -s ~/data/* .  The files that we've downloaded are FASTQ files. Take a look at one of them with  zless SRR957824_500K_R1.fastq.gz   Tip  Use the spacebar to scroll down, and type \u2018q\u2019 to exit \u2018less\u2019   You can read more on the FASTQ format in the  File Formats  lesson.   Question  Where does the filename come from?    Question  Why are there 1 and 2 in the file names?",
            "title": "Working Directory"
        },
        {
            "location": "/qc/#fastqc",
            "text": "To check the quality of the sequence data we will use a tool called FastQC.  FastQC has a graphical interface and can be downloaded and run on a Windows or Linux computer without installation.\nIt is available  here .  However, FastQC is also available as a command line utility on the training server you are using.\nTo run FastQC on our two files  fastqc SRR957824_500K_R1.fastq.gz SRR957824_500K_R2.fastq.gz  and look what FastQC has produced  ls *fastqc*  For each file, FastQC has produced both a .zip archive containing all the plots, and a html report.  Download and open the html files with your favourite web browser.  Alternatively you can look a these copies of them:   SRR957824_500K_R1_fastqc.html  SRR957824_500K_R2_fastqc.html    Question  What should you pay attention to in the FastQC report?    Question  Which file is of better quality?   Pay special attention to the per base sequence quality and sequence length distribution.\nExplanations for the various quality modules can be found  here .\nAlso, have a look at examples of a  good  and a  bad  illumina read set for comparison.  You will note that the reads in your uploaded dataset have fairly poor quality (<20) towards the end. There are also outlier reads that have very poor quality for most of the second half of the reads.",
            "title": "FastQC"
        },
        {
            "location": "/qc/#scythe",
            "text": "Now we'll do some trimming!  Scythe uses a Naive Bayesian approach to classify contaminant substrings in sequence reads.\nIt considers quality information, which can make it robust in picking out 3'-end adapters, which often include poor quality bases.  The first thing we need is the adapters to trim off  curl -O -J -L https://osf.io/v24pt/download  Now we run scythe on both our read files  scythe -a adapters.fasta -o SRR957824_adapt_R1.fastq SRR957824_500K_R1.fastq.gz\nscythe -a adapters.fasta -o SRR957824_adapt_R2.fastq SRR957824_500K_R2.fastq.gz   Question  What adapters do you use?",
            "title": "Scythe"
        },
        {
            "location": "/qc/#sickle",
            "text": "Most modern sequencing technologies produce reads that have deteriorating quality towards the 3'-end and some towards the 5'-end as well.\nIncorrectly called bases in both regions negatively impact assembles, mapping, and downstream bioinformatics analyses.  We will trim each read individually down to the good quality part to keep the bad part from interfering with downstream applications.  To do so, we will use sickle. Sickle is a tool that uses sliding windows along with quality and length thresholds to determine when quality is sufficiently low to trim the 3'-end of reads and also determines when the quality is sufficiently high enough to trim the 5'-end of reads. It will also discard reads based upon a length threshold.  To run sickle  sickle pe -f SRR957824_adapt_R1.fastq -r SRR957824_adapt_R2.fastq  \\ \n    -t sanger -o SRR957824_trimmed_R1.fastq -p SRR957824_trimmed_R2.fastq  \\ \n    -s /dev/null -q  25   which should output something like  PE forward file: SRR957824_trimmed_R1.fastq\nPE reverse file: SRR957824_trimmed_R2.fastq\n\nTotal input FastQ records: 1000000 (500000 pairs)\n\nFastQ paired records kept: 834570 (417285 pairs)\nFastQ single records kept: 13263 (from PE1: 11094, from PE2: 2169)\nFastQ paired records discarded: 138904 (69452 pairs)\nFastQ single records discarded: 13263 (from PE1: 2169, from PE2: 11094)",
            "title": "Sickle"
        },
        {
            "location": "/qc/#fastqc-again",
            "text": "Run fastqc again on the filtered reads  fastqc SRR957824_trimmed_R1.fastq SRR957824_trimmed_R2.fastq  and look at the reports   SRR957824_trimmed_R1_fastqc.html  SRR957824_trimmed_R2_fastqc.html",
            "title": "FastQC again"
        },
        {
            "location": "/qc/#multiqc",
            "text": "MultiQC  is a tool that aggreagtes results from several popular QC bioinformatics software into one html report.  Let's run MultiQC in our current directory  multiqc .  You can download the report or view it by clickinh on the link below   multiqc_report.html    Question  What did the trimming do to the per-base sequence quality, the per sequence quality scores and the sequence length distribution?",
            "title": "MultiQC"
        },
        {
            "location": "/mapping/",
            "text": "Mapping and Variant Calling\n\u00b6\n\n\nIn this practical you will learn to map NGS reads to a reference sequence, check the output using a viewer software and investigate some aspects of the results.\nYou will be using the read data from the \nQuality Control\n practical.\n\n\nEHEC O157 strains generally carry a large virulence plasmid, pO157.\nPlasmids are circular genetic elements that many bacteria carry in addition to their chromosomes.\nThis particular plasmid encodes a number of proteins which are known or suspected to be involved in the ability to cause severe disease in infected humans.\nYour task in this practical is to map your prepared read set to a reference sequence of the virulence plasmid, to determine if the pO157 plasmid is present in the St. Louis outbreak strain.\n\n\n\n\n\n\nIllustration of plasmid integration into a host bacteria\n\n\n\n\nDownloading a Reference\n\u00b6\n\n\nYou will need a reference sequence to map your reads to.\n\n\ncd\n ~/work\ncurl -O -J -L https://osf.io/rnzbe/download\n\n\n\n\n\nThis file contains the sequence of the pO157 plasmid from the Sakai outbreak strain of E. coli O157.\nIn contrast to the strain we are working on, this strain is available as a finished genome, i.e. the whole sequence of both the single chromosome and the large virulence plasmid are known.\n\n\nIndexing the reference\n\u00b6\n\n\nBefore aligning the reads against a reference, it is necessary to build an index of that reference\n\n\nbowtie2-build pO157_Sakai.fasta.gz pO157_Sakai\n\n\n\n\n\n\n\nNote\n\n\nIndexing the reference is a necessary pre-processing step that makes searching for patterns much much faster. Many popular aligners such as Bowtie and BWA use an algorithm called the \nBurrows\u2013Wheeler transform\n to build the index.\n\n\n\n\nAligning reads\n\u00b6\n\n\nNow we are ready to map our reads\n\n\nbowtie2 -x pO157_Sakai -1 SRR957824_trimmed_R1.fastq.gz \n\\\n\n    -2 SRR957824_trimmed_R2.fastq.gz -S SRR957824.sam\n\n\n\n\n\nThe output of the mapping will be in the SAM format.\nYou can find a brief explanation of the SAM format \nhere\n\n\n\n\nNote\n\n\nIn this tutorial as well as many other places, you'll often see the terms \nmapping\n and \nalignment\n being used interchangeably. If you want to read more about the difference between the two, I invite you to read this excellent \nBiostars discussion\n\n\n\n\nVisualising with tview\n\u00b6\n\n\nhead SRR957824.sam\n\n\n\n\n\nBut it is not very informative.\nWe'll use samtools to visualise our data\n\n\nBefore downloading the data in tablet, we have to convert our SAM file into BAM, a compressed version of SAM that can be indexed.\n\n\nsamtools view -hSbo SRR957824.bam SRR957824.sam\n\n\n\n\n\nSort the bam file per position in the genome and index it\n\n\nsamtools sort SRR957824.bam SRR2584857.sorted.bam\nsamtools index SRR2584857.sorted.bam\n\n\n\n\n\nFinally we can visualise with \nsamtools tview\n\n\nsamtools tview SRR2584857.sorted.bam pO157_Sakai.fasta.gz\n\n\n\n\n\n\n\nTip\n\n\nnavigate in tview:\n\n    - left and right arrows scroll  \n\n    - \nq\n to quit \n\n    - \nCTRL-h\n and \nCTRL-l\n scrolls more\n\n    - \ng gi|10955266|ref|NC_002128.1|:8000\n will take you to a specific location.   \n\n\n\n\nVariant Calling\n\u00b6\n\n\nA frequent application for mapping reads is variant calling, i.e. finding positions where the reads are systematically different from the reference genome.\nSingle nucleotide polymorphism (SNP)-based typing is particularly popular and used for a broad range of applications.\nFor an EHEC O157 outbreak you could use it to identify the source, for instance.\n\n\nWe can call the variants using \nsamtools mpileup\n\n\nsamtools mpileup -uD -f pO157_Sakai.fasta.gz SRR2584857.sorted.bam \n|\n \n\\\n\n    bcftools view - > variants.vcf\n\n\n\n\n\nYou can read about the structure of vcf files \nhere\n.\nThe documentation is quite painful to read and take a look at the file\n\n\nLook at the non-commented lines\n\n\ngrep -v ^## variants.vcf\n\n\n\n\n\nThe first five columns are \nCHROM POS ID REF ALT\n.\n\n\nUse\n\n\ngrep -v ^## variants.vcf \n|\n less -S\n\n\n\n\n\nfor a better view.\n\n\n\n\nTip\n\n\nUse your left and right arrows to scroll horizontally, and \nq\n to quit.\n\n\n\n\n\n\nQuestion\n\n\nHow many SNPs did the variant caller find? Did you find any indels?\n\n\n\n\nExamine one of the variants with tview\n\n\nsamtools tview SRR2584857.sorted.bam pO157_Sakai.fasta.gz \n\\\n\n    -p \n'gi|10955266|ref|NC_002128.1|:43071'\n\n\n\n\n\n\nThat seems very real!\n\n\n\n\nQuestion\n\n\nWhere do reference genomes come from?",
            "title": "Mapping and Variant Calling"
        },
        {
            "location": "/mapping/#mapping-and-variant-calling",
            "text": "In this practical you will learn to map NGS reads to a reference sequence, check the output using a viewer software and investigate some aspects of the results.\nYou will be using the read data from the  Quality Control  practical.  EHEC O157 strains generally carry a large virulence plasmid, pO157.\nPlasmids are circular genetic elements that many bacteria carry in addition to their chromosomes.\nThis particular plasmid encodes a number of proteins which are known or suspected to be involved in the ability to cause severe disease in infected humans.\nYour task in this practical is to map your prepared read set to a reference sequence of the virulence plasmid, to determine if the pO157 plasmid is present in the St. Louis outbreak strain.    Illustration of plasmid integration into a host bacteria",
            "title": "Mapping and Variant Calling"
        },
        {
            "location": "/mapping/#downloading-a-reference",
            "text": "You will need a reference sequence to map your reads to.  cd  ~/work\ncurl -O -J -L https://osf.io/rnzbe/download  This file contains the sequence of the pO157 plasmid from the Sakai outbreak strain of E. coli O157.\nIn contrast to the strain we are working on, this strain is available as a finished genome, i.e. the whole sequence of both the single chromosome and the large virulence plasmid are known.",
            "title": "Downloading a Reference"
        },
        {
            "location": "/mapping/#indexing-the-reference",
            "text": "Before aligning the reads against a reference, it is necessary to build an index of that reference  bowtie2-build pO157_Sakai.fasta.gz pO157_Sakai   Note  Indexing the reference is a necessary pre-processing step that makes searching for patterns much much faster. Many popular aligners such as Bowtie and BWA use an algorithm called the  Burrows\u2013Wheeler transform  to build the index.",
            "title": "Indexing the reference"
        },
        {
            "location": "/mapping/#aligning-reads",
            "text": "Now we are ready to map our reads  bowtie2 -x pO157_Sakai -1 SRR957824_trimmed_R1.fastq.gz  \\ \n    -2 SRR957824_trimmed_R2.fastq.gz -S SRR957824.sam  The output of the mapping will be in the SAM format.\nYou can find a brief explanation of the SAM format  here   Note  In this tutorial as well as many other places, you'll often see the terms  mapping  and  alignment  being used interchangeably. If you want to read more about the difference between the two, I invite you to read this excellent  Biostars discussion",
            "title": "Aligning reads"
        },
        {
            "location": "/mapping/#visualising-with-tview",
            "text": "head SRR957824.sam  But it is not very informative.\nWe'll use samtools to visualise our data  Before downloading the data in tablet, we have to convert our SAM file into BAM, a compressed version of SAM that can be indexed.  samtools view -hSbo SRR957824.bam SRR957824.sam  Sort the bam file per position in the genome and index it  samtools sort SRR957824.bam SRR2584857.sorted.bam\nsamtools index SRR2584857.sorted.bam  Finally we can visualise with  samtools tview  samtools tview SRR2584857.sorted.bam pO157_Sakai.fasta.gz   Tip  navigate in tview: \n    - left and right arrows scroll   \n    -  q  to quit  \n    -  CTRL-h  and  CTRL-l  scrolls more \n    -  g gi|10955266|ref|NC_002128.1|:8000  will take you to a specific location.",
            "title": "Visualising with tview"
        },
        {
            "location": "/mapping/#variant-calling",
            "text": "A frequent application for mapping reads is variant calling, i.e. finding positions where the reads are systematically different from the reference genome.\nSingle nucleotide polymorphism (SNP)-based typing is particularly popular and used for a broad range of applications.\nFor an EHEC O157 outbreak you could use it to identify the source, for instance.  We can call the variants using  samtools mpileup  samtools mpileup -uD -f pO157_Sakai.fasta.gz SRR2584857.sorted.bam  |   \\ \n    bcftools view - > variants.vcf  You can read about the structure of vcf files  here .\nThe documentation is quite painful to read and take a look at the file  Look at the non-commented lines  grep -v ^## variants.vcf  The first five columns are  CHROM POS ID REF ALT .  Use  grep -v ^## variants.vcf  |  less -S  for a better view.   Tip  Use your left and right arrows to scroll horizontally, and  q  to quit.    Question  How many SNPs did the variant caller find? Did you find any indels?   Examine one of the variants with tview  samtools tview SRR2584857.sorted.bam pO157_Sakai.fasta.gz  \\ \n    -p  'gi|10955266|ref|NC_002128.1|:43071'   That seems very real!   Question  Where do reference genomes come from?",
            "title": "Variant Calling"
        },
        {
            "location": "/assembly/",
            "text": "De-novo Genome Assembly\n\u00b6\n\n\nIn this practical we will perform the assembly of \nM. genitalium\n, a bacterium published in 1995 by Fraser et al in Science (\nabstract link\n).\n\n\nGetting the data\n\u00b6\n\n\nM. genitalium\n was sequenced using the MiSeq platform (2 * 150bp).\nThe reads were deposited in the ENA Short Read Archive under the accession \nERR486840\n\n\nDownload the 2 fastq files associated with the run.\n\n\nwget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR486/ERR486840/ERR486840_1.fastq.gz\nwget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR486/ERR486840/ERR486840_2.fastq.gz\n\n\n\n\n\nThe files that were deposited in ENA were already trimmed, so we do not have to trim ourselves!\n\n\n\n\nQuestion\n\n\nHow many reads are in the files?\n\n\n\n\nDe-novo assembly\n\u00b6\n\n\nWe will be using the MEGAHIT assembler to assemble our bacterium\n\n\nmegahit -1 ERR486840_1.fastq.gz -2 ERR486840_2.fastq.gz -o m_genitalium\n\n\n\n\n\nThis will take a few minutes.\n\n\nThe result of the assembly is in the directory m_genitalium under the name \nfinal.contigs.fa\n\n\nLet's make a copy of it\n\n\ncp m_genitalium/final.contigs.fa m_genitalium.fasta\n\n\n\n\n\nand look at it\n\n\nhead m_genitalium.fasta\n\n\n\n\n\nQuality of the Assembly\n\u00b6\n\n\nQUAST is a software evaluating the quality of genome assemblies by computing various metrics, including\n\n\nRun Quast on your assembly\n\n\nquast.py m_genitalium.fasta -o m_genitalium_report\n\n\n\n\n\nand take a look at the text report\n\n\ncat m_genitalium_report/report.txt\n\n\n\n\n\nYou should see something like\n\n\nAll statistics are based on contigs of size >= 500 bp, unless otherwise noted (e.g., \"# contigs (>= 0 bp)\" and \"Total length (>= 0 bp)\" include all contigs).\n\nAssembly                    m_genitalium\n# contigs (>= 0 bp)         17          \n# contigs (>= 1000 bp)      8           \n# contigs (>= 5000 bp)      7           \n# contigs (>= 10000 bp)     6           \n# contigs (>= 25000 bp)     5           \n# contigs (>= 50000 bp)     2           \nTotal length (>= 0 bp)      584267      \nTotal length (>= 1000 bp)   580160      \nTotal length (>= 5000 bp)   577000      \nTotal length (>= 10000 bp)  570240      \nTotal length (>= 25000 bp)  554043      \nTotal length (>= 50000 bp)  446481      \n# contigs                   11          \nLargest contig              368542      \nTotal length                582257      \nGC (%)                      31.71       \nN50                         368542      \nN75                         77939       \nL50                         1           \nL75                         2           \n# N's per 100 kbp           0.00    \n\n\n\n\n\nwhich is a summary stats about our assembly.\nAdditionally, the file \nm_genitalium_report/report.html\n\n\nYou can either download it and open it in your own web browser, or we make it available for your convenience:\n\n\n\n\nm_genitalium_report/report.html\n\n\n\n\n\n\nNote\n\n\nN50: length for which the collection of all contigs of that length or longer covers at least 50% of assembly length\n\n\n\n\n\n\nQuestion\n\n\nHow well does the assembly total consensus size and coverage correspond to your earlier estimation?\n\n\n\n\n\n\nQuestion\n\n\nHow many contigs in total did the assembly produce?\n\n\n\n\n\n\nQuestion\n\n\nWhat is the N50 of the assembly? What does this mean?\n\n\n\n\nFixing misassemblies\n\u00b6\n\n\nPilon is a software tool which can be used to automatically improve draft assemblies.\nIt attempts to make improvements to the input genome, including:\n\n\n\n\nSingle base differences\n\n\nSmall Indels\n\n\nLarger Indels or block substitution events\n\n\nGap filling\n\n\nIdentification of local misassemblies, including optional opening of new gaps\n\n\n\n\nPilon then outputs a FASTA file containing an improved representation of the genome from the read data and an optional VCF file detailing variation seen between the read data and the input genome.\n\n\nBefore running Pilon itself, we have to align our reads against the assembly\n\n\nbowtie2-build m_genitalium.fasta m_genitalium\nbowtie2 -x m_genitalium -1 ERR486840_1.fastq.gz -2 ERR486840_2.fastq.gz | \\\n    samtools view -bS -o m_genitalium.bam\nsamtools sort m_genitalium.bam -o m_genitalium.sorted.bam\nsamtools index m_genitalium.sorted.bam\n\n\n\n\n\nthen we run Pilon\n\n\npilon --genome m_genitalium.fasta --frags m_genitalium.sorted.bam --output m_genitalium_improved\n\n\n\n\n\nwhich will correct eventual misamatches in our assembly and write the new improved assembly to \nm_genitalium_improved.fasta",
            "title": "De-novo Genome Assembly"
        },
        {
            "location": "/assembly/#de-novo-genome-assembly",
            "text": "In this practical we will perform the assembly of  M. genitalium , a bacterium published in 1995 by Fraser et al in Science ( abstract link ).",
            "title": "De-novo Genome Assembly"
        },
        {
            "location": "/assembly/#getting-the-data",
            "text": "M. genitalium  was sequenced using the MiSeq platform (2 * 150bp).\nThe reads were deposited in the ENA Short Read Archive under the accession  ERR486840  Download the 2 fastq files associated with the run.  wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR486/ERR486840/ERR486840_1.fastq.gz\nwget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR486/ERR486840/ERR486840_2.fastq.gz  The files that were deposited in ENA were already trimmed, so we do not have to trim ourselves!   Question  How many reads are in the files?",
            "title": "Getting the data"
        },
        {
            "location": "/assembly/#de-novo-assembly",
            "text": "We will be using the MEGAHIT assembler to assemble our bacterium  megahit -1 ERR486840_1.fastq.gz -2 ERR486840_2.fastq.gz -o m_genitalium  This will take a few minutes.  The result of the assembly is in the directory m_genitalium under the name  final.contigs.fa  Let's make a copy of it  cp m_genitalium/final.contigs.fa m_genitalium.fasta  and look at it  head m_genitalium.fasta",
            "title": "De-novo assembly"
        },
        {
            "location": "/assembly/#quality-of-the-assembly",
            "text": "QUAST is a software evaluating the quality of genome assemblies by computing various metrics, including  Run Quast on your assembly  quast.py m_genitalium.fasta -o m_genitalium_report  and take a look at the text report  cat m_genitalium_report/report.txt  You should see something like  All statistics are based on contigs of size >= 500 bp, unless otherwise noted (e.g., \"# contigs (>= 0 bp)\" and \"Total length (>= 0 bp)\" include all contigs).\n\nAssembly                    m_genitalium\n# contigs (>= 0 bp)         17          \n# contigs (>= 1000 bp)      8           \n# contigs (>= 5000 bp)      7           \n# contigs (>= 10000 bp)     6           \n# contigs (>= 25000 bp)     5           \n# contigs (>= 50000 bp)     2           \nTotal length (>= 0 bp)      584267      \nTotal length (>= 1000 bp)   580160      \nTotal length (>= 5000 bp)   577000      \nTotal length (>= 10000 bp)  570240      \nTotal length (>= 25000 bp)  554043      \nTotal length (>= 50000 bp)  446481      \n# contigs                   11          \nLargest contig              368542      \nTotal length                582257      \nGC (%)                      31.71       \nN50                         368542      \nN75                         77939       \nL50                         1           \nL75                         2           \n# N's per 100 kbp           0.00      which is a summary stats about our assembly.\nAdditionally, the file  m_genitalium_report/report.html  You can either download it and open it in your own web browser, or we make it available for your convenience:   m_genitalium_report/report.html    Note  N50: length for which the collection of all contigs of that length or longer covers at least 50% of assembly length    Question  How well does the assembly total consensus size and coverage correspond to your earlier estimation?    Question  How many contigs in total did the assembly produce?    Question  What is the N50 of the assembly? What does this mean?",
            "title": "Quality of the Assembly"
        },
        {
            "location": "/assembly/#fixing-misassemblies",
            "text": "Pilon is a software tool which can be used to automatically improve draft assemblies.\nIt attempts to make improvements to the input genome, including:   Single base differences  Small Indels  Larger Indels or block substitution events  Gap filling  Identification of local misassemblies, including optional opening of new gaps   Pilon then outputs a FASTA file containing an improved representation of the genome from the read data and an optional VCF file detailing variation seen between the read data and the input genome.  Before running Pilon itself, we have to align our reads against the assembly  bowtie2-build m_genitalium.fasta m_genitalium\nbowtie2 -x m_genitalium -1 ERR486840_1.fastq.gz -2 ERR486840_2.fastq.gz | \\\n    samtools view -bS -o m_genitalium.bam\nsamtools sort m_genitalium.bam -o m_genitalium.sorted.bam\nsamtools index m_genitalium.sorted.bam  then we run Pilon  pilon --genome m_genitalium.fasta --frags m_genitalium.sorted.bam --output m_genitalium_improved  which will correct eventual misamatches in our assembly and write the new improved assembly to  m_genitalium_improved.fasta",
            "title": "Fixing misassemblies"
        },
        {
            "location": "/annotation/",
            "text": "Genome Annotation\n\u00b6\n\n\nAfter you have de novo assembled your genome sequencing reads into contigs, it is useful to know what genomic features are on those contigs. The process of identifying and labelling those features is called genome annotation.\n\n\nProkka is a \u201cwrapper\u201d; it collects together several pieces of software (from various authors), and so avoids \u201cre-inventing the wheel\u201d.\n\n\nProkka finds and annotates features (both protein coding regions and RNA genes, i.e. tRNA, rRNA) present on on a sequence. Prokka uses a two-step process for the annotation of protein coding regions: first, protein coding regions on the genome are identified using \nProdigal\n; second, the function of the encoded protein is predicted by similarity to proteins in one of many protein or protein domain databases. Prokka is a software tool that can be used to annotate bacterial, archaeal and viral genomes quickly, generating standard output files in GenBank, EMBL and gff formats. More information about Prokka can be found \nhere\n.\n\n\nInput data\n\u00b6\n\n\nProkka requires assembled contigs. You will need your best assembly from the assembly tutorial.\n\n\nAlternatively, you can download an assembly \nhere\n\n\nRunning prokka\n\u00b6\n\n\nmodule\n \nload\n \nprokka\n\n\nawk\n \n'/^>/{print \">ctg\" ++i; next}{print}'\n < \nassembly\n.\nfasta\n > \ngood_contigs\n.\nfasta\n\n\nprokka\n --\noutdir\n \nannotation\n --\nkingdom\n \nBacteria\n \\\n--\nproteins\n \nm_genitalium\n.\nfaa\n \ngood_contigs\n.\nfasta\n\n\n\n\n\n\nOnce Prokka has finished, examine each of its output files.\n\n\n\n\nThe GFF and GBK files contain all of the information about the features annotated (in different formats.)\n\n\nThe .txt file contains a summary of the number of features annotated.\n\n\nThe .faa file contains the protein sequences of the genes annotated.\n\n\nThe .ffn file contains the nucleotide sequences of the genes annotated.\n\n\n\n\nVisualising the annotation\n\u00b6\n\n\nArtemis is a graphical Java program to browse annotated genomes. Download it \nhere\n and install it on your local computer.\n\n\nCopy the .gff file produced by prokka on your computer, and open it with artemis.\n\n\nYou will be overwhelmed and/or confused at first, and possibly permanently. Here are some tips:\n\n\n\n\nThere are 3 panels: feature map (top), sequence (middle), feature list (bottom)\n\n\nClick right-mouse-button on bottom panel and select Show products\n\n\nZooming is done via the verrtical scroll bars in the two top panels",
            "title": "Genome Annotation"
        },
        {
            "location": "/annotation/#genome-annotation",
            "text": "After you have de novo assembled your genome sequencing reads into contigs, it is useful to know what genomic features are on those contigs. The process of identifying and labelling those features is called genome annotation.  Prokka is a \u201cwrapper\u201d; it collects together several pieces of software (from various authors), and so avoids \u201cre-inventing the wheel\u201d.  Prokka finds and annotates features (both protein coding regions and RNA genes, i.e. tRNA, rRNA) present on on a sequence. Prokka uses a two-step process for the annotation of protein coding regions: first, protein coding regions on the genome are identified using  Prodigal ; second, the function of the encoded protein is predicted by similarity to proteins in one of many protein or protein domain databases. Prokka is a software tool that can be used to annotate bacterial, archaeal and viral genomes quickly, generating standard output files in GenBank, EMBL and gff formats. More information about Prokka can be found  here .",
            "title": "Genome Annotation"
        },
        {
            "location": "/annotation/#input-data",
            "text": "Prokka requires assembled contigs. You will need your best assembly from the assembly tutorial.  Alternatively, you can download an assembly  here",
            "title": "Input data"
        },
        {
            "location": "/annotation/#running-prokka",
            "text": "module   load   prokka  awk   '/^>/{print \">ctg\" ++i; next}{print}'  <  assembly . fasta  >  good_contigs . fasta  prokka  -- outdir   annotation  -- kingdom   Bacteria  \\\n-- proteins   m_genitalium . faa   good_contigs . fasta   Once Prokka has finished, examine each of its output files.   The GFF and GBK files contain all of the information about the features annotated (in different formats.)  The .txt file contains a summary of the number of features annotated.  The .faa file contains the protein sequences of the genes annotated.  The .ffn file contains the nucleotide sequences of the genes annotated.",
            "title": "Running prokka"
        },
        {
            "location": "/annotation/#visualising-the-annotation",
            "text": "Artemis is a graphical Java program to browse annotated genomes. Download it  here  and install it on your local computer.  Copy the .gff file produced by prokka on your computer, and open it with artemis.  You will be overwhelmed and/or confused at first, and possibly permanently. Here are some tips:   There are 3 panels: feature map (top), sequence (middle), feature list (bottom)  Click right-mouse-button on bottom panel and select Show products  Zooming is done via the verrtical scroll bars in the two top panels",
            "title": "Visualising the annotation"
        },
        {
            "location": "/pan_genome/",
            "text": "Pan-Genome Analysis\n\u00b6\n\n\nIn this tutorial we will learn how to determine a pan-genome from a collection of isolate genomes.\n\n\nThis tutorial is inspired from \nGenome annotation and Pangenome Analysis\n from the CBIB in Santiago, Chile\n\n\nGetting the data\n\u00b6\n\n\nWe'll use six \nListeria monocytogenes\n genomes in this tutorial.\n\n\nwget https://github.com/HadrienG/tutorials/blob/master/data/pangenome.tar.gz\ntar xzf pangenome.tar.gz\n\ncd\n pangenome\n\n\n\n\n\nThese genomes correspond to isolates of \nL. monocytogenes\n reported in\n\n\n\n\nXiangyu Deng, Adam M Phillippy, Zengxin Li, Steven L Salzberg and Wei Zhang. (2010) Probing the pan-genome of Listeria monocytogenes: new insights into intraspecific niche expansion and genomic diversification. doi:10.1186/1471-2164-11-500\n\n\n\n\nThe six genomes you downloaded were selected based on their level of completeness (finished; contigs, etc) and their genotype (type I-III):\n\n\n\n\n\n\n\n\nGenome Assembly\n\n\nGenome Accession\n\n\nGenotype\n\n\nSequenced by\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\nGCA_000026705\n\n\nFM242711\n\n\ntype I\n\n\nInstitut_Pasteur\n\n\nFinished\n\n\n\n\n\n\nGCA_000008285\n\n\nAE017262\n\n\ntype I\n\n\nTIGR\n\n\nFinished\n\n\n\n\n\n\nGCA_000168815\n\n\nAATL00000000\n\n\ntype I\n\n\nBroad Institute\n\n\n79 contigs\n\n\n\n\n\n\nGCA_000196035\n\n\nAL591824\n\n\ntype II\n\n\nEuropean Consortium\n\n\nFinished\n\n\n\n\n\n\nGCA_000168635\n\n\nAARW00000000\n\n\ntype II\n\n\nBroad Institute\n\n\n25 contigs\n\n\n\n\n\n\nGCA_000021185\n\n\nCP001175\n\n\ntype III\n\n\nMSU\n\n\nFinished\n\n\n\n\n\n\n\n\nAnnotation of the genomes\n\u00b6\n\n\nBy annotating the genomes we mean to add information regarding genes, their location, strandedness, and features and attributes. Now that you have the genomes, we need to annotate them to determine the location and attributes of the genes contained in them. We will use Prokka for the annotation.\n\n\nprokka --kingdom Bacteria --outdir prokka_GCA_000008285 --genus Listeria --locustag GCA_000008285 GCA_000008285.1_ASM828v1_genomic.fna\n\n\n\n\n\nAnnotate the 6 genomes, by replacing the \n-outdir\n and \n-locustag\n and \nfasta file\n accordingly.\n\n\nPan-genome analysis\n\u00b6\n\n\nput all the .gff files in the same folder (e.g., ./gff) and run Roary\n\n\nroary -f results -e -n -v gff/*.gff\n\n\nRoary will get all the coding sequences, convert them into protein, and create pre-clusters. Then, using BLASTP and MCL, Roary will create clusters, and check for paralogs. Finally, Roary will take every isolate and order them by presence/absence of orthologs. The summary output is present in the \nsummary_statistics.txt\n file.\n\n\nAdditionally, Roary produces a \ngene_presence_absence.csv\n file that can be opened in any spreadsheet software to manually explore the results. In this file, you will find information such as gene name and gene annotation, and, of course, whether a gene is present in a genome or not.\n\n\nPlotting the result\n\u00b6\n\n\nRoary comes with a python script that allows you to generate a few plots to graphically assess your analysis output.\n\n\nFirst, we need to generate a tree file from the alignment generated by Roary:\n\n\nFastTree \u2013nt \u2013gtr core_gene_alignment.aln > my_tree.newick\n\n\n\n\n\nThen we can plot the Roary results:\n\n\nroary_plots.py my_tree.newick gene_presence_absence.csv",
            "title": "Pan-Genome Analysis"
        },
        {
            "location": "/pan_genome/#pan-genome-analysis",
            "text": "In this tutorial we will learn how to determine a pan-genome from a collection of isolate genomes.  This tutorial is inspired from  Genome annotation and Pangenome Analysis  from the CBIB in Santiago, Chile",
            "title": "Pan-Genome Analysis"
        },
        {
            "location": "/pan_genome/#getting-the-data",
            "text": "We'll use six  Listeria monocytogenes  genomes in this tutorial.  wget https://github.com/HadrienG/tutorials/blob/master/data/pangenome.tar.gz\ntar xzf pangenome.tar.gz cd  pangenome  These genomes correspond to isolates of  L. monocytogenes  reported in   Xiangyu Deng, Adam M Phillippy, Zengxin Li, Steven L Salzberg and Wei Zhang. (2010) Probing the pan-genome of Listeria monocytogenes: new insights into intraspecific niche expansion and genomic diversification. doi:10.1186/1471-2164-11-500   The six genomes you downloaded were selected based on their level of completeness (finished; contigs, etc) and their genotype (type I-III):     Genome Assembly  Genome Accession  Genotype  Sequenced by  Status      GCA_000026705  FM242711  type I  Institut_Pasteur  Finished    GCA_000008285  AE017262  type I  TIGR  Finished    GCA_000168815  AATL00000000  type I  Broad Institute  79 contigs    GCA_000196035  AL591824  type II  European Consortium  Finished    GCA_000168635  AARW00000000  type II  Broad Institute  25 contigs    GCA_000021185  CP001175  type III  MSU  Finished",
            "title": "Getting the data"
        },
        {
            "location": "/pan_genome/#annotation-of-the-genomes",
            "text": "By annotating the genomes we mean to add information regarding genes, their location, strandedness, and features and attributes. Now that you have the genomes, we need to annotate them to determine the location and attributes of the genes contained in them. We will use Prokka for the annotation.  prokka --kingdom Bacteria --outdir prokka_GCA_000008285 --genus Listeria --locustag GCA_000008285 GCA_000008285.1_ASM828v1_genomic.fna  Annotate the 6 genomes, by replacing the  -outdir  and  -locustag  and  fasta file  accordingly.",
            "title": "Annotation of the genomes"
        },
        {
            "location": "/pan_genome/#pan-genome-analysis_1",
            "text": "put all the .gff files in the same folder (e.g., ./gff) and run Roary  roary -f results -e -n -v gff/*.gff  Roary will get all the coding sequences, convert them into protein, and create pre-clusters. Then, using BLASTP and MCL, Roary will create clusters, and check for paralogs. Finally, Roary will take every isolate and order them by presence/absence of orthologs. The summary output is present in the  summary_statistics.txt  file.  Additionally, Roary produces a  gene_presence_absence.csv  file that can be opened in any spreadsheet software to manually explore the results. In this file, you will find information such as gene name and gene annotation, and, of course, whether a gene is present in a genome or not.",
            "title": "Pan-genome analysis"
        },
        {
            "location": "/pan_genome/#plotting-the-result",
            "text": "Roary comes with a python script that allows you to generate a few plots to graphically assess your analysis output.  First, we need to generate a tree file from the alignment generated by Roary:  FastTree \u2013nt \u2013gtr core_gene_alignment.aln > my_tree.newick  Then we can plot the Roary results:  roary_plots.py my_tree.newick gene_presence_absence.csv",
            "title": "Plotting the result"
        },
        {
            "location": "/16S/",
            "text": "Metabarcoding\n\u00b6\n\n\nThis tutorial is aimed at being a walkthrough of the DADA2 pipeline.\nIt uses the data of the now famous \nMiSeq SOP\n by the Mothur authors but analyses the data using DADA2.\n\n\nDADA2 is a relatively new method to analyse amplicon data which uses exact variants instead of OTUs.    \n\n\nThe advantages of the DADA2 method is described in the \npaper\n\n\nBefore Starting\n\u00b6\n\n\nThere are two ways to follow this tutorial: you can copy and paste all the codes blocks below in R directly, or you can download this document in the Rmarkdown format and execute the cells.\n\n\n\n\nLink to the document in Rmarkdown\n\n\n\n\nInstall and Load Packages\n\u00b6\n\n\nFirst install DADA2 and other necessary packages\n\n\nsource\n(\n'https://bioconductor.org/biocLite.R'\n)\n\nbiocLite\n(\n'dada2'\n)\n\nbiocLite\n(\n'phyloseq'\n)\n\nbiocLite\n(\n'DECIPHER'\n)\n\ninstall.packages\n(\n'ggplot2'\n)\n\ninstall.packages\n(\n'phangorn'\n)\n\n\n\n\n\n\nNow load the packages and verify you have the correct DADA2 version\n\n\nlibrary\n(\ndada2\n)\n\n\nlibrary\n(\nggplot2\n)\n\n\nlibrary\n(\nphyloseq\n)\n\n\nlibrary\n(\nphangorn\n)\n\n\nlibrary\n(\nDECIPHER\n)\n\npackageVersion\n(\n'dada2'\n)\n\n\n\n\n\n\nDownload the Data\n\u00b6\n\n\nYou will also need to download the data, as well as the SILVA database\n\n\n\n\nWarning\n\n\nIf you are following the tutorial on the website, the following block of commands has to be executed outside of R.\nIf you run this tutorial with the R notebook, you can simply execute the cell block\n\n\n\n\nwget http://www.mothur.org/w/images/d/d6/MiSeqSOPData.zip\nunzip MiSeqSOPData.zip\nrm -r __MACOSX/\n\ncd\n MiSeq_SOP\nwget https://zenodo.org/record/824551/files/silva_nr_v128_train_set.fa.gz\nwget https://zenodo.org/record/824551/files/silva_species_assignment_v128.fa.gz\n\ncd\n ..\n\n\n\n\n\nBack in R, check that you have downloaded the data\n\n\npath \n<-\n \n'MiSeq_SOP'\n\n\nlist.files\n(\npath\n)\n\n\n\n\n\n\nFiltering and Trimming\n\u00b6\n\n\nFirst we create two lists with the sorted name of the reads: one for the forward reads, one for the reverse reads\n\n\nraw_forward \n<-\n \nsort\n(\nlist.files\n(\npath\n,\n pattern\n=\n\"_R1_001.fastq\"\n,\n\n                               full.names\n=\nTRUE\n))\n\n\nraw_reverse \n<-\n \nsort\n(\nlist.files\n(\npath\n,\n pattern\n=\n\"_R2_001.fastq\"\n,\n\n                               full.names\n=\nTRUE\n))\n\n\n\n# we also need the sample names\n\nsample_names \n<-\n \nsapply\n(\nstrsplit\n(\nbasename\n(\nraw_forward\n),\n \n\"_\"\n),\n\n                       \n`[`\n,\n  \n# extracts the first element of a subset\n\n                       \n1\n)\n\n\n\n\n\n\nthen we visualise the quality of our reads\n\n\nplotQualityProfile\n(\nraw_forward\n[\n1\n:\n2\n])\n\nplotQualityProfile\n(\nraw_reverse\n[\n1\n:\n2\n])\n\n\n\n\n\n\n\n\nQuestion\n\n\nWhat do you think of the read quality?\n\n\n\n\nThe forward reads are good quality (although dropping a bit at the end as usual) while the reverse are way worse.\n\n\nBased on these profiles, we will truncate the forward reads at position 240 and the reverse reads at position 160 where the quality distribution crashes.\n\n\n\n\nNote\n\n\nin this tutorial we perform the trimming using DADA2's own functions.\nIf you wish to do it outside of DADA2, you can refer to the \nQuality Control tutorial\n\n\n\n\n\n\nDada2 requires us to define the name of our output files\n\n\n# place filtered files in filtered/ subdirectory\n\nfiltered_path \n<-\n \nfile.path\n(\npath\n,\n \n\"filtered\"\n)\n\n\nfiltered_forward \n<-\n \nfile.path\n(\nfiltered_path\n,\n\n                              \npaste0\n(\nsample_names\n,\n \n\"_R1_trimmed.fastq.gz\"\n))\n\n\nfiltered_reverse \n<-\n \nfile.path\n(\nfiltered_path\n,\n\n                              \npaste0\n(\nsample_names\n,\n \n\"_R2_trimmed.fastq.gz\"\n))\n\n\n\n\n\n\nWe\u2019ll use standard filtering parameters: \nmaxN=0\n (DADA22 requires no Ns), \ntruncQ=2\n, \nrm.phix=TRUE\n and \nmaxEE=2\n.\nThe maxEE parameter sets the maximum number of \u201cexpected errors\u201d allowed in a read, which \naccording to the USEARCH authors\n is a better filter than simply averaging quality scores.\n\n\nout \n<-\n filterAndTrim\n(\nraw_forward\n,\n filtered_forward\n,\n raw_reverse\n,\n\n                     filtered_reverse\n,\n truncLen\n=\nc\n(\n240\n,\n160\n),\n maxN\n=\n0\n,\n\n                     maxEE\n=\nc\n(\n2\n,\n2\n),\n truncQ\n=\n2\n,\n rm.phix\n=\nTRUE\n,\n compress\n=\nTRUE\n,\n\n                     multithread\n=\nTRUE\n)\n\n\nhead\n(\nout\n)\n\n\n\n\n\n\nLearn the Error Rates\n\u00b6\n\n\nThe DADA2 algorithm depends on a parametric error model and every amplicon dataset has a slightly different error rate.\nThe \nlearnErrors\n of Dada2 learns the error model from the data and will help DADA2 to fits its method to your data\n\n\nerrors_forward \n<-\n learnErrors\n(\nfiltered_forward\n,\n multithread\n=\nTRUE\n)\n\nerrors_reverse \n<-\n learnErrors\n(\nfiltered_reverse\n,\n multithread\n=\nTRUE\n)\n\n\n\n\n\n\nthen we visualise the estimated error rates\n\n\nplotErrors\n(\nerrors_forward\n,\n nominalQ\n=\nTRUE\n)\n \n+\n\n    theme_minimal\n()\n\n\n\n\n\n\n\n\nQuestion\n\n\nDo you think the error model fits your data correctly?\n\n\n\n\nDereplication\n\u00b6\n\n\nFrom the Dada2 documentation:\n\n\n\n\nDereplication\n combines all identical sequencing reads into into \u201cunique sequences\u201d with a corresponding \u201cabundance\u201d: the number of reads with that unique sequence.\n\nDereplication\n substantially reduces computation time by eliminating redundant comparisons.\n\n\n\n\nderep_forward \n<-\n derepFastq\n(\nfiltered_forward\n,\n verbose\n=\nTRUE\n)\n\nderep_reverse \n<-\n derepFastq\n(\nfiltered_reverse\n,\n verbose\n=\nTRUE\n)\n\n\n# name the derep-class objects by the sample names\n\n\nnames\n(\nderep_forward\n)\n \n<-\n sample_names\n\nnames\n(\nderep_reverse\n)\n \n<-\n sample_names\n\n\n\n\n\nSample inference\n\u00b6\n\n\nWe are now ready to apply the core sequence-variant inference algorithm to the dereplicated data.\n\n\ndada_forward \n<-\n dada\n(\nderep_forward\n,\n err\n=\nerrors_forward\n,\n multithread\n=\nTRUE\n)\n\ndada_reverse \n<-\n dada\n(\nderep_reverse\n,\n err\n=\nerrors_reverse\n,\n multithread\n=\nTRUE\n)\n\n\n\n# inspect the dada-class object\n\ndada_forward\n[[\n1\n]]\n\n\n\n\n\n\nThe DADA2 algorithm inferred 128 real sequence variants from the 1979 unique sequences in the first sample.\n\n\nMerge Paired-end Reads\n\u00b6\n\n\nNow that the reads are trimmed, dereplicated and error-corrected we can merge them together\n\n\nmerged_reads \n<-\n mergePairs\n(\ndada_forward\n,\n derep_forward\n,\n dada_reverse\n,\n\n                           derep_reverse\n,\n verbose\n=\nTRUE\n)\n\n\n\n# inspect the merger data.frame from the first sample\n\n\nhead\n(\nmerged_reads\n[[\n1\n]])\n\n\n\n\n\n\nConstruct Sequence Table\n\u00b6\n\n\nWe can now construct a sequence table of our mouse samples, a higher-resolution version of the OTU table produced by traditional methods.\n\n\nseq_table \n<-\n makeSequenceTable\n(\nmerged_reads\n)\n\n\ndim\n(\nseq_table\n)\n\n\n\n# inspect distribution of sequence lengths\n\n\ntable\n(\nnchar\n(\ngetSequences\n(\nseq_table\n)))\n\n\n\n\n\n\nRemove Chimeras\n\u00b6\n\n\nThe \ndada\n method used earlier removes substitutions and indel errors but chimeras remain.\nWe remove the chimeras with\n\n\nseq_table_nochim \n<-\n removeBimeraDenovo\n(\nseq_table\n,\n method\n=\n'consensus'\n,\n\n                                       multithread\n=\nTRUE\n,\n verbose\n=\nTRUE\n)\n\n\ndim\n(\nseq_table_nochim\n)\n\n\n\n# which percentage of our reads did we keep?\n\n\nsum\n(\nseq_table_nochim\n)\n \n/\n \nsum\n(\nseq_table\n)\n\n\n\n\n\n\nAs a final check of our progress, we\u2019ll look at the number of reads that made it through each step in the pipeline\n\n\nget_n \n<-\n \nfunction\n(\nx\n)\n \nsum\n(\ngetUniques\n(\nx\n))\n\n\ntrack \n<-\n \ncbind\n(\nout\n,\n \nsapply\n(\ndada_forward\n,\n get_n\n),\n \nsapply\n(\nmerged_reads\n,\n get_n\n),\n\n               \nrowSums\n(\nseq_table\n),\n \nrowSums\n(\nseq_table_nochim\n))\n\n\n\ncolnames\n(\ntrack\n)\n \n<-\n \nc\n(\n'input'\n,\n \n'filtered'\n,\n \n'denoised'\n,\n \n'merged'\n,\n \n'tabled'\n,\n\n                     \n'nonchim'\n)\n\n\nrownames\n(\ntrack\n)\n \n<-\n sample_names\n\nhead\n(\ntrack\n)\n\n\n\n\n\n\nWe kept the majority of our reads!\n\n\nAssign Taxonomy\n\u00b6\n\n\nNow we assign taxonomy to our sequences using the SILVA database\n\n\ntaxa \n<-\n assignTaxonomy\n(\nseq_table_nochim\n,\n\n                       \n'MiSeq_SOP/silva_nr_v128_train_set.fa.gz'\n,\n\n                       multithread\n=\nTRUE\n)\n\ntaxa \n<-\n addSpecies\n(\ntaxa\n,\n \n'MiSeq_SOP/silva_species_assignment_v128.fa.gz'\n)\n\n\n\n\n\n\nfor inspecting the classification\n\n\ntaxa_print \n<-\n taxa  \n# removing sequence rownames for display only\n\n\nrownames\n(\ntaxa_print\n)\n \n<-\n \nNULL\n\n\nhead\n(\ntaxa_print\n)\n\n\n\n\n\n\nPhylogenetic Tree\n\u00b6\n\n\nDADA2 is reference-free so we have to build the tree ourselves\n\n\nWe first align our sequences\n\n\nsequences \n<-\n getSequences\n(\nseq_table\n)\n\n\nnames\n(\nsequences\n)\n \n<-\n sequences  \n# this propagates to the tip labels of the tree\n\nalignment \n<-\n AlignSeqs\n(\nDNAStringSet\n(\nsequences\n),\n anchor\n=\nNA\n)\n\n\n\n\n\n\nThen we build a neighbour-joining tree then fit a maximum likelihood tree using the neighbour-joining tree as a starting point\n\n\nphang_align \n<-\n phyDat\n(\nas\n(\nalignment\n,\n \n'matrix'\n),\n type\n=\n'DNA'\n)\n\ndm \n<-\n dist.ml\n(\nphang_align\n)\n\ntreeNJ \n<-\n NJ\n(\ndm\n)\n  \n# note, tip order != sequence order\n\nfit \n=\n pml\n(\ntreeNJ\n,\n data\n=\nphang_align\n)\n\n\n\n## negative edges length changed to 0!\n\n\nfitGTR \n<-\n update\n(\nfit\n,\n k\n=\n4\n,\n inv\n=\n0.2\n)\n\nfitGTR \n<-\n optim.pml\n(\nfitGTR\n,\n model\n=\n'GTR'\n,\n optInv\n=\nTRUE\n,\n optGamma\n=\nTRUE\n,\n\n                    rearrangement \n=\n \n'stochastic'\n,\n\n                    control \n=\n pml.control\n(\ntrace \n=\n \n0\n))\n\n\ndetach\n(\n'package:phangorn'\n,\n unload\n=\nTRUE\n)\n\n\n\n\n\n\nPhyloseq\n\u00b6\n\n\nFirst load the metadata\n\n\nsample_data \n<-\n read.table\n(\n\n    \n'https://hadrieng.github.io/tutorials/data/16S_metadata.txt'\n,\n\n    header\n=\nTRUE\n,\n row.names\n=\n\"sample_name\"\n)\n\n\n\n\n\n\nWe can now construct a phyloseq object from our output and newly created metadata\n\n\nphyseq \n<-\n phyloseq\n(\notu_table\n(\nseq_table_nochim\n,\n taxa_are_rows\n=\nFALSE\n),\n\n                   sample_data\n(\nsample_data\n),\n\n                   tax_table\n(\ntaxa\n),\n\n                   phy_tree\n(\nfitGTR\n$\ntree\n))\n\n\n# remove mock sample\n\nphyseq \n<-\n prune_samples\n(\nsample_names\n(\nphyseq\n)\n \n!=\n \n'Mock'\n,\n physeq\n)\n\nphyseq\n\n\n\n\n\nLet's look at the alpha diversity of our samples\n\n\nplot_richness\n(\nphyseq\n,\n x\n=\n'day'\n,\n measures\n=\nc\n(\n'Shannon'\n,\n \n'Fisher'\n),\n color\n=\n'when'\n)\n \n+\n\n    theme_minimal\n()\n\n\n\n\n\n\nNo obvious differences. Let's look at ordination methods (beta diversity)\n\n\nWe can perform an MDS with euclidean distance (mathematically equivalent to a PCA)\n\n\nord \n<-\n ordinate\n(\nphyseq\n,\n \n'MDS'\n,\n \n'euclidean'\n)\n\nplot_ordination\n(\nphyseq\n,\n ord\n,\n type\n=\n'samples'\n,\n color\n=\n'when'\n,\n\n                title\n=\n'PCA of the samples from the MiSeq SOP'\n)\n \n+\n\n    theme_minimal\n()\n\n\n\n\n\n\nnow with the Bray-Curtis distance\n\n\nord \n<-\n ordinate\n(\nphyseq\n,\n \n'NMDS'\n,\n \n'bray'\n)\n\nplot_ordination\n(\nphyseq\n,\n ord\n,\n type\n=\n'samples'\n,\n color\n=\n'when'\n,\n\n                title\n=\n'PCA of the samples from the MiSeq SOP'\n)\n \n+\n\n    theme_minimal\n()\n\n\n\n\n\n\nThere we can see a clear difference between our samples.\n\n\nLet us take a look a the distribution of the most abundant families\n\n\ntop20 \n<-\n \nnames\n(\nsort\n(\ntaxa_sums\n(\nphyseq\n),\n decreasing\n=\nTRUE\n))[\n1\n:\n20\n]\n\nphyseq_top20 \n<-\n transform_sample_counts\n(\nphyseq\n,\n \nfunction\n(\nOTU\n)\n OTU\n/\nsum\n(\nOTU\n))\n\nphyseq_top20 \n<-\n prune_taxa\n(\ntop20\n,\n physeq_top20\n)\n\nplot_bar\n(\nphyseq_top20\n,\n x\n=\n'day'\n,\n fill\n=\n'Family'\n)\n \n+\n\n    facet_wrap\n(\n~\nwhen\n,\n scales\n=\n'free_x'\n)\n \n+\n\n    theme_minimal\n()\n\n\n\n\n\n\nWe can place them in a tree\n\n\nbacteroidetes \n<-\n subset_taxa\n(\nphyseq\n,\n Phylum \n%in%\n \nc\n(\n'Bacteroidetes'\n))\n\nplot_tree\n(\nbacteroidetes\n,\n ladderize\n=\n'left'\n,\n size\n=\n'abundance'\n,\n\n          color\n=\n'when'\n,\n label.tips\n=\n'Family'\n)",
            "title": "Metabarcoding"
        },
        {
            "location": "/16S/#metabarcoding",
            "text": "This tutorial is aimed at being a walkthrough of the DADA2 pipeline.\nIt uses the data of the now famous  MiSeq SOP  by the Mothur authors but analyses the data using DADA2.  DADA2 is a relatively new method to analyse amplicon data which uses exact variants instead of OTUs.      The advantages of the DADA2 method is described in the  paper",
            "title": "Metabarcoding"
        },
        {
            "location": "/16S/#before-starting",
            "text": "There are two ways to follow this tutorial: you can copy and paste all the codes blocks below in R directly, or you can download this document in the Rmarkdown format and execute the cells.   Link to the document in Rmarkdown",
            "title": "Before Starting"
        },
        {
            "location": "/16S/#install-and-load-packages",
            "text": "First install DADA2 and other necessary packages  source ( 'https://bioconductor.org/biocLite.R' ) \nbiocLite ( 'dada2' ) \nbiocLite ( 'phyloseq' ) \nbiocLite ( 'DECIPHER' ) \ninstall.packages ( 'ggplot2' ) \ninstall.packages ( 'phangorn' )   Now load the packages and verify you have the correct DADA2 version  library ( dada2 )  library ( ggplot2 )  library ( phyloseq )  library ( phangorn )  library ( DECIPHER ) \npackageVersion ( 'dada2' )",
            "title": "Install and Load Packages"
        },
        {
            "location": "/16S/#download-the-data",
            "text": "You will also need to download the data, as well as the SILVA database   Warning  If you are following the tutorial on the website, the following block of commands has to be executed outside of R.\nIf you run this tutorial with the R notebook, you can simply execute the cell block   wget http://www.mothur.org/w/images/d/d6/MiSeqSOPData.zip\nunzip MiSeqSOPData.zip\nrm -r __MACOSX/ cd  MiSeq_SOP\nwget https://zenodo.org/record/824551/files/silva_nr_v128_train_set.fa.gz\nwget https://zenodo.org/record/824551/files/silva_species_assignment_v128.fa.gz cd  ..  Back in R, check that you have downloaded the data  path  <-   'MiSeq_SOP'  list.files ( path )",
            "title": "Download the Data"
        },
        {
            "location": "/16S/#filtering-and-trimming",
            "text": "First we create two lists with the sorted name of the reads: one for the forward reads, one for the reverse reads  raw_forward  <-   sort ( list.files ( path ,  pattern = \"_R1_001.fastq\" , \n                               full.names = TRUE )) \n\nraw_reverse  <-   sort ( list.files ( path ,  pattern = \"_R2_001.fastq\" , \n                               full.names = TRUE ))  # we also need the sample names \nsample_names  <-   sapply ( strsplit ( basename ( raw_forward ),   \"_\" ), \n                        `[` ,    # extracts the first element of a subset \n                        1 )   then we visualise the quality of our reads  plotQualityProfile ( raw_forward [ 1 : 2 ]) \nplotQualityProfile ( raw_reverse [ 1 : 2 ])    Question  What do you think of the read quality?   The forward reads are good quality (although dropping a bit at the end as usual) while the reverse are way worse.  Based on these profiles, we will truncate the forward reads at position 240 and the reverse reads at position 160 where the quality distribution crashes.   Note  in this tutorial we perform the trimming using DADA2's own functions.\nIf you wish to do it outside of DADA2, you can refer to the  Quality Control tutorial    Dada2 requires us to define the name of our output files  # place filtered files in filtered/ subdirectory \nfiltered_path  <-   file.path ( path ,   \"filtered\" ) \n\nfiltered_forward  <-   file.path ( filtered_path , \n                               paste0 ( sample_names ,   \"_R1_trimmed.fastq.gz\" )) \n\nfiltered_reverse  <-   file.path ( filtered_path , \n                               paste0 ( sample_names ,   \"_R2_trimmed.fastq.gz\" ))   We\u2019ll use standard filtering parameters:  maxN=0  (DADA22 requires no Ns),  truncQ=2 ,  rm.phix=TRUE  and  maxEE=2 .\nThe maxEE parameter sets the maximum number of \u201cexpected errors\u201d allowed in a read, which  according to the USEARCH authors  is a better filter than simply averaging quality scores.  out  <-  filterAndTrim ( raw_forward ,  filtered_forward ,  raw_reverse , \n                     filtered_reverse ,  truncLen = c ( 240 , 160 ),  maxN = 0 , \n                     maxEE = c ( 2 , 2 ),  truncQ = 2 ,  rm.phix = TRUE ,  compress = TRUE , \n                     multithread = TRUE )  head ( out )",
            "title": "Filtering and Trimming"
        },
        {
            "location": "/16S/#learn-the-error-rates",
            "text": "The DADA2 algorithm depends on a parametric error model and every amplicon dataset has a slightly different error rate.\nThe  learnErrors  of Dada2 learns the error model from the data and will help DADA2 to fits its method to your data  errors_forward  <-  learnErrors ( filtered_forward ,  multithread = TRUE ) \nerrors_reverse  <-  learnErrors ( filtered_reverse ,  multithread = TRUE )   then we visualise the estimated error rates  plotErrors ( errors_forward ,  nominalQ = TRUE )   + \n    theme_minimal ()    Question  Do you think the error model fits your data correctly?",
            "title": "Learn the Error Rates"
        },
        {
            "location": "/16S/#dereplication",
            "text": "From the Dada2 documentation:   Dereplication  combines all identical sequencing reads into into \u201cunique sequences\u201d with a corresponding \u201cabundance\u201d: the number of reads with that unique sequence. Dereplication  substantially reduces computation time by eliminating redundant comparisons.   derep_forward  <-  derepFastq ( filtered_forward ,  verbose = TRUE ) \nderep_reverse  <-  derepFastq ( filtered_reverse ,  verbose = TRUE )  # name the derep-class objects by the sample names  names ( derep_forward )   <-  sample_names names ( derep_reverse )   <-  sample_names",
            "title": "Dereplication"
        },
        {
            "location": "/16S/#sample-inference",
            "text": "We are now ready to apply the core sequence-variant inference algorithm to the dereplicated data.  dada_forward  <-  dada ( derep_forward ,  err = errors_forward ,  multithread = TRUE ) \ndada_reverse  <-  dada ( derep_reverse ,  err = errors_reverse ,  multithread = TRUE )  # inspect the dada-class object \ndada_forward [[ 1 ]]   The DADA2 algorithm inferred 128 real sequence variants from the 1979 unique sequences in the first sample.",
            "title": "Sample inference"
        },
        {
            "location": "/16S/#merge-paired-end-reads",
            "text": "Now that the reads are trimmed, dereplicated and error-corrected we can merge them together  merged_reads  <-  mergePairs ( dada_forward ,  derep_forward ,  dada_reverse , \n                           derep_reverse ,  verbose = TRUE )  # inspect the merger data.frame from the first sample  head ( merged_reads [[ 1 ]])",
            "title": "Merge Paired-end Reads"
        },
        {
            "location": "/16S/#construct-sequence-table",
            "text": "We can now construct a sequence table of our mouse samples, a higher-resolution version of the OTU table produced by traditional methods.  seq_table  <-  makeSequenceTable ( merged_reads )  dim ( seq_table )  # inspect distribution of sequence lengths  table ( nchar ( getSequences ( seq_table )))",
            "title": "Construct Sequence Table"
        },
        {
            "location": "/16S/#remove-chimeras",
            "text": "The  dada  method used earlier removes substitutions and indel errors but chimeras remain.\nWe remove the chimeras with  seq_table_nochim  <-  removeBimeraDenovo ( seq_table ,  method = 'consensus' , \n                                       multithread = TRUE ,  verbose = TRUE )  dim ( seq_table_nochim )  # which percentage of our reads did we keep?  sum ( seq_table_nochim )   /   sum ( seq_table )   As a final check of our progress, we\u2019ll look at the number of reads that made it through each step in the pipeline  get_n  <-   function ( x )   sum ( getUniques ( x )) \n\ntrack  <-   cbind ( out ,   sapply ( dada_forward ,  get_n ),   sapply ( merged_reads ,  get_n ), \n                rowSums ( seq_table ),   rowSums ( seq_table_nochim ))  colnames ( track )   <-   c ( 'input' ,   'filtered' ,   'denoised' ,   'merged' ,   'tabled' , \n                      'nonchim' )  rownames ( track )   <-  sample_names head ( track )   We kept the majority of our reads!",
            "title": "Remove Chimeras"
        },
        {
            "location": "/16S/#assign-taxonomy",
            "text": "Now we assign taxonomy to our sequences using the SILVA database  taxa  <-  assignTaxonomy ( seq_table_nochim , \n                        'MiSeq_SOP/silva_nr_v128_train_set.fa.gz' , \n                       multithread = TRUE ) \ntaxa  <-  addSpecies ( taxa ,   'MiSeq_SOP/silva_species_assignment_v128.fa.gz' )   for inspecting the classification  taxa_print  <-  taxa   # removing sequence rownames for display only  rownames ( taxa_print )   <-   NULL  head ( taxa_print )",
            "title": "Assign Taxonomy"
        },
        {
            "location": "/16S/#phylogenetic-tree",
            "text": "DADA2 is reference-free so we have to build the tree ourselves  We first align our sequences  sequences  <-  getSequences ( seq_table )  names ( sequences )   <-  sequences   # this propagates to the tip labels of the tree \nalignment  <-  AlignSeqs ( DNAStringSet ( sequences ),  anchor = NA )   Then we build a neighbour-joining tree then fit a maximum likelihood tree using the neighbour-joining tree as a starting point  phang_align  <-  phyDat ( as ( alignment ,   'matrix' ),  type = 'DNA' ) \ndm  <-  dist.ml ( phang_align ) \ntreeNJ  <-  NJ ( dm )    # note, tip order != sequence order \nfit  =  pml ( treeNJ ,  data = phang_align )  ## negative edges length changed to 0! \n\nfitGTR  <-  update ( fit ,  k = 4 ,  inv = 0.2 ) \nfitGTR  <-  optim.pml ( fitGTR ,  model = 'GTR' ,  optInv = TRUE ,  optGamma = TRUE , \n                    rearrangement  =   'stochastic' , \n                    control  =  pml.control ( trace  =   0 ))  detach ( 'package:phangorn' ,  unload = TRUE )",
            "title": "Phylogenetic Tree"
        },
        {
            "location": "/16S/#phyloseq",
            "text": "First load the metadata  sample_data  <-  read.table ( \n     'https://hadrieng.github.io/tutorials/data/16S_metadata.txt' , \n    header = TRUE ,  row.names = \"sample_name\" )   We can now construct a phyloseq object from our output and newly created metadata  physeq  <-  phyloseq ( otu_table ( seq_table_nochim ,  taxa_are_rows = FALSE ), \n                   sample_data ( sample_data ), \n                   tax_table ( taxa ), \n                   phy_tree ( fitGTR $ tree ))  # remove mock sample \nphyseq  <-  prune_samples ( sample_names ( physeq )   !=   'Mock' ,  physeq ) \nphyseq  Let's look at the alpha diversity of our samples  plot_richness ( physeq ,  x = 'day' ,  measures = c ( 'Shannon' ,   'Fisher' ),  color = 'when' )   + \n    theme_minimal ()   No obvious differences. Let's look at ordination methods (beta diversity)  We can perform an MDS with euclidean distance (mathematically equivalent to a PCA)  ord  <-  ordinate ( physeq ,   'MDS' ,   'euclidean' ) \nplot_ordination ( physeq ,  ord ,  type = 'samples' ,  color = 'when' , \n                title = 'PCA of the samples from the MiSeq SOP' )   + \n    theme_minimal ()   now with the Bray-Curtis distance  ord  <-  ordinate ( physeq ,   'NMDS' ,   'bray' ) \nplot_ordination ( physeq ,  ord ,  type = 'samples' ,  color = 'when' , \n                title = 'PCA of the samples from the MiSeq SOP' )   + \n    theme_minimal ()   There we can see a clear difference between our samples.  Let us take a look a the distribution of the most abundant families  top20  <-   names ( sort ( taxa_sums ( physeq ),  decreasing = TRUE ))[ 1 : 20 ] \nphyseq_top20  <-  transform_sample_counts ( physeq ,   function ( OTU )  OTU / sum ( OTU )) \nphyseq_top20  <-  prune_taxa ( top20 ,  physeq_top20 ) \nplot_bar ( physeq_top20 ,  x = 'day' ,  fill = 'Family' )   + \n    facet_wrap ( ~ when ,  scales = 'free_x' )   + \n    theme_minimal ()   We can place them in a tree  bacteroidetes  <-  subset_taxa ( physeq ,  Phylum  %in%   c ( 'Bacteroidetes' )) \nplot_tree ( bacteroidetes ,  ladderize = 'left' ,  size = 'abundance' , \n          color = 'when' ,  label.tips = 'Family' )",
            "title": "Phyloseq"
        },
        {
            "location": "/wms/",
            "text": "Whole Metagenome Sequencin\n\u00b6\n\n\nTable of Contents\n\u00b6\n\n\n\n\nIntroduction\n\n\nThe Pig Microbiome\n\n\nWhole Metagenome Sequencing\n\n\n\n\n\n\nSoftwares Required for this Tutorial\n\n\nGetting the Data and Checking their Quality\n\n\nTaxonomic Classification\n\n\nVisualization\n\n\n\n\nIntroduction\n\u00b6\n\n\nMicrobiome used\n\u00b6\n\n\nIn this tutorial we will compare samples from the Pig Gut Microbiome to samples from the Human Gut Microbiome. Below you'll find a brief description of the two projects:\n\n\nThe Pig Microbiome:\n\n\n\n\nPig is a main species for livestock and biomedicine. The pig genome sequence was recently reported. To boost research, we established a catalogue of the genes of the gut microbiome based on faecal samples of 287 pigs from France, Denmark and China. More than 7.6 million non-redundant genes representing 719 metagenomic species were identified by deep metagenome sequencing, highlighting more similarities with the human than with the mouse catalogue. The pig and human catalogues share only 12.6 and 9.3 % of their genes, respectively, but 70 and 95% of their functional pathways. The pig gut microbiota is influenced by gender, age and breed. Analysis of the prevalence of antibiotics resistance genes (ARGs) reflected antibiotics supplementation in each farm system, and revealed that non-antibiotics-fed animals still harbour ARGs. The pig catalogue creates a resource for whole metagenomics-based studies, highly valuable for research in biomedicine and for sustainable knowledge-based pig farming\n\n\n\n\nThe Human Microbiome:\n\n\n\n\nWe are facing a global metabolic health crisis provoked by an obesity epidemic. Here we report the human gut microbial composition in a population sample of 123 non-obese and 169 obese Danish individuals. We find two groups of individuals that differ by the number of gut microbial genes and thus gut bacterial richness. They harbour known and previously unknown bacterial species at different proportions; individuals with a low bacterial richness (23% of the population) are characterized by more marked overall adiposity, insulin resistance and dyslipidaemia and a more pronounced inflammatory phenotype when compared with high bacterial richness individuals. The obese individuals among the former also gain more weight over time. Only a few bacterial species are sufficient to distinguish between individuals with high and low bacterial richness, and even between lean and obese. Our classifications based on variation in the gut microbiome identify subsets of individuals in the general white adult population who may be at increased risk of progressing to adiposity-associated co-morbidities\n\n\n\n\nWhole Metagenome Sequencing\n\u00b6\n\n\nWhole Metagenome sequencing (WMS), or shotgun metagenome sequencing, is a relatively new and powerful sequencing approach that provides insight into community biodiversity and function. On the contrary of Metabarcoding, where only a specific region of the bacterial community (the 16s rRNA) is sequenced, WMS aims at sequencing all the genomic material present in the environment.\n\n\nThe choice of shotgun or 16S approaches is usually dictated by the nature of the studies being conducted. For instance, 16S is well suited for analysis of large number of samples, i.e., multiple patients, longitudinal studies, etc. but offers limited taxonomical and functional resolution. WMS is generally more expensive but offers increased resolution, and allows the discovery of viruses as well as other mobile genetic elements.\n\n\nSoftwares Required for this Tutorial\n\u00b6\n\n\n\n\nFastQC\n\n\nKraken\n\n\nBracken\n\n\nR\n\n\nPavian\n\n\n\n\nGetting the Data and Checking their Quality\n\u00b6\n\n\nIf you are reading this tutorial online and haven't cloned the directory, first download and unpack the data:\n\n\nwget http://77.235.253.14/metlab/wms.tar\ntar xvf wms.tar\ncd wms\n\n\n\n\n\nWe'll use FastQC to check the quality of our data. FastQC can be downloaded and\nrun on a Windows or Linux computer without installation. It is available \nhere\n\n\nStart FastQC and select the fastq file you just downloaded with \nfile -> open\n\nWhat do you think about the quality of the reads? Do they need trimming? Are there still adapters\npresent? Overrepresented sequences?\n\n\nAlternatively, run fastqc on the command-line:\n\n\nfastqc *.fastq.gz\n\n\nIf the quality appears to be good, it's because it was probably the cleaned reads that were deposited into SRA.\nWe can directly move to the classification step.\n\n\nTaxonomic Classification\n\u00b6\n\n\nKraken\n is a system for assigning taxonomic labels to short DNA sequences (i.e. reads)\n\nKraken aims to achieve high sensitivity and high speed by utilizing exact alignments of k-mers and a novel classification algorithm (sic).\n\n\nIn short, kraken uses a new approach with exact k-mer matching to assign taxonomy to short reads. It is \nextremely\n fast compared to traditional\napproaches (i.e. BLAST).\n\n\nBy default, the authors of kraken built their database based on RefSeq Bacteria, Archea and Viruses. We'll use it for the purpose of this tutorial.\n\n\nNOTE: The database may have been installed already! Ask your instructor!\n\n\n# You might not need this step (for example if you're working on Uppmax!)\n\nwget https://ccb.jhu.edu/software/kraken/dl/minikraken.tgz\ntar xzf minikraken.tgz\n\n$KRAKEN_DB\n=\nminikraken_20141208\n\n\n\n\n\nNow run kraken on the reads\n\n\nmkdir kraken_results\n\nfor\n i in *_1.fastq.gz\n\ndo\n\n    \nprefix\n=\n$(\nbasename \n$i\n _1.fastq.gz\n)\n\n    \n# set number of threads to number of cores if running under SLURM, otherwise use 2 threads\n\n    \nnthreads\n=\n${\nSLURM_NPROCS\n:=2\n}\n\n    kraken --db \n$KRAKEN_DB\n --threads \n${\nnthreads\n}\n --fastq-input --gzip-compressed \n\\\n\n        \n${\nprefix\n}\n_1.fastq.gz \n${\nprefix\n}\n_2.fastq.gz > kraken_results/\n${\nprefix\n}\n.tab\n    kraken-report --db \n$KRAKEN_DB\n \n\\\n\n        kraken_results/\n${\nprefix\n}\n.tab > kraken_results/\n${\nprefix\n}\n_tax.txt\n\ndone\n\n\n\n\n\n\nwhich produces a tab-delimited file with an assigned TaxID for each read.\n\n\nKraken includes a script called \nkraken-report\n to transform this file into a \"tree\" view with the percentage of reads assigned to each taxa. We've run this script at each step in the loop. Take a look at the \n_tax.txt\n files!\n\n\nAbundance estimation using Bracken\n\u00b6\n\n\nBracken (Bayesian Reestimation of Abundance with KrakEN) is a highly accurate statistical method that computes the abundance of species in DNA sequences from a metagenomics sample\n\n\nBefore starting, you need to install Bracken:\n\n\ncd\n\ngit clone https://github.com/jenniferlu717/Bracken.git\nchmod \n755\n Bracken/*.py\nchmod \n755\n Bracken/*.pl\n\nexport\n \nPATH\n=\n$PATH\n:\n$HOME\n/Bracken\n\n\n\n\n\nUnfortunately, Uppmax lacks some perl packages necessary for Bracken to work:\n\n\nFollow the tutorial \nhere\n to install \ncpanm\n\n\nthen install the two perl libraries that are missing:\n\n\ncpanm Parallel::ForkManager\ncpanm List::MoreUtils\n\n\n\n\n\nThree steps are necessary to set up Kraken abundance estimation.\n\n\n\n\n\n\nClassify all reads using Kraken and Generate a Kraken report file. We've done this!\n\n\n\n\n\n\nSearch all library input sequences against the database and compute the classifications for each perfect read of ${READ_LENGTH} base pairs from one of the input sequences.\n\n\n\n\n\n\nfind -L \n$KRAKEN_DB\n/library -name \n\"*.fna\"\n -o -name \n\"*.fa\"\n -o -name \n\"*.fasta\"\n > genomes.list\ncat \n$(\ngrep -v \n'^#'\n genomes.list\n)\n > genomes.fasta\nkraken --db\n=\n${\nKRAKEN_DB\n}\n --fasta-input --threads\n=\n${\nSLURM_NPROCS\n:=10\n}\n kraken.fasta > database.kraken\ncount-kmer-abundances.pl --db\n=\n${\nKRAKEN_DB\n}\n --read-length\n=\n100\n database.kraken > database100mers.kraken_cnts\n\n\n\n\n\n\n\nGenerate the kmer distribution file\n\n\n\n\npython generate_kmer_distribution.py -i database100mers.kraken_cnts -o KMER_DISTR.TXT\n\n\n\n\n\nNow, given the expected kmer distribution for genomes in a kraken database along\nwith a kraken report file, the number of reads belonging to each species (or\ngenus) is estimated using the estimate_abundance.py file, run with the\nfollowing command line:\n\n\npython estimate_abundance.py -i KRAKEN.REPORT -k KMER_DISTR.TXT -o OUTPUT_FILE.TXT\n\n\nRun this command for the six \n_tax.txt\n files that you generated with kraken!\n\n\nThe following required parameters must be specified:\n- KRAKEN.REPORT     :: the kraken report generated for a given dataset\n- KMER_DISTR.TXT    :: the file generated by generate_kmer_distribution.py\n- OUTPUT_FILE.TXT   :: the desired name of the output file to be generated by the code\n\n\nVisualization\n\u00b6\n\n\nAlternative 1: Pavian\n\u00b6\n\n\nPavian is a web application for exploring metagenomics classification results.\n\n\nInstall and run Pavian:\n\n\n(In R or Rstudio)\n\n\n## Installs required packages from CRAN and Bioconductor\n\n\nsource\n(\n\"https://raw.githubusercontent.com/fbreitwieser/pavian/master/inst/shinyapp/install-pavian.R\"\n)\n\npavian\n::\nrunApp\n(\nport\n=\n5000\n)\n\n\n\n\n\n\nPavian will be available at http://127.0.0.1:5000",
            "title": "Whole Metagenome Sequencing"
        },
        {
            "location": "/wms/#whole-metagenome-sequencin",
            "text": "",
            "title": "Whole Metagenome Sequencin"
        },
        {
            "location": "/wms/#table-of-contents",
            "text": "Introduction  The Pig Microbiome  Whole Metagenome Sequencing    Softwares Required for this Tutorial  Getting the Data and Checking their Quality  Taxonomic Classification  Visualization",
            "title": "Table of Contents"
        },
        {
            "location": "/wms/#introduction",
            "text": "",
            "title": "Introduction"
        },
        {
            "location": "/wms/#microbiome-used",
            "text": "In this tutorial we will compare samples from the Pig Gut Microbiome to samples from the Human Gut Microbiome. Below you'll find a brief description of the two projects:  The Pig Microbiome:   Pig is a main species for livestock and biomedicine. The pig genome sequence was recently reported. To boost research, we established a catalogue of the genes of the gut microbiome based on faecal samples of 287 pigs from France, Denmark and China. More than 7.6 million non-redundant genes representing 719 metagenomic species were identified by deep metagenome sequencing, highlighting more similarities with the human than with the mouse catalogue. The pig and human catalogues share only 12.6 and 9.3 % of their genes, respectively, but 70 and 95% of their functional pathways. The pig gut microbiota is influenced by gender, age and breed. Analysis of the prevalence of antibiotics resistance genes (ARGs) reflected antibiotics supplementation in each farm system, and revealed that non-antibiotics-fed animals still harbour ARGs. The pig catalogue creates a resource for whole metagenomics-based studies, highly valuable for research in biomedicine and for sustainable knowledge-based pig farming   The Human Microbiome:   We are facing a global metabolic health crisis provoked by an obesity epidemic. Here we report the human gut microbial composition in a population sample of 123 non-obese and 169 obese Danish individuals. We find two groups of individuals that differ by the number of gut microbial genes and thus gut bacterial richness. They harbour known and previously unknown bacterial species at different proportions; individuals with a low bacterial richness (23% of the population) are characterized by more marked overall adiposity, insulin resistance and dyslipidaemia and a more pronounced inflammatory phenotype when compared with high bacterial richness individuals. The obese individuals among the former also gain more weight over time. Only a few bacterial species are sufficient to distinguish between individuals with high and low bacterial richness, and even between lean and obese. Our classifications based on variation in the gut microbiome identify subsets of individuals in the general white adult population who may be at increased risk of progressing to adiposity-associated co-morbidities",
            "title": "Microbiome used"
        },
        {
            "location": "/wms/#whole-metagenome-sequencing",
            "text": "Whole Metagenome sequencing (WMS), or shotgun metagenome sequencing, is a relatively new and powerful sequencing approach that provides insight into community biodiversity and function. On the contrary of Metabarcoding, where only a specific region of the bacterial community (the 16s rRNA) is sequenced, WMS aims at sequencing all the genomic material present in the environment.  The choice of shotgun or 16S approaches is usually dictated by the nature of the studies being conducted. For instance, 16S is well suited for analysis of large number of samples, i.e., multiple patients, longitudinal studies, etc. but offers limited taxonomical and functional resolution. WMS is generally more expensive but offers increased resolution, and allows the discovery of viruses as well as other mobile genetic elements.",
            "title": "Whole Metagenome Sequencing"
        },
        {
            "location": "/wms/#softwares-required-for-this-tutorial",
            "text": "FastQC  Kraken  Bracken  R  Pavian",
            "title": "Softwares Required for this Tutorial"
        },
        {
            "location": "/wms/#getting-the-data-and-checking-their-quality",
            "text": "If you are reading this tutorial online and haven't cloned the directory, first download and unpack the data:  wget http://77.235.253.14/metlab/wms.tar\ntar xvf wms.tar\ncd wms  We'll use FastQC to check the quality of our data. FastQC can be downloaded and\nrun on a Windows or Linux computer without installation. It is available  here  Start FastQC and select the fastq file you just downloaded with  file -> open \nWhat do you think about the quality of the reads? Do they need trimming? Are there still adapters\npresent? Overrepresented sequences?  Alternatively, run fastqc on the command-line:  fastqc *.fastq.gz  If the quality appears to be good, it's because it was probably the cleaned reads that were deposited into SRA.\nWe can directly move to the classification step.",
            "title": "Getting the Data and Checking their Quality"
        },
        {
            "location": "/wms/#taxonomic-classification",
            "text": "Kraken  is a system for assigning taxonomic labels to short DNA sequences (i.e. reads) \nKraken aims to achieve high sensitivity and high speed by utilizing exact alignments of k-mers and a novel classification algorithm (sic).  In short, kraken uses a new approach with exact k-mer matching to assign taxonomy to short reads. It is  extremely  fast compared to traditional\napproaches (i.e. BLAST).  By default, the authors of kraken built their database based on RefSeq Bacteria, Archea and Viruses. We'll use it for the purpose of this tutorial.  NOTE: The database may have been installed already! Ask your instructor!  # You might not need this step (for example if you're working on Uppmax!) \nwget https://ccb.jhu.edu/software/kraken/dl/minikraken.tgz\ntar xzf minikraken.tgz $KRAKEN_DB = minikraken_20141208  Now run kraken on the reads  mkdir kraken_results for  i in *_1.fastq.gz do \n     prefix = $( basename  $i  _1.fastq.gz ) \n     # set number of threads to number of cores if running under SLURM, otherwise use 2 threads \n     nthreads = ${ SLURM_NPROCS :=2 } \n    kraken --db  $KRAKEN_DB  --threads  ${ nthreads }  --fastq-input --gzip-compressed  \\ \n         ${ prefix } _1.fastq.gz  ${ prefix } _2.fastq.gz > kraken_results/ ${ prefix } .tab\n    kraken-report --db  $KRAKEN_DB   \\ \n        kraken_results/ ${ prefix } .tab > kraken_results/ ${ prefix } _tax.txt done   which produces a tab-delimited file with an assigned TaxID for each read.  Kraken includes a script called  kraken-report  to transform this file into a \"tree\" view with the percentage of reads assigned to each taxa. We've run this script at each step in the loop. Take a look at the  _tax.txt  files!",
            "title": "Taxonomic Classification"
        },
        {
            "location": "/wms/#abundance-estimation-using-bracken",
            "text": "Bracken (Bayesian Reestimation of Abundance with KrakEN) is a highly accurate statistical method that computes the abundance of species in DNA sequences from a metagenomics sample  Before starting, you need to install Bracken:  cd \ngit clone https://github.com/jenniferlu717/Bracken.git\nchmod  755  Bracken/*.py\nchmod  755  Bracken/*.pl export   PATH = $PATH : $HOME /Bracken  Unfortunately, Uppmax lacks some perl packages necessary for Bracken to work:  Follow the tutorial  here  to install  cpanm  then install the two perl libraries that are missing:  cpanm Parallel::ForkManager\ncpanm List::MoreUtils  Three steps are necessary to set up Kraken abundance estimation.    Classify all reads using Kraken and Generate a Kraken report file. We've done this!    Search all library input sequences against the database and compute the classifications for each perfect read of ${READ_LENGTH} base pairs from one of the input sequences.    find -L  $KRAKEN_DB /library -name  \"*.fna\"  -o -name  \"*.fa\"  -o -name  \"*.fasta\"  > genomes.list\ncat  $( grep -v  '^#'  genomes.list )  > genomes.fasta\nkraken --db = ${ KRAKEN_DB }  --fasta-input --threads = ${ SLURM_NPROCS :=10 }  kraken.fasta > database.kraken\ncount-kmer-abundances.pl --db = ${ KRAKEN_DB }  --read-length = 100  database.kraken > database100mers.kraken_cnts   Generate the kmer distribution file   python generate_kmer_distribution.py -i database100mers.kraken_cnts -o KMER_DISTR.TXT  Now, given the expected kmer distribution for genomes in a kraken database along\nwith a kraken report file, the number of reads belonging to each species (or\ngenus) is estimated using the estimate_abundance.py file, run with the\nfollowing command line:  python estimate_abundance.py -i KRAKEN.REPORT -k KMER_DISTR.TXT -o OUTPUT_FILE.TXT  Run this command for the six  _tax.txt  files that you generated with kraken!  The following required parameters must be specified:\n- KRAKEN.REPORT     :: the kraken report generated for a given dataset\n- KMER_DISTR.TXT    :: the file generated by generate_kmer_distribution.py\n- OUTPUT_FILE.TXT   :: the desired name of the output file to be generated by the code",
            "title": "Abundance estimation using Bracken"
        },
        {
            "location": "/wms/#visualization",
            "text": "",
            "title": "Visualization"
        },
        {
            "location": "/wms/#alternative-1-pavian",
            "text": "Pavian is a web application for exploring metagenomics classification results.  Install and run Pavian:  (In R or Rstudio)  ## Installs required packages from CRAN and Bioconductor  source ( \"https://raw.githubusercontent.com/fbreitwieser/pavian/master/inst/shinyapp/install-pavian.R\" ) \npavian :: runApp ( port = 5000 )   Pavian will be available at http://127.0.0.1:5000",
            "title": "Alternative 1: Pavian"
        },
        {
            "location": "/meta_assembly/",
            "text": "Metagenome assembly\n\u00b6\n\n\nIn this tutorial you'll learn how to inspect the quality of High-throughput sequencing and\nperform a metagenomic assembly.\n\n\nWe will use data under the accession SRS018585 in the Sequence Read Archive. this sample is\n\"a Human Metagenome sample from G_DNA_Anterior nares of a male participant in the dbGaP study\nHMP Core Microbiome Sampling Protocol A (HMP-A)\"\n\n\nTable of Contents\n\u00b6\n\n\n\n\nSoftwares Required for this Tutorial\n\n\nGetting the Data\n\n\nQuality Control\n\n\nAssembly\n\n\nTaxonomic Classification and Visualization\n\n\n\n\nSoftwares Required for this Tutorial\n\u00b6\n\n\n\n\nFastQC\n\n\nsickle\n\n\nSPAdes\n\n\nBlast\n\n\nblobtools\n\n\n\n\nGetting the Data\n\u00b6\n\n\nwget http://downloads.hmpdacc.org/data/Illumina/anterior_nares/SRS018585.tar.bz2\ntar xjf SRS018585.tar.bz2\ncd SRS018585\n\n\n\n\n\nQuality Control\n\u00b6\n\n\nwe'll use FastQC to check the quality of our data. FastQC can be downloaded and\nran on a Windows or LINUX computer without installation. It is available \nhere\n\n\nStart FastQC and select the fastq files you just downloaded with \nfile -> open\n\n\nWhat is the average read length? The average quality?\n\n\nNow we'll trim the reads using sickle\n\n\nsickle pe -f SRS018585.denovo_duplicates_marked.trimmed.1.fastq \\\n-r SRS018585.denovo_duplicates_marked.trimmed.2.fastq -t sanger \\\n-o SRS018585_trimmed_1.fastq -p SRS018585_trimmed_2.fastq -s unpaired.fastq\n\n\n\n\n\nsickle normally gives you a summary of how many reads were trimmed.\n\n\nAssembly\n\u00b6\n\n\nSPAdes will be used for the assembly. Since version 3.7, SPAdes includes a metagenomic version of its algorithm, callable\nwith the option --meta\n\n\nspades.py --meta -1 SRS018585_trimmed_1.fastq -2 SRS018585_trimmed_2.fastq -t 8 -o assembly\n\n\n\n\n\nthe resulting assenmbly can be found under assembly/scaffolds.fasta. How many contigs does this assembly contain?\nHow long is the longest contig and to what organism does it belong to?\n\n\nTaxonomic Classification and Visualization\n\u00b6\n\n\nFor the vizualisation of the assembly we will use a tool called blobtools.\nBlobtools produces \"Taxon annotated GC-coverage plots\" (TAGC) and was orignially made for\nthe visualisation of (draft) genome assemblies.  \n\n\nmkdir blobtools && cd $_\nblastn -num_threads 8 -db nt -query ../assembly/scaffolds.fasta -out blastresults.txt -outfmt '6 qseqid staxids bitscore'\n\n\n\n\n\nThis blast step is necessary to obtain the taxonomic information of your contigs.\nIt might take a while. Be patient!\n\n\nblobtools create -i ../assembly/scaffolds.fasta -y spades -t blastresults.txt \\\n    --nodes /export/databases/taxonomy/nodes.dmp \\\n    --names /export/databases/taxonomy/names.dmp \\\n    -o scaffolds --title SRS018585\nblobtools plot -i scaffolds.blob.BlobDB.json -o scaffolds --title -r family\n\n\n\n\n\nInspect the plot, what is the most abundant families? try to play with the parameters\n(especially \n-r\n)",
            "title": "Metagenome assembly"
        },
        {
            "location": "/meta_assembly/#metagenome-assembly",
            "text": "In this tutorial you'll learn how to inspect the quality of High-throughput sequencing and\nperform a metagenomic assembly.  We will use data under the accession SRS018585 in the Sequence Read Archive. this sample is\n\"a Human Metagenome sample from G_DNA_Anterior nares of a male participant in the dbGaP study\nHMP Core Microbiome Sampling Protocol A (HMP-A)\"",
            "title": "Metagenome assembly"
        },
        {
            "location": "/meta_assembly/#table-of-contents",
            "text": "Softwares Required for this Tutorial  Getting the Data  Quality Control  Assembly  Taxonomic Classification and Visualization",
            "title": "Table of Contents"
        },
        {
            "location": "/meta_assembly/#softwares-required-for-this-tutorial",
            "text": "FastQC  sickle  SPAdes  Blast  blobtools",
            "title": "Softwares Required for this Tutorial"
        },
        {
            "location": "/meta_assembly/#getting-the-data",
            "text": "wget http://downloads.hmpdacc.org/data/Illumina/anterior_nares/SRS018585.tar.bz2\ntar xjf SRS018585.tar.bz2\ncd SRS018585",
            "title": "Getting the Data"
        },
        {
            "location": "/meta_assembly/#quality-control",
            "text": "we'll use FastQC to check the quality of our data. FastQC can be downloaded and\nran on a Windows or LINUX computer without installation. It is available  here  Start FastQC and select the fastq files you just downloaded with  file -> open  What is the average read length? The average quality?  Now we'll trim the reads using sickle  sickle pe -f SRS018585.denovo_duplicates_marked.trimmed.1.fastq \\\n-r SRS018585.denovo_duplicates_marked.trimmed.2.fastq -t sanger \\\n-o SRS018585_trimmed_1.fastq -p SRS018585_trimmed_2.fastq -s unpaired.fastq  sickle normally gives you a summary of how many reads were trimmed.",
            "title": "Quality Control"
        },
        {
            "location": "/meta_assembly/#assembly",
            "text": "SPAdes will be used for the assembly. Since version 3.7, SPAdes includes a metagenomic version of its algorithm, callable\nwith the option --meta  spades.py --meta -1 SRS018585_trimmed_1.fastq -2 SRS018585_trimmed_2.fastq -t 8 -o assembly  the resulting assenmbly can be found under assembly/scaffolds.fasta. How many contigs does this assembly contain?\nHow long is the longest contig and to what organism does it belong to?",
            "title": "Assembly"
        },
        {
            "location": "/meta_assembly/#taxonomic-classification-and-visualization",
            "text": "For the vizualisation of the assembly we will use a tool called blobtools.\nBlobtools produces \"Taxon annotated GC-coverage plots\" (TAGC) and was orignially made for\nthe visualisation of (draft) genome assemblies.    mkdir blobtools && cd $_\nblastn -num_threads 8 -db nt -query ../assembly/scaffolds.fasta -out blastresults.txt -outfmt '6 qseqid staxids bitscore'  This blast step is necessary to obtain the taxonomic information of your contigs.\nIt might take a while. Be patient!  blobtools create -i ../assembly/scaffolds.fasta -y spades -t blastresults.txt \\\n    --nodes /export/databases/taxonomy/nodes.dmp \\\n    --names /export/databases/taxonomy/names.dmp \\\n    -o scaffolds --title SRS018585\nblobtools plot -i scaffolds.blob.BlobDB.json -o scaffolds --title -r family  Inspect the plot, what is the most abundant families? try to play with the parameters\n(especially  -r )",
            "title": "Taxonomic Classification and Visualization"
        },
        {
            "location": "/rna/",
            "text": "RNA-Seq\n\u00b6\n\n\nLoad salmon\n\u00b6\n\n\nmodule\n \nload\n \nSalmon\n\n\n\n\n\n\nDownloading the data\n\u00b6\n\n\nFor this tutorial we will use the test data from \nthis\n paper:\n\n\n\n\nMalachi Griffith\n, Jason R. Walker, Nicholas C. Spies, Benjamin J. Ainscough, Obi L. Griffith\n. 2015. Informatics for RNA-seq: A web resource for analysis on the cloud. PLoS Comp Biol. 11(8):e1004393.\n\n\n\n\nThe test data consists of two commercially available RNA samples: Universal Human Reference (UHR) and Human Brain Reference (HBR). The UHR is total RNA isolated from a diverse set of 10 cancer cell lines. The HBR is total RNA isolated from the brains of 23 Caucasians, male and female, of varying age but mostly 60-80 years old.\n\n\nIn addition, a spike-in control was used. Specifically we added an aliquot of the ERCC ExFold RNA Spike-In Control Mixes to each sample. The spike-in consists of 92 transcripts that are present in known concentrations across a wide abundance range (from very few copies to many copies). This range allows us to test the degree to which the RNA-seq assay (including all laboratory and analysis steps) accurately reflects the relative abundance of transcript species within a sample. There are two 'mixes' of these transcripts to allow an assessment of differential expression output between samples if you put one mix in each of your two comparisons. In our case, Mix1 was added to the UHR sample, and Mix2 was added to the HBR sample. We also have 3 complete experimental replicates for each sample. This allows us to assess the technical variability of our overall process of producing RNA-seq data in the lab.\n\n\nFor all libraries we prepared low-throughput (Set A) TruSeq Stranded Total RNA Sample Prep Kit libraries with Ribo-Zero Gold to remove both cytoplasmic and mitochondrial rRNA. Triplicate, indexed libraries were made starting with 100ng Agilent/Strategene Universal Human Reference total RNA and 100ng Ambion Human Brain Reference total RNA. The Universal Human Reference replicates received 2 ul of 1:1000 ERCC Mix 1. The Human Brain Reference replicates received 1:1000 ERCC Mix 2. The libraries were quantified with KAPA Library Quantification qPCR and adjusted to the appropriate concentration for sequencing. The triplicate, indexed libraries were then pooled prior to sequencing. Each pool of three replicate libraries were sequenced across 2 lanes of a HiSeq 2000 using paired-end sequence chemistry with 100bp read lengths.\n\n\nSo to summarize we have:\n\n\n\n\nUHR + ERCC Spike-In Mix1, Replicate 1\n\n\nUHR + ERCC Spike-In Mix1, Replicate 2\n\n\nUHR + ERCC Spike-In Mix1, Replicate 3\n\n\nHBR + ERCC Spike-In Mix2, Replicate 1\n\n\nHBR + ERCC Spike-In Mix2, Replicate 2\n\n\nHBR + ERCC Spike-In Mix2, Replicate 3\n\n\n\n\nYou can download the data from \nhere\n.\n\n\nUnpack the data and go into the toy_rna directory\n\n\ntar xzf toy_rna.tar.gz\ncd toy_rna\n\n\n\n\n\nIndexing transcriptome\n\u00b6\n\n\nsalmon index -t chr22_transcripts.fa -i chr22_index\n\n\n\n\n\nQuantify reads using salmon\n\u00b6\n\n\nfor\n i in *_R1.fastq.gz\n\ndo\n\n   \nprefix\n=\n$(\nbasename \n$i\n _R1.fastq.gz\n)\n\n   salmon quant -i chr22_index --libType A \n\\\n\n          -1 \n${\nprefix\n}\n_R1.fastq.gz -2 \n${\nprefix\n}\n_R2.fastq.gz -o quant/\n${\nprefix\n}\n;\n\n\ndone\n\n\n\n\n\n\nThis loop simply goes through each sample and invokes salmon using fairly basic options:\n\n\n\n\nThe -i argument tells salmon where to find the index\n\n\n--libType A tells salmon that it should automatically determine the library type of the sequencing reads (e.g. stranded vs. unstranded etc.)\n\n\nThe -1 and -2 arguments tell salmon where to find the left and right reads for this sample (notice, salmon will accept gzipped FASTQ files directly).\n\n\nthe -o argument specifies the directory where salmon\u2019s quantification results sould be written.\n\n\n\n\nSalmon exposes many different options to the user that enable extra features or modify default behavior. However, the purpose and behavior of all of those options is beyond the scope of this introductory tutorial. You can read about salmon\u2019s many options in the \ndocumentation\n.\n\n\nAfter the salmon commands finish running, you should have a directory named \nquant\n, which will have a sub-directory for each sample. These sub-directories contain the quantification results of salmon, as well as a lot of other information salmon records about the sample and the run. The main output file (called quant.sf) is rather self-explanatory. For example, take a peek at the quantification file for sample \nHBR_Rep1\n in \nquant/HBR_Rep1/quant.sf\n and you\u2019ll see a simple TSV format file listing the name (Name) of each transcript, its length (Length), effective length (EffectiveLength) (more details on this in the documentation), and its abundance in terms of Transcripts Per Million (TPM) and estimated number of reads (NumReads) originating from this transcript.\n\n\nImport read counts using tximport\n\u00b6\n\n\nUsing the tximport R package, you can import salmon\u2019s transcript-level quantifications and optionally aggregate them to the gene level for gene-level differential expression analysis.\n\n\nFirst, open up your favourite R IDE and install the necessary packages:\n\n\nsource\n(\n\"https://bioconductor.org/biocLite.R\"\n)\n\nbiocLite\n(\n\"tximport\"\n)\n\nbiocLite\n(\n\"GenomicFeatures\"\n)\n\n\ninstall.packages\n(\n\"readr\"\n)\n\n\n\n\n\n\nThen load the modules:\n\n\nlibrary\n(\ntximport\n)\n\n\nlibrary\n(\nGenomicFeatures\n)\n\n\nlibrary\n(\nreadr\n)\n\n\n\n\n\n\nSalmon did the quantifiation of the transcript level. We want to see which genes are differentially expressed, so we need to link the transcript names to the gene names. We can use our .gtf annotation for that, and the GenomicFeatures package:\n\n\ntxdb \n<-\n makeTxDbFromGFF\n(\n\"chr22_genes.gtf\"\n)\n\nk \n<-\n keys\n(\ntxdb\n,\n keytype \n=\n \n\"GENEID\"\n)\n\ntx2gene \n<-\n select\n(\ntxdb\n,\n keys \n=\n k\n,\n keytype \n=\n \n\"GENEID\"\n,\n columns \n=\n \n\"TXNAME\"\n)\n\n\nhead\n(\ntx2gene\n)\n\n\n\n\n\n\nNow we can import the salmon quantification. First, download the file with sample descriptions from \nhere\n and put it in the toy_rna directory. Then, use that file to load the corresponding quantification data.\n\n\nsamples \n<-\n read.table\n(\n\"samples.txt\"\n,\n header \n=\n \nTRUE\n)\n\nfiles \n<-\n \nfile.path\n(\n\"quant\"\n,\n samples\n$\nsample\n,\n \n\"quant.sf\"\n)\n\n\nnames\n(\nfiles\n)\n \n<-\n \npaste0\n(\nsamples\n$\nsample\n)\n\ntxi.salmon \n<-\n tximport\n(\nfiles\n,\n type \n=\n \n\"salmon\"\n,\n tx2gene \n=\n tx2gene\n,\n reader \n=\n read_tsv\n)\n\n\n\n\n\n\nTake a look at the data:\n\n\nhead\n(\ntxi.salmon\n$\ncounts\n)\n\n\n\n\n\n\nDifferential expression using DESeq2\n\u00b6\n\n\nInstall the necessary package:\n\n\nbiocLite\n(\n'DESeq2'\n)\n\n\n\n\n\n\nThen load it:\n\n\nlibrary\n(\nDESeq2\n)\n\n\n\n\n\n\nInstantiate the DESeqDataSet and generate result table. See \n?DESeqDataSetFromTximport\n and \n?DESeq\n for more information about the steps performed by the program.\n\n\ndds \n<-\n DESeqDataSetFromTximport\n(\ntxi.salmon\n,\n samples\n,\n \n~\ncondition\n)\n\ndds \n<-\n DESeq\n(\ndds\n)\n\nres \n<-\n results\n(\ndds\n)\n\n\n\n\n\n\nRun the \nsummary\n command to get an idea of how many genes are up- and downregulated between the two conditions:\n\n\nsummary(res)\n\n\nDESeq uses a negative binomial distribution. Such distributions have two parameters: mean and dispersion. The dispersion is a parameter describing how much the variance deviates from the mean.\n\n\nYou can read more about the methods used by DESeq2 in the \npaper\n or the \nvignette\n\n\nPlot dispersions:\n\n\nplotDispEsts\n(\ndds\n,\n main\n=\n\"Dispersion plot\"\n)\n\n\n\n\n\n\nFor clustering and heatmaps, we need to log transform our data:\n\n\nrld \n<-\n rlogTransformation\n(\ndds\n)\n\n\nhead\n(\nassay\n(\nrld\n))\n\n\n\n\n\n\nThen, we create a sample distance heatmap:\n\n\nlibrary\n(\nRColorBrewer\n)\n\n\nlibrary\n(\ngplots\n)\n \n# you may need to install this package\n\n\n\n(\nmycols \n<-\n brewer.pal\n(\n8\n,\n \n\"Dark2\"\n)[\n1\n:\nlength\n(\nunique\n(\nsamples\n$\ncondition\n))])\n\nsampleDists \n<-\n \nas.matrix\n(\ndist\n(\nt\n(\nassay\n(\nrld\n))))\n\nheatmap.2\n(\nas.matrix\n(\nsampleDists\n),\n key\n=\nF\n,\n trace\n=\n\"none\"\n,\n\n          col\n=\ncolorpanel\n(\n100\n,\n \n\"black\"\n,\n \n\"white\"\n),\n\n          ColSideColors\n=\nmycols\n[\nsamples\n$\ncondition\n],\n\n          RowSideColors\n=\nmycols\n[\nsamples\n$\ncondition\n],\n\n          margin\n=\nc\n(\n10\n,\n \n10\n),\n main\n=\n\"Sample Distance Matrix\"\n)\n\n\n\n\n\n\nWe can also plot a PCA:\n\n\nDESeq2\n::\nplotPCA\n(\nrld\n,\n intgroup\n=\n\"condition\"\n)\n\n\n\n\n\n\nIt is time to look at some p-values:\n\n\ntable\n(\nres\n$\npadj\n<\n0.05\n)\n\nres \n<-\n res\n[\norder\n(\nres\n$\npadj\n),\n \n]\n\nresdata \n<-\n \nmerge\n(\nas.data.frame\n(\nres\n),\n \nas.data.frame\n(\ncounts\n(\ndds\n,\n normalized\n=\nTRUE\n)),\n by\n=\n\"row.names\"\n,\n sort\n=\nFALSE\n)\n\n\nnames\n(\nresdata\n)[\n1\n]\n \n<-\n \n\"Gene\"\n\n\nhead\n(\nresdata\n)\n\n\n\n\n\n\nExamine plot of p-values, the MA plot and the Volcano Plot:\n\n\nhist\n(\nres\n$\npvalue\n,\n breaks\n=\n50\n,\n col\n=\n\"grey\"\n)\n\nDESeq2\n::\nplotMA\n(\ndds\n,\n ylim\n=\nc\n(\n-1\n,\n1\n),\n cex\n=\n1\n)\n\n\n\n# Volcano plot\n\n\nwith\n(\nres\n,\n plot\n(\nlog2FoldChange\n,\n \n-\nlog10\n(\npvalue\n),\n pch\n=\n20\n,\n main\n=\n\"Volcano plot\"\n,\n xlim\n=\nc\n(\n-2.5\n,\n2\n)))\n\n\nwith\n(\nsubset\n(\nres\n,\n padj\n<\n.05\n \n),\n points\n(\nlog2FoldChange\n,\n \n-\nlog10\n(\npvalue\n),\n pch\n=\n20\n,\n col\n=\n\"red\"\n))\n\n\n\n\n\n\nKEGG pathway analysis\n\u00b6\n\n\nAs always, install and load the necessary packages:\n\n\nbiocLite\n(\n\"AnnotationDbi\"\n)\n\nbiocLite\n(\n\"org.Hs.eg.db\"\n)\n\nbiocLite\n(\n\"pathview\"\n)\n\nbiocLite\n(\n\"gage\"\n)\n\nbiocLite\n(\n\"gageData\"\n)\n\n\n\nlibrary\n(\nAnnotationDbi\n)\n\n\nlibrary\n(\norg.Hs.eg.db\n)\n\n\nlibrary\n(\npathview\n)\n\n\nlibrary\n(\ngage\n)\n\n\nlibrary\n(\ngageData\n)\n\n\n\n\n\n\nLet\u2019s use the \nmapIds\n function to add more columns to the results. The row.names of our results table has the Ensembl gene ID (our key), so we need to specify  \nkeytype=ENSEMBL\n. The column argument tells the \nmapIds\n function which information we want, and the \nmultiVals\n argument tells the function what to do if there are multiple possible values for a single input value. Here we ask to just give us back the first one that occurs in the database. Let\u2019s get the Entrez IDs, gene symbols, and full gene names.\n\n\nres\n$\nsymbol \n<-\n mapIds\n(\norg.Hs.eg.db\n,\n\n                     keys\n=\nrow.names\n(\nres\n),\n\n                     column\n=\n\"SYMBOL\"\n,\n\n                     keytype\n=\n\"ENSEMBL\"\n,\n\n                     multiVals\n=\n\"first\"\n)\n\nres\n$\nentrez \n<-\n mapIds\n(\norg.Hs.eg.db\n,\n\n                     keys\n=\nrow.names\n(\nres\n),\n\n                     column\n=\n\"ENTREZID\"\n,\n\n                     keytype\n=\n\"ENSEMBL\"\n,\n\n                     multiVals\n=\n\"first\"\n)\n\nres\n$\nname \n<-\n mapIds\n(\norg.Hs.eg.db\n,\n\n                     keys\n=\nrow.names\n(\nres\n),\n\n                     column\n=\n\"GENENAME\"\n,\n\n                     keytype\n=\n\"ENSEMBL\"\n,\n\n                     multiVals\n=\n\"first\"\n)\n\n\n\nhead\n(\nres\n)\n\n\n\n\n\n\nWe\u2019re going to use the \ngage\n package for pathway analysis, and the \npathview\n package to draw a pathway diagram.\n\n\nThe gageData package has pre-compiled databases mapping genes to KEGG pathways and GO terms for common organisms:\n\n\ndata\n(\nkegg.sets.hs\n)\n\ndata\n(\nsigmet.idx.hs\n)\n\nkegg.sets.hs \n<-\n kegg.sets.hs\n[\nsigmet.idx.hs\n]\n\n\nhead\n(\nkegg.sets.hs\n,\n \n3\n)\n\n\n\n\n\n\nRun the pathway analysis. See help on the gage function with \n?gage\n. Specifically, you might want to try changing the value of same.dir.\n\n\nfoldchanges \n<-\n res\n$\nlog2FoldChange\n\nnames\n(\nfoldchanges\n)\n \n<-\n res\n$\nentrez\nkeggres \n<-\n gage\n(\nfoldchanges\n,\n gsets\n=\nkegg.sets.hs\n,\n same.dir\n=\nTRUE\n)\n\n\nlapply\n(\nkeggres\n,\n \nhead\n)\n\n\n\n\n\n\nPull out the top 5 upregulated pathways, then further process that just to get the IDs. We\u2019ll use these KEGG pathway IDs downstream for plotting. The \ndplyr\n package is required to use the pipe (\n%>%\n) construct.\n\n\nlibrary\n(\ndplyr\n)\n\n\n\n# Get the pathways\n\nkeggrespathways \n<-\n \ndata.frame\n(\nid\n=\nrownames\n(\nkeggres\n$\ngreater\n),\n keggres\n$\ngreater\n)\n \n%>%\n\n  tbl_df\n()\n \n%>%\n\n  filter\n(\nrow_number\n()\n<=\n5\n)\n \n%>%\n\n  \n.\n$\nid \n%>%\n\n  \nas.character\n()\n\nkeggrespathways\n\n\n# Get the IDs.\n\nkeggresids \n<-\n \nsubstr\n(\nkeggrespathways\n,\n start\n=\n1\n,\n stop\n=\n8\n)\n\nkeggresids\n\n\n\n\n\nFinally, the \npathview()\n function in the pathview package makes the plots. Let\u2019s write a function so we can loop through and draw plots for the top 5 pathways we created above.\n\n\n# Define plotting function for applying later\n\nplot_pathway \n<-\n \nfunction\n(\npid\n)\n pathview\n(\ngene.data\n=\nfoldchanges\n,\n pathway.id\n=\npid\n,\n species\n=\n\"hsa\"\n,\n new.signature\n=\nFALSE\n)\n\n\n\n# Unload dplyr since it conflicts with the next line\n\n\ndetach\n(\n\"package:dplyr\"\n,\n unload\n=\nT\n)\n\n\n\n# plot multiple pathways (plots saved to disk and returns a throwaway list object)\n\ntmp \n<-\n \nsapply\n(\nkeggresids\n,\n \nfunction\n(\npid\n)\n pathview\n(\ngene.data\n=\nfoldchanges\n,\n pathway.id\n=\npid\n,\n species\n=\n\"hsa\"\n))\n\n\n\n\n\n\nThanks\n\u00b6\n\n\nThis material was inspired by Stephen Turner's blog post:\n\n\n\n\nTutorial: RNA-seq differential expression & pathway analysis with Sailfish, DESeq2, GAGE, and Pathview: http://www.gettinggeneticsdone.com/2015/12/tutorial-rna-seq-differential.html",
            "title": "RNA-Seq"
        },
        {
            "location": "/rna/#rna-seq",
            "text": "",
            "title": "RNA-Seq"
        },
        {
            "location": "/rna/#load-salmon",
            "text": "module   load   Salmon",
            "title": "Load salmon"
        },
        {
            "location": "/rna/#downloading-the-data",
            "text": "For this tutorial we will use the test data from  this  paper:   Malachi Griffith , Jason R. Walker, Nicholas C. Spies, Benjamin J. Ainscough, Obi L. Griffith . 2015. Informatics for RNA-seq: A web resource for analysis on the cloud. PLoS Comp Biol. 11(8):e1004393.   The test data consists of two commercially available RNA samples: Universal Human Reference (UHR) and Human Brain Reference (HBR). The UHR is total RNA isolated from a diverse set of 10 cancer cell lines. The HBR is total RNA isolated from the brains of 23 Caucasians, male and female, of varying age but mostly 60-80 years old.  In addition, a spike-in control was used. Specifically we added an aliquot of the ERCC ExFold RNA Spike-In Control Mixes to each sample. The spike-in consists of 92 transcripts that are present in known concentrations across a wide abundance range (from very few copies to many copies). This range allows us to test the degree to which the RNA-seq assay (including all laboratory and analysis steps) accurately reflects the relative abundance of transcript species within a sample. There are two 'mixes' of these transcripts to allow an assessment of differential expression output between samples if you put one mix in each of your two comparisons. In our case, Mix1 was added to the UHR sample, and Mix2 was added to the HBR sample. We also have 3 complete experimental replicates for each sample. This allows us to assess the technical variability of our overall process of producing RNA-seq data in the lab.  For all libraries we prepared low-throughput (Set A) TruSeq Stranded Total RNA Sample Prep Kit libraries with Ribo-Zero Gold to remove both cytoplasmic and mitochondrial rRNA. Triplicate, indexed libraries were made starting with 100ng Agilent/Strategene Universal Human Reference total RNA and 100ng Ambion Human Brain Reference total RNA. The Universal Human Reference replicates received 2 ul of 1:1000 ERCC Mix 1. The Human Brain Reference replicates received 1:1000 ERCC Mix 2. The libraries were quantified with KAPA Library Quantification qPCR and adjusted to the appropriate concentration for sequencing. The triplicate, indexed libraries were then pooled prior to sequencing. Each pool of three replicate libraries were sequenced across 2 lanes of a HiSeq 2000 using paired-end sequence chemistry with 100bp read lengths.  So to summarize we have:   UHR + ERCC Spike-In Mix1, Replicate 1  UHR + ERCC Spike-In Mix1, Replicate 2  UHR + ERCC Spike-In Mix1, Replicate 3  HBR + ERCC Spike-In Mix2, Replicate 1  HBR + ERCC Spike-In Mix2, Replicate 2  HBR + ERCC Spike-In Mix2, Replicate 3   You can download the data from  here .  Unpack the data and go into the toy_rna directory  tar xzf toy_rna.tar.gz\ncd toy_rna",
            "title": "Downloading the data"
        },
        {
            "location": "/rna/#indexing-transcriptome",
            "text": "salmon index -t chr22_transcripts.fa -i chr22_index",
            "title": "Indexing transcriptome"
        },
        {
            "location": "/rna/#quantify-reads-using-salmon",
            "text": "for  i in *_R1.fastq.gz do \n    prefix = $( basename  $i  _R1.fastq.gz ) \n   salmon quant -i chr22_index --libType A  \\ \n          -1  ${ prefix } _R1.fastq.gz -2  ${ prefix } _R2.fastq.gz -o quant/ ${ prefix } ;  done   This loop simply goes through each sample and invokes salmon using fairly basic options:   The -i argument tells salmon where to find the index  --libType A tells salmon that it should automatically determine the library type of the sequencing reads (e.g. stranded vs. unstranded etc.)  The -1 and -2 arguments tell salmon where to find the left and right reads for this sample (notice, salmon will accept gzipped FASTQ files directly).  the -o argument specifies the directory where salmon\u2019s quantification results sould be written.   Salmon exposes many different options to the user that enable extra features or modify default behavior. However, the purpose and behavior of all of those options is beyond the scope of this introductory tutorial. You can read about salmon\u2019s many options in the  documentation .  After the salmon commands finish running, you should have a directory named  quant , which will have a sub-directory for each sample. These sub-directories contain the quantification results of salmon, as well as a lot of other information salmon records about the sample and the run. The main output file (called quant.sf) is rather self-explanatory. For example, take a peek at the quantification file for sample  HBR_Rep1  in  quant/HBR_Rep1/quant.sf  and you\u2019ll see a simple TSV format file listing the name (Name) of each transcript, its length (Length), effective length (EffectiveLength) (more details on this in the documentation), and its abundance in terms of Transcripts Per Million (TPM) and estimated number of reads (NumReads) originating from this transcript.",
            "title": "Quantify reads using salmon"
        },
        {
            "location": "/rna/#import-read-counts-using-tximport",
            "text": "Using the tximport R package, you can import salmon\u2019s transcript-level quantifications and optionally aggregate them to the gene level for gene-level differential expression analysis.  First, open up your favourite R IDE and install the necessary packages:  source ( \"https://bioconductor.org/biocLite.R\" ) \nbiocLite ( \"tximport\" ) \nbiocLite ( \"GenomicFeatures\" ) \n\ninstall.packages ( \"readr\" )   Then load the modules:  library ( tximport )  library ( GenomicFeatures )  library ( readr )   Salmon did the quantifiation of the transcript level. We want to see which genes are differentially expressed, so we need to link the transcript names to the gene names. We can use our .gtf annotation for that, and the GenomicFeatures package:  txdb  <-  makeTxDbFromGFF ( \"chr22_genes.gtf\" ) \nk  <-  keys ( txdb ,  keytype  =   \"GENEID\" ) \ntx2gene  <-  select ( txdb ,  keys  =  k ,  keytype  =   \"GENEID\" ,  columns  =   \"TXNAME\" )  head ( tx2gene )   Now we can import the salmon quantification. First, download the file with sample descriptions from  here  and put it in the toy_rna directory. Then, use that file to load the corresponding quantification data.  samples  <-  read.table ( \"samples.txt\" ,  header  =   TRUE ) \nfiles  <-   file.path ( \"quant\" ,  samples $ sample ,   \"quant.sf\" )  names ( files )   <-   paste0 ( samples $ sample ) \ntxi.salmon  <-  tximport ( files ,  type  =   \"salmon\" ,  tx2gene  =  tx2gene ,  reader  =  read_tsv )   Take a look at the data:  head ( txi.salmon $ counts )",
            "title": "Import read counts using tximport"
        },
        {
            "location": "/rna/#differential-expression-using-deseq2",
            "text": "Install the necessary package:  biocLite ( 'DESeq2' )   Then load it:  library ( DESeq2 )   Instantiate the DESeqDataSet and generate result table. See  ?DESeqDataSetFromTximport  and  ?DESeq  for more information about the steps performed by the program.  dds  <-  DESeqDataSetFromTximport ( txi.salmon ,  samples ,   ~ condition ) \ndds  <-  DESeq ( dds ) \nres  <-  results ( dds )   Run the  summary  command to get an idea of how many genes are up- and downregulated between the two conditions:  summary(res)  DESeq uses a negative binomial distribution. Such distributions have two parameters: mean and dispersion. The dispersion is a parameter describing how much the variance deviates from the mean.  You can read more about the methods used by DESeq2 in the  paper  or the  vignette  Plot dispersions:  plotDispEsts ( dds ,  main = \"Dispersion plot\" )   For clustering and heatmaps, we need to log transform our data:  rld  <-  rlogTransformation ( dds )  head ( assay ( rld ))   Then, we create a sample distance heatmap:  library ( RColorBrewer )  library ( gplots )   # you may need to install this package  ( mycols  <-  brewer.pal ( 8 ,   \"Dark2\" )[ 1 : length ( unique ( samples $ condition ))]) \nsampleDists  <-   as.matrix ( dist ( t ( assay ( rld )))) \nheatmap.2 ( as.matrix ( sampleDists ),  key = F ,  trace = \"none\" , \n          col = colorpanel ( 100 ,   \"black\" ,   \"white\" ), \n          ColSideColors = mycols [ samples $ condition ], \n          RowSideColors = mycols [ samples $ condition ], \n          margin = c ( 10 ,   10 ),  main = \"Sample Distance Matrix\" )   We can also plot a PCA:  DESeq2 :: plotPCA ( rld ,  intgroup = \"condition\" )   It is time to look at some p-values:  table ( res $ padj < 0.05 ) \nres  <-  res [ order ( res $ padj ),   ] \nresdata  <-   merge ( as.data.frame ( res ),   as.data.frame ( counts ( dds ,  normalized = TRUE )),  by = \"row.names\" ,  sort = FALSE )  names ( resdata )[ 1 ]   <-   \"Gene\"  head ( resdata )   Examine plot of p-values, the MA plot and the Volcano Plot:  hist ( res $ pvalue ,  breaks = 50 ,  col = \"grey\" ) \nDESeq2 :: plotMA ( dds ,  ylim = c ( -1 , 1 ),  cex = 1 )  # Volcano plot  with ( res ,  plot ( log2FoldChange ,   - log10 ( pvalue ),  pch = 20 ,  main = \"Volcano plot\" ,  xlim = c ( -2.5 , 2 )))  with ( subset ( res ,  padj < .05   ),  points ( log2FoldChange ,   - log10 ( pvalue ),  pch = 20 ,  col = \"red\" ))",
            "title": "Differential expression using DESeq2"
        },
        {
            "location": "/rna/#kegg-pathway-analysis",
            "text": "As always, install and load the necessary packages:  biocLite ( \"AnnotationDbi\" ) \nbiocLite ( \"org.Hs.eg.db\" ) \nbiocLite ( \"pathview\" ) \nbiocLite ( \"gage\" ) \nbiocLite ( \"gageData\" )  library ( AnnotationDbi )  library ( org.Hs.eg.db )  library ( pathview )  library ( gage )  library ( gageData )   Let\u2019s use the  mapIds  function to add more columns to the results. The row.names of our results table has the Ensembl gene ID (our key), so we need to specify   keytype=ENSEMBL . The column argument tells the  mapIds  function which information we want, and the  multiVals  argument tells the function what to do if there are multiple possible values for a single input value. Here we ask to just give us back the first one that occurs in the database. Let\u2019s get the Entrez IDs, gene symbols, and full gene names.  res $ symbol  <-  mapIds ( org.Hs.eg.db , \n                     keys = row.names ( res ), \n                     column = \"SYMBOL\" , \n                     keytype = \"ENSEMBL\" , \n                     multiVals = \"first\" ) \nres $ entrez  <-  mapIds ( org.Hs.eg.db , \n                     keys = row.names ( res ), \n                     column = \"ENTREZID\" , \n                     keytype = \"ENSEMBL\" , \n                     multiVals = \"first\" ) \nres $ name  <-  mapIds ( org.Hs.eg.db , \n                     keys = row.names ( res ), \n                     column = \"GENENAME\" , \n                     keytype = \"ENSEMBL\" , \n                     multiVals = \"first\" )  head ( res )   We\u2019re going to use the  gage  package for pathway analysis, and the  pathview  package to draw a pathway diagram.  The gageData package has pre-compiled databases mapping genes to KEGG pathways and GO terms for common organisms:  data ( kegg.sets.hs ) \ndata ( sigmet.idx.hs ) \nkegg.sets.hs  <-  kegg.sets.hs [ sigmet.idx.hs ]  head ( kegg.sets.hs ,   3 )   Run the pathway analysis. See help on the gage function with  ?gage . Specifically, you might want to try changing the value of same.dir.  foldchanges  <-  res $ log2FoldChange names ( foldchanges )   <-  res $ entrez\nkeggres  <-  gage ( foldchanges ,  gsets = kegg.sets.hs ,  same.dir = TRUE )  lapply ( keggres ,   head )   Pull out the top 5 upregulated pathways, then further process that just to get the IDs. We\u2019ll use these KEGG pathway IDs downstream for plotting. The  dplyr  package is required to use the pipe ( %>% ) construct.  library ( dplyr )  # Get the pathways \nkeggrespathways  <-   data.frame ( id = rownames ( keggres $ greater ),  keggres $ greater )   %>% \n  tbl_df ()   %>% \n  filter ( row_number () <= 5 )   %>% \n   . $ id  %>% \n   as.character () \nkeggrespathways # Get the IDs. \nkeggresids  <-   substr ( keggrespathways ,  start = 1 ,  stop = 8 ) \nkeggresids  Finally, the  pathview()  function in the pathview package makes the plots. Let\u2019s write a function so we can loop through and draw plots for the top 5 pathways we created above.  # Define plotting function for applying later \nplot_pathway  <-   function ( pid )  pathview ( gene.data = foldchanges ,  pathway.id = pid ,  species = \"hsa\" ,  new.signature = FALSE )  # Unload dplyr since it conflicts with the next line  detach ( \"package:dplyr\" ,  unload = T )  # plot multiple pathways (plots saved to disk and returns a throwaway list object) \ntmp  <-   sapply ( keggresids ,   function ( pid )  pathview ( gene.data = foldchanges ,  pathway.id = pid ,  species = \"hsa\" ))",
            "title": "KEGG pathway analysis"
        },
        {
            "location": "/rna/#thanks",
            "text": "This material was inspired by Stephen Turner's blog post:   Tutorial: RNA-seq differential expression & pathway analysis with Sailfish, DESeq2, GAGE, and Pathview: http://www.gettinggeneticsdone.com/2015/12/tutorial-rna-seq-differential.html",
            "title": "Thanks"
        },
        {
            "location": "/nanopore/",
            "text": "Introduction to Nanopore Sequencing\n\u00b6\n\n\nIn this tutorial we will assemble the \nE. coli\n genome using a mix of long, error-prone reads from the MinION (Oxford Nanopore) and short reads from a HiSeq instrument (Illumina).\n\n\nThe MinION data used in this tutorial come a test run by the \nLoman lab\n.  \n\nThe Illumina data were simulated using \nInSilicoSeq\n\n\nGet the Data\n\u00b6\n\n\nFirst download the nanopore data\n\n\nfastq-dump ERR1147227\n\n\n\n\n\nYou will not need the HiSeq data right away, but you can start the download in another window\n\n\ncurl -O -J -L https://osf.io/pxk7f/download\ncurl -O -J -L https://osf.io/zax3c/download\n\n\n\n\n\nlook at basic stats of the nanopore reads\n\n\nassembly-stats ERR1147227.fastq\n\n\n\n\n\n\n\nQuestion\n\n\nHow many nanopore reads do we have?\n\n\n\n\n\n\nQuestion\n\n\nHow long is the longest read?\n\n\n\n\n\n\nQuestion\n\n\nWhat is the average read length?\n\n\n\n\nAdapter trimming\n\u00b6\n\n\nWe'll use \nporechop\n to remove the adapters from the reads.\nAdditionally to trim the adapters at the 3' and 5' ends, porechop can split the reads if it finds adapters in the middle.\n\n\nporechop -i ERR1147227.fastq -o ERR1147227_trimmed.fastq\n\n\n\n\n\nAssembly\n\u00b6\n\n\nWe assemble the reads using miniasm\n\n\nminimap2 -x ava-ont ERR1147227_trimmed.fastq ERR1147227_trimmed.fastq \n|\n \n\\\n\n    gzip -1 > ERR1147227.paf.gz\nminiasm -f ERR1147227_trimmed.fastq ERR1147227.paf.gz > ERR1147227.gfa\nawk \n'/^S/{print \">\"$2\"\\n\"$3}'\n ERR1147227.gfa \n|\n fold > ERR1147227.fasta\n\n\n\n\n\n\n\nNote\n\n\nMiniasm is a fast but has no consensus step.\nThe accuracy of the assembly will be equal to the base accuracy.\n\n\n\n\nPolishing\n\u00b6\n\n\nSince the miniasm assembly likely contains a lot if errors, we correct it with  Illumina reads.\n\n\nFirst we map the short reads against the assembly\n\n\nbowtie2-build ERR1147227.fasta ERR1147227\nbowtie2 -x ERR1147227 -1 ecoli_hiseq_R1.fastq.gz -2 ecoli_hiseq_R2.fastq.gz | \\\n    samtools view -bS -o ERR1147227.bam\nsamtools sort ERR1147227.bam -o ERR1147227.sorted.bam\nsamtools index ERR1147227.sorted.bam\n\n\n\n\n\nthen we run Pilon\n\n\npilon --genome ERR1147227.fasta --frags ERR1147227.sorted.bam \\\n    --output ERR1147227_improved\n\n\n\n\n\nwhich will correct eventual misamatches in our assembly and write the new improved assembly to \nERR1147227_improved.fasta\n\n\nFor better results we should perform more than one round of polishing.\n\n\nCompare with the existing assembly\n\u00b6\n\n\nGo to \nhttps://www.ncbi.nlm.nih.gov\n and search for NC_000913.\nDownload the associated genome in fasta format and rename it to \necoli_ref.fasta\n\n\nnucmer --maxmatch -c \n100\n -p ecoli ERR1147227_trimmed.fastq ecoli_ref.fasta\nmummerplot --fat --filter --png --large -p ecoli ecoli.delta\n\n\n\n\n\nthen take a look at \necoli.png\n\n\nAnnotation\n\u00b6\n\n\nawk \n'/^>/{print \">ctg\" ++i; next}{print}'\n < ERR1147227_improved.fasta \n\\\n\n    > ERR1147227_formatted.fasta\nprokka --outdir annotation --kingdom Bacteria ERR1147227_formatted.fasta\n\n\n\n\n\nYou can open the output to see how it went\n\n\ncat annotation/PROKKA_11232017.txt\n\n\n\n\n\n\n\nQuestion\n\n\nDoes it fit your expecations? How many genes were you expecting?",
            "title": "Introduction to Nanopore Sequencing"
        },
        {
            "location": "/nanopore/#introduction-to-nanopore-sequencing",
            "text": "In this tutorial we will assemble the  E. coli  genome using a mix of long, error-prone reads from the MinION (Oxford Nanopore) and short reads from a HiSeq instrument (Illumina).  The MinION data used in this tutorial come a test run by the  Loman lab .   \nThe Illumina data were simulated using  InSilicoSeq",
            "title": "Introduction to Nanopore Sequencing"
        },
        {
            "location": "/nanopore/#get-the-data",
            "text": "First download the nanopore data  fastq-dump ERR1147227  You will not need the HiSeq data right away, but you can start the download in another window  curl -O -J -L https://osf.io/pxk7f/download\ncurl -O -J -L https://osf.io/zax3c/download  look at basic stats of the nanopore reads  assembly-stats ERR1147227.fastq   Question  How many nanopore reads do we have?    Question  How long is the longest read?    Question  What is the average read length?",
            "title": "Get the Data"
        },
        {
            "location": "/nanopore/#adapter-trimming",
            "text": "We'll use  porechop  to remove the adapters from the reads.\nAdditionally to trim the adapters at the 3' and 5' ends, porechop can split the reads if it finds adapters in the middle.  porechop -i ERR1147227.fastq -o ERR1147227_trimmed.fastq",
            "title": "Adapter trimming"
        },
        {
            "location": "/nanopore/#assembly",
            "text": "We assemble the reads using miniasm  minimap2 -x ava-ont ERR1147227_trimmed.fastq ERR1147227_trimmed.fastq  |   \\ \n    gzip -1 > ERR1147227.paf.gz\nminiasm -f ERR1147227_trimmed.fastq ERR1147227.paf.gz > ERR1147227.gfa\nawk  '/^S/{print \">\"$2\"\\n\"$3}'  ERR1147227.gfa  |  fold > ERR1147227.fasta   Note  Miniasm is a fast but has no consensus step.\nThe accuracy of the assembly will be equal to the base accuracy.",
            "title": "Assembly"
        },
        {
            "location": "/nanopore/#polishing",
            "text": "Since the miniasm assembly likely contains a lot if errors, we correct it with  Illumina reads.  First we map the short reads against the assembly  bowtie2-build ERR1147227.fasta ERR1147227\nbowtie2 -x ERR1147227 -1 ecoli_hiseq_R1.fastq.gz -2 ecoli_hiseq_R2.fastq.gz | \\\n    samtools view -bS -o ERR1147227.bam\nsamtools sort ERR1147227.bam -o ERR1147227.sorted.bam\nsamtools index ERR1147227.sorted.bam  then we run Pilon  pilon --genome ERR1147227.fasta --frags ERR1147227.sorted.bam \\\n    --output ERR1147227_improved  which will correct eventual misamatches in our assembly and write the new improved assembly to  ERR1147227_improved.fasta  For better results we should perform more than one round of polishing.",
            "title": "Polishing"
        },
        {
            "location": "/nanopore/#compare-with-the-existing-assembly",
            "text": "Go to  https://www.ncbi.nlm.nih.gov  and search for NC_000913.\nDownload the associated genome in fasta format and rename it to  ecoli_ref.fasta  nucmer --maxmatch -c  100  -p ecoli ERR1147227_trimmed.fastq ecoli_ref.fasta\nmummerplot --fat --filter --png --large -p ecoli ecoli.delta  then take a look at  ecoli.png",
            "title": "Compare with the existing assembly"
        },
        {
            "location": "/nanopore/#annotation",
            "text": "awk  '/^>/{print \">ctg\" ++i; next}{print}'  < ERR1147227_improved.fasta  \\ \n    > ERR1147227_formatted.fasta\nprokka --outdir annotation --kingdom Bacteria ERR1147227_formatted.fasta  You can open the output to see how it went  cat annotation/PROKKA_11232017.txt   Question  Does it fit your expecations? How many genes were you expecting?",
            "title": "Annotation"
        }
    ]
}