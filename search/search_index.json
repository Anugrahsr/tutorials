{
    "docs": [
        {
            "location": "/",
            "text": "Home\n\u00b6\n\n\nTODO\n introduction, info about the group and SLU\n\n\nTable of contents\n\u00b6\n\n\n\n\nHome\n\n\nThe command-line\n\n\nFile Formats\n\n\nQuality Control and Trimming\n\n\nMapping and Variant Calling\n\n\nDe-novo Genome Assembly\n\n\nGenome Annotation\n\n\nPan-Genome Analysis\n\n\nMetabarcoding\n\n\nWhole Metagenome Sequencing\n\n\nMetagenome assembly\n\n\nRNA-Seq\n\n\n\n\nContributing\n\u00b6\n\n\nA typo? Something that irks you? Submit an \nissue\n\nor a pull request.\n\n\nLicense\n\u00b6\n\n\nThis work is licensed under the Creative Commons Attribution 4.0 International License.\nTo view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/ or send a letter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.",
            "title": "Home"
        },
        {
            "location": "/#home",
            "text": "TODO  introduction, info about the group and SLU",
            "title": "Home"
        },
        {
            "location": "/#table-of-contents",
            "text": "Home  The command-line  File Formats  Quality Control and Trimming  Mapping and Variant Calling  De-novo Genome Assembly  Genome Annotation  Pan-Genome Analysis  Metabarcoding  Whole Metagenome Sequencing  Metagenome assembly  RNA-Seq",
            "title": "Table of contents"
        },
        {
            "location": "/#contributing",
            "text": "A typo? Something that irks you? Submit an  issue \nor a pull request.",
            "title": "Contributing"
        },
        {
            "location": "/#license",
            "text": "This work is licensed under the Creative Commons Attribution 4.0 International License.\nTo view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/ or send a letter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.",
            "title": "License"
        },
        {
            "location": "/command_line/",
            "text": "The command-line\n\u00b6\n\n\nThis tutorial is largely inspired of the \nIntroduction to UNIX\n course from the Sanger Institute.\n\n\nThe aim of this module is to introduce Unix and cover some of the basics that will allow you to be more comfortable with the command-line. Several of the programs that you are going to use during this course are useful for bioinformatics analyses. This module is only designed to provide a very brief introduction to some of the features and useful commands of Unix. During this module we will also obtain a genome sequence and examine the basic structure of an EMBL entry.\n\n\nIntroduction\n\u00b6\n\n\nUnix is the standard operating system on most large computer systems in scientific research, in the same way that Microsoft Windows is the dominant operating system on desktop PCs. Unix and MS Windows both perform the important job of managing the computer\u2019s hardware (screen, keyboard, mouse, hard disks, network connections, etc...) on your behalf. They also provide you with tools to manage your files and to run application software. They both offer a graphical user interface (desktop). The desktops look different, call things by different names but they mostly can do the same things. Unix is a powerful, secure, robust and stable operating system that allows dozens of people to run programs on the same computer at the same time. This is why it is the preferred operating system for large-scale scientific computing. It is run on all kind of machines, like mobile phones (Android), desktop PCs, kitchen appliances,... all the way up to supercomputers. Unix powers the majority of the Internet.\n\n\nAims\n\u00b6\n\n\nThe aim of this course is to introduce Unix and cover the basics. The programs that you are going to use during the courses, plus many others that are useful for bioinformatics analyses, are run in Unix. This module is only designed to provide a very brief introduction to some of the features and useful commands of Unix. During this module we will also obtain a genome sequence and examine the basic structure of an EMBL entry.\n\n\nWhy use Unix?\n\u00b6\n\n\n\n\nUnix is a well established, very widespread operating system. You probably have a device running on Unix in your home without realising it (e.g. playstation, TV box, wireless router, android tablets/phones,...\n\n\nCommand line driven, with a huge number of often terse, but powerful commands.\n\n\nIn contrast to Windows, it is designed to allow many users to run their programs simultaneously on the same computer.\n\n\nDesigned to work in computer networks - for example, most of the Internet is Unix based.\n\n\nIt is used on many of the powerful computers at bioinformatics centres and also on many desktops and laptops (MacOS is largely UNIX compatible).\n\n\nThe major difference between Unix and Windows is that it is free (as in freedom) and you can modify it to work however you want. This same principle of freedom is also used in most bioinformatics software.\n\n\nThere are many distributions of Unix such as Ubuntu, RedHat, Fedora, Mint,...). These are all Unix, but they bundle up extra software in a different way or combinations. Some are known for being conservative and reliable; whilst others are know for being cutting-edge (and less reliable).\n\n\nThe MacOSX operating system used by the \neBioKit\n is also based on Unix.\n\n\n\n\nGetting started\n\u00b6\n\n\nFor this course, you will have to connect to the eBiokit using SSH. SSH stands for Secure Shell and is a network protocol used to securely connect to a server. To do so, you will need an SSH client:\n\n\n\n\nOn Linux: it is included by default, named Terminal.\n\n\nOn MacOS: it is included by default, also named Terminal.\n\n\nOn Windows: you'll have to download and install \nMobaXterm\n, a terminal emulator.\n\n\n\n\nOnce you've opened your terminal (or terminal emulator), type\n\n\nssh username@ip_address\n\n\nreplacing \nusername\n and \nip_address\n with your username and the ip address of the server you are connecting to.\nType your password when prompted. As you type, nothing will show on screen. No stars, no dots.\nIt is supposed to be that way. Just type the password and press enter!\n\n\n\n\nYou can type commands directly into the terminal at the \u2018$' prompt. A list of useful commands can be found on the next page. Many of them are two- or three-letter abbreviations. The earliest Unix systems (\ncirca\n 1970) only had slow Teletype terminals, so it was faster to type 'rm' to remove a file than 'delete' or 'erase'. This terseness is a feature of Unix that still survives.\n\n\nThe command line\n\u00b6\n\n\nAll Unix programs may be run by typing commands at the Unix prompt. The command line tells the computer what to do. You may subtly alter these commands by specifying certain options when typing in the command line.\n\n\nCommand line Arguments\n\u00b6\n\n\nTyping any Unix command for example \nls\n, \nmv\n or \ncd\n at the Unix prompt with the appropriate variables such as files names or directories will result in the tasks being performed on pressing the enter key.\n\n\n\n\nThe command is separated from the options and arguments by a space. Additional options and/or arguments can be added to the commands to affect the way the command works. Options usually have one dash and a letter (e.g. -h) or two dashes and a word (--help) with no space between the dash and the letter/word. Arguments are usually filenames or directories.\n\n\nFor example: List the contents of a directory\n\n\nls\n List the contents of a directoryList the contents of a directory with extra information about the files\n\nls \u2013l\n List the contents of a directory with extra information about the files\n\nls \u2013a\n List all contents including hidden files & directories\n\nls -al\n List all contents including hidden files & directories, with extra information about the files\n\nls \u2013l /usr/\n List the contents of the directory /usr/, with extra information about the files\n\n\nFiles and Directories\n\u00b6\n\n\nDirectories are the Unix equivalent of folders on a PC or Mac. They are organised in a hierarchy, so directories can have sub-directories. Directories are very useful for organising your work and keeping your account tidy - for example, if you have more than one project, you can organise the files for each project into different directories to keep them separate. You can think of directories as rooms in a house. You can only be in one room (directory) at a time. When you are in a room you can see everything in that room easily. To see things in other rooms, you have to go to the appropriate door and crane your head around. Unix works in a similar manner, moving from directory to directory to access files. The location or directory that you are in is referred to as the current working directory.\n\n\nDirectory structure example\n\n\n\n\nTherefore if there is a file called genome.seq in the \ndna\n directory its location or full pathname can be expressed as /nfs/dna/genome.seq.\n\n\nGeneral Points\n\u00b6\n\n\nUnix is pretty straightforward, but there are some general points to remember that will make your life easier: most flavors of UNIX are case sensitive - typing \nls\n is generally not the same as typing \nLS\n. You need to put a space between a command and its argument - for example, \nless my_file\n will show you the contents of the file called my_file; \nlessmyfile\n will just give you an error! Unix is not psychic: If you misspell the name of a command or the name of a file, it will not understand you. Many of the commands are only a few letters long; this can be confusing until you start to think logically about why those letters were chosen - ls for list, rm for remove and so on. Often when you have problems with Unix, it is due to a spelling mistake, or perhaps you have omitted a space. If you want to know more about Unix and its commands there are plenty of resources available that provide a more comprehensive guide (including a cheat sheet at the end of this chapter.\n\n\n\n\nhttp://unix.t-a-y-l-o-r.com/\n\n\n\n\nIn what follows, we shall use the following typographical conventions: Characters written in \nbold typewriter font\n are commands to be typed into the computer as they stand. Characters written in \nitalic typewriter font\n indicate non-specific file or directory names. Words inserted within square brackets [Ctrl] indicate keys to be pressed. So, for example,\n\n$ **ls** *any_directory* [Enter]\n means \"at the Unix prompt $, type ls followed by the name of some directory, then press Enter\"\nDon't forget to press the [Enter] key: commands are not sent to the computer until this is done.\n\n\nSome useful Unix commands Command\u00a0and What it does\n\u00b6\n\n\n\n\n\n\n\n\nCommand\n\n\nWhat it does\n\n\n\n\n\n\n\n\n\n\nls\n\n\nLists the contents of the current directory\n\n\n\n\n\n\nmkdir\n\n\nCreates a new directory\n\n\n\n\n\n\nmv\n\n\nMoves or renames a file\n\n\n\n\n\n\ncp\n\n\nCopies a file\n\n\n\n\n\n\nrm\n\n\nRemoves a file\n\n\n\n\n\n\ncat\n\n\nConcatenates files\n\n\n\n\n\n\nless\n\n\nDisplays the contents of a file one page at a time\n\n\n\n\n\n\nhead\n\n\nDisplays the first ten lines of a file\n\n\n\n\n\n\ntail\n\n\nDisplays the last ten lines of a file\n\n\n\n\n\n\ncd\n\n\nChanges current working directory\n\n\n\n\n\n\npwd\n\n\nPrints working directory\n\n\n\n\n\n\nfind\n\n\nFinds files matching an expression\n\n\n\n\n\n\ngrep\n\n\nSearches a file for patterns\n\n\n\n\n\n\nwc\n\n\nCounts the lines, words, characters, and bytes in a file\n\n\n\n\n\n\nkill\n\n\nStops a process\n\n\n\n\n\n\njobs\n\n\nLists the processes that are running\n\n\n\n\n\n\n\n\nFirts steps\n\u00b6\n\n\nThe following exercise introduces a few useful Unix commands and provides examples of how they can be used. Many people panic when they are confronted with an Unix prompt! Don\u2019t! The exercise is designed to be step-by-step, so all the commands you need are provided in the text. If you get lost ask a demonstrator. If you are a person skilled at Unix, be patient it is only a short exercise. Finding where you are and what you\u2019ve got\n\n\npwd\n\u00a0\u00a0\u00a0\u00a0\u00a0\nPrint the working directory As seen previously directories are arranged in a hierarchical structure. To determine where you are in the hierarchy you can use the pwd command to display the name of the current working directory. The current working directory may be thought of as the directory you are in, i.e. your current position in the file-system tree To find out where you are type\n\n\npwd [enter]\n\n\nYou will see that you are in your home directory. We need to move into the ngs_course_data directory. Remember, Unix is case sensitive \nPWD\n is not the same as \npwd\n\n\n\n  \n\n\n\n\n\ncd\n\u00a0\nChange current working directory The cd command will change the current working directory to another, in other words allow you to move up or down in the directory hierarchy. First of all we are going to move into the \"ngs_course_data\" directory below. To do this type:\n\n\ncd ngs_course_data [enter]\n\n\nNow use the pwd command to check your location in the directory hierarchy.\n\n\nChange again the directory to \nModule_Unix\n  \n\n\nls\n\nList the contents of a directory To find out what are the contents of the current directory type \nls\n [enter] The ls command lists the contents of your current directory, this includes files and directories You should see that there are several other directories.\n\n\nNow use the \ncd\n command again to change to the \nModule_Unix\n directory.\n\n\n\n  \n\n\n\n\n\nChanging and moving what you\u2019ve got\n\u00b6\n\n\ncp\n\nCopy a file. \ncp file1 file2\n is the command which makes a copy of file1 in the current working directory and calls it file2! What you are going to do is make a copy of AL513382.embl. This file contains the genome of Salmonella typhi strain CT18 in EMBL format (we'll learn more about file formats later during the course). The new file will be called S_typhi.embl.\n\n\ncp AL513382.embl S_typhi.embl [enter]\n\nIf you use the ls command to check the contents of the current directory you will see that there is an extra file called S_typhi.embl.\n\n\n\n  \u00a0\n\n\n\n\n\nrm\n\nDelete a file. This command removes a file permanently, so be careful! You are now going to remove the old version of the \nS. typhi\n genome file, AL513382.embl\n\nrm AL513382.embl [enter]\n\n\nThe file will be removed. Use the \nls\n command to check the contents of the current directory to see that AL513382.embl has been removed.\n\n\nUnix, as a general rule does exactly what you ask, and does not ask for confirmation. Unfortunately there is no \"recycle bin\" on the command line to recover the file from, so you have to be careful.\n\n\n\n  \u00a0\n\n\n\n\n\ncd\n\nChange current working directory. As before the cd command will change the current working directory to another, in other words allow you to move up or down in the directory hierarchy. First of all we are going to move into the directory above, type:\n\ncd .. [enter]\n\n\nNow use the \npwd\n command to check your location in the directory hierarchy. Next, we are going to move into the Module_Artemis directory. To change to the Module_Artemis directory type:\n\ncd Module_Artemis [enter]\n use the ls command to check the contents of the directory.\n\n\n\n  \n\n\n\n\n\nTips\n\u00b6\n\n\nThere are some short cuts for referring to directories:\n\n\n. \u00a0Current directory (one full stop)  \n.. Directory above (two full stops)  \n~ \u00a0Home directory (tilde)  \n/ \u00a0Root of the file system (like C:\\ in Windows)\n\n\n\n\nPressing the tab key twice will try and autocomplete what you\u2019ve started typing or give you a list of all possible completions. This saves a lot of typing and typos. Pressing the up/down arrows will let you scroll through the previous commands. If you highlight some text, middle clicking will paste it on the command line.\n\n\nmv\n\nMove a file. To move a file from one place to another use the \nmv\n command. This moves the file rather than copies it, therefore you end up with only one file rather than two. When using the command the path or pathname is used to tell Unix where to find the file. You refer to files in other directories by using the list of hierarchical names separated by slashes. For example, the file bases in the directory genome has the path genome/bases If no path is specified Unix assumes that the file is in the current working directory. What you are going to do is move the file S_typhi.embl from the Module_Unix directory, to the current working directory.\n\nmv ../Module_Unix/S_typhi.embl . [enter]\n\nUse the ls command to check the contents of the current directory to see that S_typhi.embl has been moved. ../Module_Unix/S_typhi.embl specifies that S_typhi.embl is in the Module_Unix directory. If the file was in the directory above, the path would change to: ../ S_typhi.embl\n\n\n\n  \n\n\n\n\n\nThe command can also be used to rename a file in the current working directory. Previously we used the cp command, but mv provides an alternative without the need to delete the original file. Therefore we could have used: \u00a0\n\n\nmv AL513382.embl S_typhi.embl [enter]\n instead of:\n\n\ncp AL513382.embl S_typhi.embl [enter]\nrm AL513382.embl [enter]\n\n\n\n\nViewing what you\u2019ve got\n\u00b6\n\n\nless\n\nDisplay file contents. This command displays the contents of a specified file one screen at a time. You are now going to look at the contents of S_typhi.embl.\n\nless S_typhi.embl [enter]\n\nThe contents of S_typhi.embl will be displayed one screen at a time, to view the next screen press the space bar. less can also scroll backwards if you hit the b key. Another useful feature is the slash key, /, to search for a word in the file. You type the word you are looking for and press enter. The screen will jump to the next occurrence and highlight it. As S_typhi.embl is a large file this will take a while, therefore you may want to escape or exit from this command. To exit press the letter \u2018q\u2019. If you really need to exit from a program and it isn\u2019t responding press \u2018control\u2019 and the letter \u2018c\u2019 at the same time.\n\n\n\n  \u00a0\n\n\n\n\n\nhead\n \n\nDisplay the first ten lines of a file\n\n\ntail\n\u00a0\n\nDisplay the last ten lines of a file  \n\n\nSometimes you may just want to view the text at the beginning or the end of a file, without having to display all of the file. The head and tail commands can be used to do this. You are now going to look at the beginning of S_typhi.embl.\n\n\nhead S_typhi.embl [enter]\n\n\nTo look at the end of S_typhi.embl type: \ntail S_typhi.embl [enter]\n\nThe number of lines that are displayed can be increased by adding extra arguments. To increase the number of lines viewed from 10 to 100 add the \u2013100 argument to the command. For example to view the last 100 lines of S_typhi.embl type:\n\n\ntail -100 S_typhi.embl [enter]\n\n\nDo this for both head and tail commands. What type of information is at the beginning and end of the EMBL format file?\n\n\n\n  \u00a0\n\n\n\n\n\ncat\n\nJoin files together. Having looked at the beginning and end of the S_typhi.embl file you should notice that in EMBL format files the annotation comes first, then the DNA sequence at the end. If you had two separate files containing the annotation and the DNA sequence, both in EMBL format, it is possible to concatenate or join the two together to make a single file like the S_typhi.embl file you have just looked at. The Unix command cat can be used to join two or more files into a single file. The order in which the files are joined is determined by the order in which they appear in the command line. For example, we have two separate files, MAL13P1.dna and MAL13P1.tab, that contain the DNA and annotation, respectively, from the \nP. falciparum\n genome. Return to the Module_Unix directory using the cd command:\n\ncd ../Module_Unix [enter]\n\n\nand type\n\ncat MAL13P1.tab MAL13P1.dna > MAL13P1.embl [enter]\n\nMAL13P1.tab and MAL13P1.dna will be joined together and written to a file called MAL13P1.embl\nThe \n>\n symbol in the command line directs the output of the cat program to the designated file MAL13P1.embl\n\n\n\n  \u00a0\n\n\n\n\n\nwc\n\nCounts the lines, words or characters of files.\nBy typing the command line:\n\nls | wc -l [enter]\n\n\nThe above command uses wc to count the number of files that are listed by ls. The \u2018-l\u2019 option tells wc to return a count of the number of lines. The | symbol (known as the \u2018pipe\u2019 character) in the command line connects the two commands into a single operation for simplicity. You can connect as many commands as you want:\n\n\nls | grep \".embl\" | wc -l\n\nThis command will list out all of the files in the current directory, then send the results to the grep command which searches for all filenames containing the \u2018embl\u2019, then sends the results to wc which counts the number of lines (which corresponds to the number of files).\n\n\n\n  \n\n\n\n\n\ngrep\n\nSearches a file for patterns. \ngrep\n is a powerful tool to search for patterns in a file. In the examples below, we are going to use the file called Malaria.fasta that contains the set of \nP. falciparum\n chromosomes in FASTA format. A FASTA file has the following format:\n\n\n\n\n\n\nSequence Header\nCTAAACCTAAACCTAAACCCTGAACCCTAA...\n\n\n\n\n\n\nTherefore if we want to get the sequence headers, we can extract the lines that match the \u2018>\u2019 symbol:\n\n\ngrep \u2018>\u2019 Malaria.fasta [enter]\n\n\nBy typing the command line:\n\n\ngrep -B 1 -A 1 'aagtagggttca' Malaria.fasta [enter]\n\n\nThis command will search for a nucliotide sequence and print 1 line before and after any match. It won\u2019t find the pattern if it spans more than 1 line.\n\n\n\n  \n\n\n\n\n\nfind\n\nFinds files matching an expression. The find command is similar to ls but in many ways it is more powerful. It can be used to recursively search the directory tree for a specified path name, seeking files that match a given Boolean expression (a test which returns true or false)\n\n\nfind . -name \u201c*.embl\u201d\n This command will return the files which name has the .embl suffix.\n\n\nmkdir test_directory\n\n\nfind . -type d\n\n\nThis command will return all the subdirectories contained in the current directory. These are just two basic examples but it is possible to search in many other ways: \n-mtime\n search files by modifying date \n-atime\n search files by last access date \n-size\n search files by file size \n-user\n search files by user they belong to.\n\n\nTips\n\u00b6\n\n\nYou need to be careful with quoting when using wildcards!\n\n\nThe wildcard * symbol represents a string of any character and of any length.\n\n\n\n  \n\n\nFor more information on Unix command see EMBNet UNIX Quick Guide. \u00a0\n\n\n\n  End of the module\n\n\n# Introduction to Unix (continued)\n\nIn this part of the Unix tutorial, you will learn to download files, compress and decompress them, and combine commands.\n\n## Download files\n\n`wget` can be used to download files from the internet and store them.\n\n`wget https://raw.githubusercontent.com/HadrienG/tutorials/master/LICENSE`\n\nwill download the file that is located at the above URL on the internet, and put it **in the current directory**. This is the license under which this course is released. Open it and read it if you like!\n\nThe `-O` option can be used to change the output file name.\n\n`wget -O GNU_FDL.txt https://raw.githubusercontent.com/HadrienG/tutorials/master/LICENSE`\n\nYou can also use wget to download a file list using -i option and giving a text file containing file URLs. The following\n\n\n\u0002wzxhzdk:2\u0003\n\n\n`wget -i download-file-list.txt`\n\n## Compressing and decompressing files\n\n### Compressing files with gzip\n\ngzip is a utility for compressing and decompressing individual files. To compress files, use:\n\n`gzip filename`\n\nThe filename will be deleted and replaced by a compressed file called filename.gz. To reverse the compression process, use:\n\n`gzip -d filename.gz`\n\nTry it on the License you just downloaded!\n\n### Tar archives\n\nQuite often, you don't want to compress just one file, but rather a bunch of them, or a directory.\n\ntar backs up entire directories and files as an archive. An archive is a file that contains other files plus information about them, such as their filename, owner, timestamps, and access permissions. tar does not perform any compression by default.\n\nTo create a gzipped disk file tar archive, use\n\n`tar -czvf archivename filenames`\n\nwhere archivename will usually have a .tar.gz extension\n\nThe c option means create, the v option means verbose (output filenames as they are archived), option f means file, and z means that the tar archive should be gzip compressed.\n\nTo list the contents of a gzipped tar archive, use\n\n`tar -tzvf archivename`\n\nTo unpack files from a tar archive, use\n\n`tar -xzvf archivename`\n\nTry to archive the folder `Module_Unix` from the previous exercise!\n\nYou will notice a file called tutorials.tar.bz2 in your home directory. This is also a compressed archive, but compressed in the bzip format. Read the tar manual and find a way to decompress it.\n\nHint: you can read the manual for any command using `man`\n\n`man tar`\n\n### Redirection\n\nSome commands give you an output to your screen, but you would have preferred it to go into another program or into a file. For those cases you have some redirection characters.\n\n#### Output redirection\n\nThe output from a command normally intended for standard output (that is, your screen) can be easily diverted to a file instead. This capability is known as output redirection:\n\nIf the notation `> file` is appended to any command that normally writes its output to standard output, the output of that command will be written to file instead of your terminal.\n\nI.e, the following who command:\n\n`who > users.txt`\n\nNo output appears at the terminal. This is because the output has been redirected into the specified file.\n\n`less users.txt`\n\nBe careful, if a command has its output redirected to a file and the file already contains some data, that data will be lost. Consider this example:\n\n`echo Hello > users.txt`\n\n`less users.txt`\n\nYou can use the `>>` operator to append the output in an existing file as follows:\n\n\n\u0002wzxhzdk:3\u0003\n\n\n`less users.txt`\n\n#### Piping\n\nYou can connect two commands together so that the output from one program becomes the input of the next program. Two or more commands connected in this way form a pipe.\n\nTo make a pipe, put a vertical bar `|` on the command line between two commands.\n\nRemember the command `grep`? We can pipe other commands to it, to refine searches per example:\n\n`ls -l ngs_course_data | grep \"Jan\"`\n\nwill only give you the files and directories created in January.\n\nTip: There are various options you can use with the grep command, look at the manual!\n\nPipes are extremely useful to connect various bioinformatics software together. We'll use them extensively later.\n# Introduction to Unix (continued)\n\nIn this part of the tutorial, we'll learn how to install programs in a Unix system\n\n## Using a package manager\n\nThis is the most straight-forward way, and the way used by most of the people using unix at home,\nor administrating their own machine.\n\nThis course is aimed at giving you a working knowledge of linux for bioinformatics, and in that setting, you will rarely, if ever, be the administrator of your own machine. The methods below are here as an information\n\n### On Ubuntu and Debian: Apt\n\nTo install a software:\n\n`apt-get install name_of_the_software`\n\nto uninstall:\n\n`apt-get remove name_of_the_software`\n\nto update all installed softwares:\n\n\n\u0002wzxhzdk:4\u0003\n\n\n### On Fedora, CentOS and RedHat: yum\n\nTo install a software:\n\n`yum install name_of_the_software`\n\nto uninstall:\n\n`yum remove name_of_the_software`\n\nto update:\n\n`yum update`\n\n### MacOS: brew\n\nAlthough there are no official package managers on MacOS, two popular, community-driven alternatives exist: macports and brew.\n\nBrew is particularly pupular within the bioinformatics community, and allows easy installation of many bioinformatics softwares on MacOS\n\nTo install brew on your mac:\n\n`/usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"`\n\nTo install a software:\n\n`brew install name_of_the_software`\n\nTo uninstall:\n\n`brew uninstall name_of_the_software`\n\nTo update all brew-installed softwares:\n\n\n\u0002wzxhzdk:5\u0003\n\n\nMore info on [brew.sh](http://brew.sh) and [brew.sh/homebrew-science/](http://brew.sh/homebrew-science/)\n\n## Downloading binaries\n\nIn a university setting, you will rarely by administrator of your own machine. This is a very good thing for one reason: it's harder for you to break something!\n\nThe downside is that it makes installing softwares more complicated. We'll start wit simply downloading the software and executing it, then we'll learn how to obtain packages from source code.\n\nfor example, we'll install the blast binaries:\n\nFirst, download the archive:\n`wget ftp://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/ncbi-blast-2.6.0+-x64-linux.tar.gz`\n\nthen unpack it and go to the newly created directory\n\n\n\u0002wzxhzdk:6\u0003\n\n\nyou should have a `bin` directory, go inside and look at the files. You have a bunch of executable files.\n\n### Execute a file\n\n\nMost of the lunix commands that you execute on a regular basis (ls, cp, mkdir) are located in `/usr/bin`, but you don't have to invoke them with their full path: i.e. you dont type `/usr/bin/ls` but just `ls`. This is because `/usr/bin/ls` is in your $PATH.\n\nto execute a file that you just downloaded, and is therefore not in your path, you have to type the absolute or relative path to that file. Meaning, for the blast program suite that we just downloaded:\n\n`bin/blastn -help`\n\nor\n\n\n\u0002wzxhzdk:7\u0003\n\n\nand that's it!\nBut it is not very convenient. You want to be able to execute blast without having to remember where it is. If you have administrator rights (sudo), you can move the software in `/usr/bin`. If you don't you can modify your $PATH in a configuration file called `.bash_profile` that is located in your home.\n\nMore information on how to correctly modify your PATH [here](http://unix.stackexchange.com/a/26059)\n\n## Compiling from source\n\nSometimes pre-compiled binaries are not available. You then have to compile from source: transforming the human-readable code (written in one or another programming language) into machine-readable code (binary)\n\nThe most common way to do so, if a software package has its source coud available online is\n\n\n\u0002wzxhzdk:8\u0003\n\n\nIf you don't have the administrator rights, you'll often have to pass an extra argument to ./configure:\n\n\n\u0002wzxhzdk:9\u0003\n\n\nMost of the softwares come with instructions on how to install them. Always read the file called README or INSTALL in the package directory before installing!\n\n### Exercice\n\nThe most popular unix distributions come with a version of python (a programming language) that is not the most recent one. Install from source the most recent version of python in a folder called `bin` in your home directory.\n\nYou can download the python source code at https://www.python.org/ftp/python/3.6.0/Python-3.6.0.tgz\n\n## Install python packages\n\nPython is a really popular programming language in the world of bioinformatics. Python has a package manager called `pip` that you can use to install softwares written in python.\n\nPlease us the python executable you installed in the above exercise!\n\nFirstly, get pip:\n\n`wget https://bootstrap.pypa.io/get-pip.py`\n\nthen execute the script\n\n`python get-pip.py`\n\nThenm you can use pip to install package, either globally (if you're an administrator):\n\n`pip install youtube_dl`\n\nor just for you:\n\n`pip install --user youtube_dl`\n\n## Final exercise\n\nOne of the oldest and most famous bioinformatics package is called EMBOSS.\nInstall EMBOSS in the bin directory of your home. Good luck!\n# Introduction to UNIX (continued)\n\nIn the 4th and last module of your unix course, we'll how to write small programs, or scripts.\n\nShell scripts allow us to program commands in chains and have the system execute them as a scripted chain of events. They also allow for far more useful functions, such as command substitution. You can invoke a command, like date, and use it\u2019s output as part of a file-naming scheme. You can automate backups and each copied file can have the current date appended to the end of its name. You can automate a bioinformatics analysis pipeline.\n\nBefore we begin our scripting tutorial, let\u2019s cover some basic information. We\u2019ll be using the bash shell, which most Linux distributions use natively. Bash is available for Mac OS users and Cygwin on Windows (which you are using with MobaXterm). Since it\u2019s so universal, you should be able to script regardless of your platform.\n\nAt their core, scripts are just plain text files. You can use nano (or any other text editor) to write them.\n\n## Permissions\n\nScripts are executed like programs. For this to happen, you need to have the proper permissions.\nYou can make the script executable for you by running the following command:\n\n`chmod u+x my_script.sh`\n\nby convention, bash script are saved with the .sh extension. Linux doesn't really care about file extension, but it is easier for the user to use the \"proper\" extensions!\n\n## executing a script\n\nYou have to cd in the proper directory, then run the script like this:\n\n`./my_script.sh`\n\nTo make things more convenient, you can place scripts in a \u201cbin\u201d folder in your home directory and add it to your path\n\n`mkdir -p ~/bin`\n\nMore information on how to correctly modify your PATH [here](http://unix.stackexchange.com/a/26059)\n\n## Getting started\n\nAs previously said, every script is a text file. Still, there are rules and conventions to follow in order of you file being recognized as a script\n\nIf you juste write a few command and try to execute it as is, with `./my_script`, it will not work. You can invoke `sh my_script`, but it is not very convenient.\n`./` tries to find out which interpreter to use (e.g. which programming language and how to execute your script). It does so by looking at the first line:\n\nThe first line of your bash scripts should be:\n\n`#!/bin/bash` or `#!/usr/bin/env bash`\n\nThe second version being better and more portable. Ask your teacher why!\n\nThis line will have the same syntax for every interpreted language. If you are programming in python:\n\n`#!/usr/bin/env python`\n\n### New line = new command\n\nAfter the firstline, every line of your script will be a new command. Your first scripts will essentially be a succession of terminal commands. We'll learn about flow control (if, for, while, ...) later on.\n\n### Comments\n\nIt is good practise to comment your scripts, i.e give some explanation of what is does, and explain a particularly arcane method that you wrote.\n\nComments start with a `#` and are snippets of texts that are ignored by the interpreter.\n\n### Your first script\n\nLet's start with a simple script, that copy files and append today's date to the end of the file name. We'll call it `datecp.sh`\n\nIn your `~/bin` folder:\n\n\n\u0002wzxhzdk:10\u0003\n\n\nand let's start writing our script\n\n`nano datecp.sh`\n\n\n\u0002wzxhzdk:11\u0003\n\n\nNext, we need to declare a variable. A variable allows us to store and reuse information (characters, the date or the command `date`). Variables have a name, but can **expend** to their content when referenced if they contain a command.\n\nVariables can hold strings and characers, like this:\n\n`my_variable=\"hippopotamus\"`\n\nor a command. In bash, the correct way to store a command in a variable is within the syntax `$()`:\n\n`variable=$(command \u2013options arguments)`\n\nStore the date and time in a variable. Test the date command first in your terminal, then when you got the right format, store it in a variable in your script.\n\nIt is generally bad practice to put spaces in file names in unix, so we'll want the following date format:\n\n`date +%m_%d_%y-%H.%M.%S`\n\nand for putting it into a variable:\n\ndate_formatted=$(date +%m_%d_%y-%H.%M.%S)\n\nYour script now can print thedate without too much more coding:\n\n\n\u0002wzxhzdk:12\u0003\n\n\nNow we need to add the copying part:\n\n`cp \u2013iv $1 $2.$date_formatted`\n\nThis will invoke the copy command, with two options: -i for asking for permission before overwriting a file, and -v for verbose.\n\nYou can also notice two variables: $1 and $2. When scripting in bash, a dollar sign ($) followed by a number will denote an argument of the script. For example in the following command:\n\n`cp \u2013iv a_file a_file_copy` the first argument ($1) is `a_file` and the second argument ($2) is `a_file_copy`\n\nWhat our script will do is a simple copy of a file, but with adding the date to the end of the file name. Save it and try it out!\n\n### Exercise\n\nWrite a script that backs itself up, that is, copies itself to a file named backup.sh.\n\nHint: Use the cat command and the appropriate positional parameter.",
            "title": "The command-line"
        },
        {
            "location": "/command_line/#the-command-line",
            "text": "This tutorial is largely inspired of the  Introduction to UNIX  course from the Sanger Institute.  The aim of this module is to introduce Unix and cover some of the basics that will allow you to be more comfortable with the command-line. Several of the programs that you are going to use during this course are useful for bioinformatics analyses. This module is only designed to provide a very brief introduction to some of the features and useful commands of Unix. During this module we will also obtain a genome sequence and examine the basic structure of an EMBL entry.",
            "title": "The command-line"
        },
        {
            "location": "/command_line/#introduction",
            "text": "Unix is the standard operating system on most large computer systems in scientific research, in the same way that Microsoft Windows is the dominant operating system on desktop PCs. Unix and MS Windows both perform the important job of managing the computer\u2019s hardware (screen, keyboard, mouse, hard disks, network connections, etc...) on your behalf. They also provide you with tools to manage your files and to run application software. They both offer a graphical user interface (desktop). The desktops look different, call things by different names but they mostly can do the same things. Unix is a powerful, secure, robust and stable operating system that allows dozens of people to run programs on the same computer at the same time. This is why it is the preferred operating system for large-scale scientific computing. It is run on all kind of machines, like mobile phones (Android), desktop PCs, kitchen appliances,... all the way up to supercomputers. Unix powers the majority of the Internet.",
            "title": "Introduction"
        },
        {
            "location": "/command_line/#aims",
            "text": "The aim of this course is to introduce Unix and cover the basics. The programs that you are going to use during the courses, plus many others that are useful for bioinformatics analyses, are run in Unix. This module is only designed to provide a very brief introduction to some of the features and useful commands of Unix. During this module we will also obtain a genome sequence and examine the basic structure of an EMBL entry.",
            "title": "Aims"
        },
        {
            "location": "/command_line/#why-use-unix",
            "text": "Unix is a well established, very widespread operating system. You probably have a device running on Unix in your home without realising it (e.g. playstation, TV box, wireless router, android tablets/phones,...  Command line driven, with a huge number of often terse, but powerful commands.  In contrast to Windows, it is designed to allow many users to run their programs simultaneously on the same computer.  Designed to work in computer networks - for example, most of the Internet is Unix based.  It is used on many of the powerful computers at bioinformatics centres and also on many desktops and laptops (MacOS is largely UNIX compatible).  The major difference between Unix and Windows is that it is free (as in freedom) and you can modify it to work however you want. This same principle of freedom is also used in most bioinformatics software.  There are many distributions of Unix such as Ubuntu, RedHat, Fedora, Mint,...). These are all Unix, but they bundle up extra software in a different way or combinations. Some are known for being conservative and reliable; whilst others are know for being cutting-edge (and less reliable).  The MacOSX operating system used by the  eBioKit  is also based on Unix.",
            "title": "Why use Unix?"
        },
        {
            "location": "/command_line/#getting-started",
            "text": "For this course, you will have to connect to the eBiokit using SSH. SSH stands for Secure Shell and is a network protocol used to securely connect to a server. To do so, you will need an SSH client:   On Linux: it is included by default, named Terminal.  On MacOS: it is included by default, also named Terminal.  On Windows: you'll have to download and install  MobaXterm , a terminal emulator.   Once you've opened your terminal (or terminal emulator), type  ssh username@ip_address  replacing  username  and  ip_address  with your username and the ip address of the server you are connecting to.\nType your password when prompted. As you type, nothing will show on screen. No stars, no dots.\nIt is supposed to be that way. Just type the password and press enter!   You can type commands directly into the terminal at the \u2018$' prompt. A list of useful commands can be found on the next page. Many of them are two- or three-letter abbreviations. The earliest Unix systems ( circa  1970) only had slow Teletype terminals, so it was faster to type 'rm' to remove a file than 'delete' or 'erase'. This terseness is a feature of Unix that still survives.",
            "title": "Getting started"
        },
        {
            "location": "/command_line/#the-command-line_1",
            "text": "All Unix programs may be run by typing commands at the Unix prompt. The command line tells the computer what to do. You may subtly alter these commands by specifying certain options when typing in the command line.",
            "title": "The command line"
        },
        {
            "location": "/command_line/#command-line-arguments",
            "text": "Typing any Unix command for example  ls ,  mv  or  cd  at the Unix prompt with the appropriate variables such as files names or directories will result in the tasks being performed on pressing the enter key.   The command is separated from the options and arguments by a space. Additional options and/or arguments can be added to the commands to affect the way the command works. Options usually have one dash and a letter (e.g. -h) or two dashes and a word (--help) with no space between the dash and the letter/word. Arguments are usually filenames or directories.  For example: List the contents of a directory  ls  List the contents of a directoryList the contents of a directory with extra information about the files ls \u2013l  List the contents of a directory with extra information about the files ls \u2013a  List all contents including hidden files & directories ls -al  List all contents including hidden files & directories, with extra information about the files ls \u2013l /usr/  List the contents of the directory /usr/, with extra information about the files",
            "title": "Command line Arguments"
        },
        {
            "location": "/command_line/#files-and-directories",
            "text": "Directories are the Unix equivalent of folders on a PC or Mac. They are organised in a hierarchy, so directories can have sub-directories. Directories are very useful for organising your work and keeping your account tidy - for example, if you have more than one project, you can organise the files for each project into different directories to keep them separate. You can think of directories as rooms in a house. You can only be in one room (directory) at a time. When you are in a room you can see everything in that room easily. To see things in other rooms, you have to go to the appropriate door and crane your head around. Unix works in a similar manner, moving from directory to directory to access files. The location or directory that you are in is referred to as the current working directory.  Directory structure example   Therefore if there is a file called genome.seq in the  dna  directory its location or full pathname can be expressed as /nfs/dna/genome.seq.",
            "title": "Files and Directories"
        },
        {
            "location": "/command_line/#general-points",
            "text": "Unix is pretty straightforward, but there are some general points to remember that will make your life easier: most flavors of UNIX are case sensitive - typing  ls  is generally not the same as typing  LS . You need to put a space between a command and its argument - for example,  less my_file  will show you the contents of the file called my_file;  lessmyfile  will just give you an error! Unix is not psychic: If you misspell the name of a command or the name of a file, it will not understand you. Many of the commands are only a few letters long; this can be confusing until you start to think logically about why those letters were chosen - ls for list, rm for remove and so on. Often when you have problems with Unix, it is due to a spelling mistake, or perhaps you have omitted a space. If you want to know more about Unix and its commands there are plenty of resources available that provide a more comprehensive guide (including a cheat sheet at the end of this chapter.   http://unix.t-a-y-l-o-r.com/   In what follows, we shall use the following typographical conventions: Characters written in  bold typewriter font  are commands to be typed into the computer as they stand. Characters written in  italic typewriter font  indicate non-specific file or directory names. Words inserted within square brackets [Ctrl] indicate keys to be pressed. So, for example, $ **ls** *any_directory* [Enter]  means \"at the Unix prompt $, type ls followed by the name of some directory, then press Enter\"\nDon't forget to press the [Enter] key: commands are not sent to the computer until this is done.",
            "title": "General Points"
        },
        {
            "location": "/command_line/#some-useful-unix-commands-command-and-what-it-does",
            "text": "Command  What it does      ls  Lists the contents of the current directory    mkdir  Creates a new directory    mv  Moves or renames a file    cp  Copies a file    rm  Removes a file    cat  Concatenates files    less  Displays the contents of a file one page at a time    head  Displays the first ten lines of a file    tail  Displays the last ten lines of a file    cd  Changes current working directory    pwd  Prints working directory    find  Finds files matching an expression    grep  Searches a file for patterns    wc  Counts the lines, words, characters, and bytes in a file    kill  Stops a process    jobs  Lists the processes that are running",
            "title": "Some useful Unix commands Command\u00a0and What it does"
        },
        {
            "location": "/command_line/#firts-steps",
            "text": "The following exercise introduces a few useful Unix commands and provides examples of how they can be used. Many people panic when they are confronted with an Unix prompt! Don\u2019t! The exercise is designed to be step-by-step, so all the commands you need are provided in the text. If you get lost ask a demonstrator. If you are a person skilled at Unix, be patient it is only a short exercise. Finding where you are and what you\u2019ve got  pwd \u00a0\u00a0\u00a0\u00a0\u00a0\nPrint the working directory As seen previously directories are arranged in a hierarchical structure. To determine where you are in the hierarchy you can use the pwd command to display the name of the current working directory. The current working directory may be thought of as the directory you are in, i.e. your current position in the file-system tree To find out where you are type  pwd [enter]  You will see that you are in your home directory. We need to move into the ngs_course_data directory. Remember, Unix is case sensitive  PWD  is not the same as  pwd  \n     cd \u00a0\nChange current working directory The cd command will change the current working directory to another, in other words allow you to move up or down in the directory hierarchy. First of all we are going to move into the \"ngs_course_data\" directory below. To do this type:  cd ngs_course_data [enter]  Now use the pwd command to check your location in the directory hierarchy.  Change again the directory to  Module_Unix     ls \nList the contents of a directory To find out what are the contents of the current directory type  ls  [enter] The ls command lists the contents of your current directory, this includes files and directories You should see that there are several other directories.  Now use the  cd  command again to change to the  Module_Unix  directory.",
            "title": "Firts steps"
        },
        {
            "location": "/command_line/#changing-and-moving-what-youve-got",
            "text": "cp \nCopy a file.  cp file1 file2  is the command which makes a copy of file1 in the current working directory and calls it file2! What you are going to do is make a copy of AL513382.embl. This file contains the genome of Salmonella typhi strain CT18 in EMBL format (we'll learn more about file formats later during the course). The new file will be called S_typhi.embl.  cp AL513382.embl S_typhi.embl [enter] \nIf you use the ls command to check the contents of the current directory you will see that there is an extra file called S_typhi.embl.  \n  \u00a0   rm \nDelete a file. This command removes a file permanently, so be careful! You are now going to remove the old version of the  S. typhi  genome file, AL513382.embl rm AL513382.embl [enter]  The file will be removed. Use the  ls  command to check the contents of the current directory to see that AL513382.embl has been removed.  Unix, as a general rule does exactly what you ask, and does not ask for confirmation. Unfortunately there is no \"recycle bin\" on the command line to recover the file from, so you have to be careful.  \n  \u00a0   cd \nChange current working directory. As before the cd command will change the current working directory to another, in other words allow you to move up or down in the directory hierarchy. First of all we are going to move into the directory above, type: cd .. [enter]  Now use the  pwd  command to check your location in the directory hierarchy. Next, we are going to move into the Module_Artemis directory. To change to the Module_Artemis directory type: cd Module_Artemis [enter]  use the ls command to check the contents of the directory.",
            "title": "Changing and moving what you\u2019ve got"
        },
        {
            "location": "/command_line/#tips",
            "text": "There are some short cuts for referring to directories:  . \u00a0Current directory (one full stop)  \n.. Directory above (two full stops)  \n~ \u00a0Home directory (tilde)  \n/ \u00a0Root of the file system (like C:\\ in Windows)  Pressing the tab key twice will try and autocomplete what you\u2019ve started typing or give you a list of all possible completions. This saves a lot of typing and typos. Pressing the up/down arrows will let you scroll through the previous commands. If you highlight some text, middle clicking will paste it on the command line.  mv \nMove a file. To move a file from one place to another use the  mv  command. This moves the file rather than copies it, therefore you end up with only one file rather than two. When using the command the path or pathname is used to tell Unix where to find the file. You refer to files in other directories by using the list of hierarchical names separated by slashes. For example, the file bases in the directory genome has the path genome/bases If no path is specified Unix assumes that the file is in the current working directory. What you are going to do is move the file S_typhi.embl from the Module_Unix directory, to the current working directory. mv ../Module_Unix/S_typhi.embl . [enter] \nUse the ls command to check the contents of the current directory to see that S_typhi.embl has been moved. ../Module_Unix/S_typhi.embl specifies that S_typhi.embl is in the Module_Unix directory. If the file was in the directory above, the path would change to: ../ S_typhi.embl  \n     The command can also be used to rename a file in the current working directory. Previously we used the cp command, but mv provides an alternative without the need to delete the original file. Therefore we could have used: \u00a0  mv AL513382.embl S_typhi.embl [enter]  instead of:  cp AL513382.embl S_typhi.embl [enter]\nrm AL513382.embl [enter]",
            "title": "Tips"
        },
        {
            "location": "/command_line/#viewing-what-youve-got",
            "text": "less \nDisplay file contents. This command displays the contents of a specified file one screen at a time. You are now going to look at the contents of S_typhi.embl. less S_typhi.embl [enter] \nThe contents of S_typhi.embl will be displayed one screen at a time, to view the next screen press the space bar. less can also scroll backwards if you hit the b key. Another useful feature is the slash key, /, to search for a word in the file. You type the word you are looking for and press enter. The screen will jump to the next occurrence and highlight it. As S_typhi.embl is a large file this will take a while, therefore you may want to escape or exit from this command. To exit press the letter \u2018q\u2019. If you really need to exit from a program and it isn\u2019t responding press \u2018control\u2019 and the letter \u2018c\u2019 at the same time.  \n  \u00a0   head   \nDisplay the first ten lines of a file  tail \u00a0 \nDisplay the last ten lines of a file    Sometimes you may just want to view the text at the beginning or the end of a file, without having to display all of the file. The head and tail commands can be used to do this. You are now going to look at the beginning of S_typhi.embl.  head S_typhi.embl [enter]  To look at the end of S_typhi.embl type:  tail S_typhi.embl [enter] \nThe number of lines that are displayed can be increased by adding extra arguments. To increase the number of lines viewed from 10 to 100 add the \u2013100 argument to the command. For example to view the last 100 lines of S_typhi.embl type:  tail -100 S_typhi.embl [enter]  Do this for both head and tail commands. What type of information is at the beginning and end of the EMBL format file?  \n  \u00a0   cat \nJoin files together. Having looked at the beginning and end of the S_typhi.embl file you should notice that in EMBL format files the annotation comes first, then the DNA sequence at the end. If you had two separate files containing the annotation and the DNA sequence, both in EMBL format, it is possible to concatenate or join the two together to make a single file like the S_typhi.embl file you have just looked at. The Unix command cat can be used to join two or more files into a single file. The order in which the files are joined is determined by the order in which they appear in the command line. For example, we have two separate files, MAL13P1.dna and MAL13P1.tab, that contain the DNA and annotation, respectively, from the  P. falciparum  genome. Return to the Module_Unix directory using the cd command: cd ../Module_Unix [enter]  and type cat MAL13P1.tab MAL13P1.dna > MAL13P1.embl [enter] \nMAL13P1.tab and MAL13P1.dna will be joined together and written to a file called MAL13P1.embl\nThe  >  symbol in the command line directs the output of the cat program to the designated file MAL13P1.embl  \n  \u00a0   wc \nCounts the lines, words or characters of files.\nBy typing the command line: ls | wc -l [enter]  The above command uses wc to count the number of files that are listed by ls. The \u2018-l\u2019 option tells wc to return a count of the number of lines. The | symbol (known as the \u2018pipe\u2019 character) in the command line connects the two commands into a single operation for simplicity. You can connect as many commands as you want:  ls | grep \".embl\" | wc -l \nThis command will list out all of the files in the current directory, then send the results to the grep command which searches for all filenames containing the \u2018embl\u2019, then sends the results to wc which counts the number of lines (which corresponds to the number of files).  \n     grep \nSearches a file for patterns.  grep  is a powerful tool to search for patterns in a file. In the examples below, we are going to use the file called Malaria.fasta that contains the set of  P. falciparum  chromosomes in FASTA format. A FASTA file has the following format:    Sequence Header\nCTAAACCTAAACCTAAACCCTGAACCCTAA...    Therefore if we want to get the sequence headers, we can extract the lines that match the \u2018>\u2019 symbol:  grep \u2018>\u2019 Malaria.fasta [enter]  By typing the command line:  grep -B 1 -A 1 'aagtagggttca' Malaria.fasta [enter]  This command will search for a nucliotide sequence and print 1 line before and after any match. It won\u2019t find the pattern if it spans more than 1 line.  \n     find \nFinds files matching an expression. The find command is similar to ls but in many ways it is more powerful. It can be used to recursively search the directory tree for a specified path name, seeking files that match a given Boolean expression (a test which returns true or false)  find . -name \u201c*.embl\u201d  This command will return the files which name has the .embl suffix.  mkdir test_directory  find . -type d  This command will return all the subdirectories contained in the current directory. These are just two basic examples but it is possible to search in many other ways:  -mtime  search files by modifying date  -atime  search files by last access date  -size  search files by file size  -user  search files by user they belong to.",
            "title": "Viewing what you\u2019ve got"
        },
        {
            "location": "/command_line/#tips_1",
            "text": "You need to be careful with quoting when using wildcards!  The wildcard * symbol represents a string of any character and of any length.  \n   \n\nFor more information on Unix command see EMBNet UNIX Quick Guide.",
            "title": "Tips"
        },
        {
            "location": "/file_formats/",
            "text": "File Formats\n\u00b6\n\n\nThis lecture is aimed at making you discover the most popular file formats used in bioinformatics. You're expected to have basic working knowledge of Linux to be able to follow the lesson.\n\n\nTable of Contents\n\u00b6\n\n\n\n\nThe fasta format\n\n\nThe fastq format\n\n\nThe sam/bam format\n\n\nThe vcf format\n\n\nThe gff format\n\n\n\n\nThe fasta format\n\u00b6\n\n\nThe fasta format was invented in 1988 and designed to represent nucleotide or peptide sequences. It originates from the \nFASTA\n software package, but is now a standard in the world of bioinformatics.\n\n\nThe first line in a FASTA file starts with a \">\" (greater-than) symbol followed by the description or identifier of the sequence. Following the initial line (used for a unique description of the sequence) is the actual sequence itself in standard one-letter code.\n\n\nA few sample sequences:\n\n\n>KX580312.1 Homo sapiens truncated breast cancer 1 (BRCA1) gene, exon 15 and partial cds\nGTCATCCCCTTCTAAATGCCCATCATTAGATGATAGGTGGTACATGCACAGTTGCTCTGGGAGTCTTCAG\nAATAGAAACTACCCATCTCAAGAGGAGCTCATTAAGGTTGTTGATGTGGAGGAGTAACAGCTGGAAGAGT\nCTGGGCCACACGATTTGACGGAAACATCTTACTTGCCAAGGCAAGATCTAG\n\n\n\n\n>KRN06561.1 heat shock [Lactobacillus sucicola DSM 21376 = JCM 15457]\nMSLVMANELTNRFNNWMKQDDFFGNLGRSFFDLDNSVNRALKTDVKETDKAYEVRIDVPGIDKKDITVDY\nHDGVLSVNAKRDSFNDESDSEGNVIASERSYGRFARQYSLPNVDESGIKAKCEDGVLKLTLPKLAEEKIN\nGNHIEIE\n\n\n\n\nA fasta file can contain multiple sequence. Each sequence will be separated by their \"header\" line, starting by \">\".\n\n\nExample:\n\n\n>KRN06561.1 heat shock [Lactobacillus sucicola DSM 21376 = JCM 15457]\nMSLVMANELTNRFNNWMKQDDFFGNLGRSFFDLDNSVNRALKTDVKETDKAYEVRIDVPGIDKKDITVDY\nHDGVLSVNAKRDSFNDESDSEGNVIASERSYGRFARQYSLPNVDESGIKAKCEDGVLKLTLPKLAEEKIN\nGNHIEIE\n>3HHU_A Chain A, Human Heat-Shock Protein 90 (Hsp90)\nMPEETQTQDQPMEEEEVETFAFQAEIAQLMSLIINTFYSNKEIFLRELISNSSDALDKIRYESLTDPSKL\nDSGKELHINLIPNKQDRTLTIVDTGIGMTKADLINNLGTIAKSGTKAFMEALQAGADISMIGQFGVGFYS\nAYLVAEKVTVITKHNDDEQYAWESSAGGSFTVRTDTGEPMGRGTKVILHLKEDQTEYLEERRIKEIVKKH\nSQFIGYPITLFVEK\n\n\n\n\nThe fastq format\n\u00b6\n\n\nThe fastq format is also a text based format to represent nucleotide sequences, but also contains the corresponding quality of each nucleotide. It is the standard for storing the output of high-throughput sequencing instruments such as the Illumina machines.\n\n\nA fastq file uses four lines per sequence:\n\n\n\n\nLine 1 begins with a '@' character and is followed by a sequence identifier and an optional description (like a FASTA title line).\n\n\nLine 2 is the raw sequence letters.\n\n\nLine 3 begins with a '+' character and is optionally followed by the same sequence identifier (and any description) again.\n\n\nLine 4 encodes the quality values for the sequence in Line 2, and must contain the same number of symbols as letters in the sequence.\n\n\n\n\nAn example sequence in fastq format:\n\n\n@SEQ_ID\nGATTTGGGGTTCAAAGCAGTATCGATCAAATAGTAAATCCATTTGTTCAACTCACAGTTT\n+\n!''*((((***+))%%%++)(%%%%).1***-+*''))**55CCF>>>>>>CCCCCCC65\n\n\n\n\nQuality\n\u00b6\n\n\nThe quality, also called phred score, is the probability that the corresponding basecall is incorrect.\n\n\nPhred scores use a logarithmic scale, and are represented by ASCII characters, mapping to a quality usually going from 0 to 40.\n\n\n\n\n\n\n\n\nPhred Quality Score\n\n\nProbability of incorrect base call\n\n\nBase call accuracy\n\n\n\n\n\n\n\n\n\n\n10\n\n\n1 in 10\n\n\n90%\n\n\n\n\n\n\n20\n\n\n1 in 100\n\n\n99%\n\n\n\n\n\n\n30\n\n\n1 in 1000\n\n\n99.9%\n\n\n\n\n\n\n40\n\n\n1 in 10,000\n\n\n99.99%\n\n\n\n\n\n\n50\n\n\n1 in 100,000\n\n\n99.999%\n\n\n\n\n\n\n60\n\n\n1 in 1,000,000\n\n\n99.9999%\n\n\n\n\n\n\n\n\nthe sam/bam format\n\u00b6\n\n\nFrom \nWikipedia\n:\n\n\nSAM (file format) is a text-based format for storing biological sequences aligned to a reference sequence developed by Heng Li. The acronym SAM stands for Sequence Alignment/Map. It is widely used for storing data, such as nucleotide sequences, generated by Next generation sequencing technologies and usually mapped to a reference.\n\n\nThe SAM format consists of a header and an alignment section. The binary representation of a SAM file is a BAM file, which is a compressed SAM file.[1] SAM files can be analysed and edited with the software SAMtools.\n\n\nThe SAM format has a really extensive and complex specification that you can find \nhere\n.\n\n\nIn brief it consists of a header section and reads (with other information) in tab delimited format.\n\n\nExample header section\n\u00b6\n\n\n@HD VN:1.0                  SO:unsorted\n@SQ SN:O_volvulusOVOC_OM1a  LN:2816604\n@SQ SN:O_volvulusOVOC_OM1b  LN:28345163\n@SQ SN:O_volvulusOVOC_OM2   LN:25485961\n\n\n\n\nExample read\n\u00b6\n\n\nM01137:130:00-A:17009:1352/14 * 0 0 * * 0 0 AGCAAAATACAACGATCTGGATGGTAGCATTAGCGATGCGACACTGCTTGAACCGTCAAAG FGGFGCFGFFGC8,,@D?E6EFCF,=AEFFGGDGGGADFGG@>FFEGGG:+<7D>AFCFGG YT:Z:UU\n\n\n\n\nthe vcf format\n\u00b6\n\n\nThe vcf format is also a text-based file format. VCF stands for Variant Call Format and is used to store gene sequence variations (SNVs, indels). The format has been developped for genotyping projects, and is the standard to represent variations in the genome of a species.\n\n\nA vcf is a tab-delimited file, described \nhere\n.\n\n\nVCF Example\n\u00b6\n\n\n##fileformat=VCFv4.0\n##fileDate=20110705\n##reference=1000GenomesPilot-NCBI37\n##phasing=partial\n##INFO=<ID=NS,Number=1,Type=Integer,Description=\"Number of Samples With Data\">\n##INFO=<ID=DP,Number=1,Type=Integer,Description=\"Total Depth\">\n##INFO=<ID=AF,Number=.,Type=Float,Description=\"Allele Frequency\">\n##INFO=<ID=AA,Number=1,Type=String,Description=\"Ancestral Allele\">\n##INFO=<ID=DB,Number=0,Type=Flag,Description=\"dbSNP membership, build 129\">\n##INFO=<ID=H2,Number=0,Type=Flag,Description=\"HapMap2 membership\">\n##FILTER=<ID=q10,Description=\"Quality below 10\">\n##FILTER=<ID=s50,Description=\"Less than 50% of samples have data\">\n##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\"Genotype Quality\">\n##FORMAT=<ID=GT,Number=1,Type=String,Description=\"Genotype\">\n##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\"Read Depth\">\n##FORMAT=<ID=HQ,Number=2,Type=Integer,Description=\"Haplotype Quality\">\n#CHROM POS    ID        REF  ALT     QUAL FILTER INFO                              FORMAT      Sample1        Sample2        Sample3\n2      4370   rs6057    G    A       29   .      NS=2;DP=13;AF=0.5;DB;H2           GT:GQ:DP:HQ 0|0:48:1:52,51 1|0:48:8:51,51 1/1:43:5:.,.\n2      7330   .         T    A       3    q10    NS=5;DP=12;AF=0.017               GT:GQ:DP:HQ 0|0:46:3:58,50 0|1:3:5:65,3   0/0:41:3\n2      110696 rs6055    A    G,T     67   PASS   NS=2;DP=10;AF=0.333,0.667;AA=T;DB GT:GQ:DP:HQ 1|2:21:6:23,27 2|1:2:0:18,2   2/2:35:4\n2      130237 .         T    .       47   .      NS=2;DP=16;AA=T                   GT:GQ:DP:HQ 0|0:54:7:56,60 0|0:48:4:56,51 0/0:61:2\n2      134567 microsat1 GTCT G,GTACT 50   PASS   NS=2;DP=9;AA=G                    GT:GQ:DP    0/1:35:4       0/2:17:2       1/1:40:3\nchr1    45796269        .       G       C\nchr1    45797505        .       C       G\nchr1    45798555        .       T       C\nchr1    45798901        .       C       T\nchr1    45805566        .       G       C\nchr2    47703379        .       C       T\nchr2    48010488        .       G       A\nchr2    48030838        .       A       T\nchr2    48032875        .       CTAT    -\nchr2    48032937        .       T       C\nchr2    48033273        .       TTTTTGTTTTAATTCCT       -\nchr2    48033551        .       C       G\nchr2    48033910        .       A       T\nchr2    215632048       .       G       T\nchr2    215632125       .       TT      -\nchr2    215632155       .       T       C\nchr2    215632192       .       G       A\nchr2    215632255       .       CA      TG\nchr2    215634055       .       C       T\n\n\n\n\nthe gff format\n\u00b6\n\n\nThe general feature format (gff) is another text file format, used for describing genes and other features of DNA, RNA and protein sequences. It is the standard for annotation of genomes.\n\n\nA gff file should contain 9 columns, described \nhere\n\n\nExample gff\n\u00b6\n\n\n##description: evidence-based annotation of the human genome (GRCh38), version 25 (Ensembl 85)\n##provider: GENCODE\n##contact: gencode-help@sanger.ac.uk\n##format: gtf\n##date: 2016-07-15\nchr1    HAVANA  gene    11869   14409   .   +   .   gene_id \"ENSG00000223972.5\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; level 2; havana_gene \"OTTHUMG00000000961.2\";\nchr1    HAVANA  transcript  11869   14409   .   +   .   gene_id \"ENSG00000223972.5\"; transcript_id \"ENST00000456328.2\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; transcript_type \"processed_transcript\"; transcript_status \"KNOWN\"; transcript_name \"DDX11L1-002\"; level 2; transcript_support_level \"1\"; tag \"basic\"; havana_gene \"OTTHUMG00000000961.2\"; havana_transcript \"OTTHUMT00000362751.1\";\nchr1    HAVANA  exon    11869   12227   .   +   .   gene_id \"ENSG00000223972.5\"; transcript_id \"ENST00000456328.2\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; transcript_type \"processed_transcript\"; transcript_status \"KNOWN\"; transcript_name \"DDX11L1-002\"; exon_number 1; exon_id \"ENSE00002234944.1\"; level 2; transcript_support_level \"1\"; tag \"basic\"; havana_gene \"OTTHUMG00000000961.2\"; havana_transcript \"OTTHUMT00000362751.1\";\nchr1    HAVANA  exon    12613   12721   .   +   .   gene_id \"ENSG00000223972.5\"; transcript_id \"ENST00000456328.2\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; transcript_type \"processed_transcript\"; transcript_status \"KNOWN\"; transcript_name \"DDX11L1-002\"; exon_number 2; exon_id \"ENSE00003582793.1\"; level 2; transcript_support_level \"1\"; tag \"basic\"; havana_gene \"OTTHUMG00000000961.2\"; havana_transcript \"OTTHUMT00000362751.1\";\nchr1    HAVANA  exon    13221   14409   .   +   .   gene_id \"ENSG00000223972.5\"; transcript_id \"ENST00000456328.2\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; transcript_type \"processed_transcript\"; transcript_status \"KNOWN\"; transcript_name \"DDX11L1-002\"; exon_number 3; exon_id \"ENSE00002312635.1\"; level 2; transcript_support_level \"1\"; tag \"basic\"; havana_gene \"OTTHUMG00000000961.2\"; havana_transcript \"OTTHUMT00000362751.1\";",
            "title": "File Formats"
        },
        {
            "location": "/file_formats/#file-formats",
            "text": "This lecture is aimed at making you discover the most popular file formats used in bioinformatics. You're expected to have basic working knowledge of Linux to be able to follow the lesson.",
            "title": "File Formats"
        },
        {
            "location": "/file_formats/#table-of-contents",
            "text": "The fasta format  The fastq format  The sam/bam format  The vcf format  The gff format",
            "title": "Table of Contents"
        },
        {
            "location": "/file_formats/#the-fasta-format",
            "text": "The fasta format was invented in 1988 and designed to represent nucleotide or peptide sequences. It originates from the  FASTA  software package, but is now a standard in the world of bioinformatics.  The first line in a FASTA file starts with a \">\" (greater-than) symbol followed by the description or identifier of the sequence. Following the initial line (used for a unique description of the sequence) is the actual sequence itself in standard one-letter code.  A few sample sequences:  >KX580312.1 Homo sapiens truncated breast cancer 1 (BRCA1) gene, exon 15 and partial cds\nGTCATCCCCTTCTAAATGCCCATCATTAGATGATAGGTGGTACATGCACAGTTGCTCTGGGAGTCTTCAG\nAATAGAAACTACCCATCTCAAGAGGAGCTCATTAAGGTTGTTGATGTGGAGGAGTAACAGCTGGAAGAGT\nCTGGGCCACACGATTTGACGGAAACATCTTACTTGCCAAGGCAAGATCTAG  >KRN06561.1 heat shock [Lactobacillus sucicola DSM 21376 = JCM 15457]\nMSLVMANELTNRFNNWMKQDDFFGNLGRSFFDLDNSVNRALKTDVKETDKAYEVRIDVPGIDKKDITVDY\nHDGVLSVNAKRDSFNDESDSEGNVIASERSYGRFARQYSLPNVDESGIKAKCEDGVLKLTLPKLAEEKIN\nGNHIEIE  A fasta file can contain multiple sequence. Each sequence will be separated by their \"header\" line, starting by \">\".  Example:  >KRN06561.1 heat shock [Lactobacillus sucicola DSM 21376 = JCM 15457]\nMSLVMANELTNRFNNWMKQDDFFGNLGRSFFDLDNSVNRALKTDVKETDKAYEVRIDVPGIDKKDITVDY\nHDGVLSVNAKRDSFNDESDSEGNVIASERSYGRFARQYSLPNVDESGIKAKCEDGVLKLTLPKLAEEKIN\nGNHIEIE\n>3HHU_A Chain A, Human Heat-Shock Protein 90 (Hsp90)\nMPEETQTQDQPMEEEEVETFAFQAEIAQLMSLIINTFYSNKEIFLRELISNSSDALDKIRYESLTDPSKL\nDSGKELHINLIPNKQDRTLTIVDTGIGMTKADLINNLGTIAKSGTKAFMEALQAGADISMIGQFGVGFYS\nAYLVAEKVTVITKHNDDEQYAWESSAGGSFTVRTDTGEPMGRGTKVILHLKEDQTEYLEERRIKEIVKKH\nSQFIGYPITLFVEK",
            "title": "The fasta format"
        },
        {
            "location": "/file_formats/#the-fastq-format",
            "text": "The fastq format is also a text based format to represent nucleotide sequences, but also contains the corresponding quality of each nucleotide. It is the standard for storing the output of high-throughput sequencing instruments such as the Illumina machines.  A fastq file uses four lines per sequence:   Line 1 begins with a '@' character and is followed by a sequence identifier and an optional description (like a FASTA title line).  Line 2 is the raw sequence letters.  Line 3 begins with a '+' character and is optionally followed by the same sequence identifier (and any description) again.  Line 4 encodes the quality values for the sequence in Line 2, and must contain the same number of symbols as letters in the sequence.   An example sequence in fastq format:  @SEQ_ID\nGATTTGGGGTTCAAAGCAGTATCGATCAAATAGTAAATCCATTTGTTCAACTCACAGTTT\n+\n!''*((((***+))%%%++)(%%%%).1***-+*''))**55CCF>>>>>>CCCCCCC65",
            "title": "The fastq format"
        },
        {
            "location": "/file_formats/#quality",
            "text": "The quality, also called phred score, is the probability that the corresponding basecall is incorrect.  Phred scores use a logarithmic scale, and are represented by ASCII characters, mapping to a quality usually going from 0 to 40.     Phred Quality Score  Probability of incorrect base call  Base call accuracy      10  1 in 10  90%    20  1 in 100  99%    30  1 in 1000  99.9%    40  1 in 10,000  99.99%    50  1 in 100,000  99.999%    60  1 in 1,000,000  99.9999%",
            "title": "Quality"
        },
        {
            "location": "/file_formats/#the-sambam-format",
            "text": "From  Wikipedia :  SAM (file format) is a text-based format for storing biological sequences aligned to a reference sequence developed by Heng Li. The acronym SAM stands for Sequence Alignment/Map. It is widely used for storing data, such as nucleotide sequences, generated by Next generation sequencing technologies and usually mapped to a reference.  The SAM format consists of a header and an alignment section. The binary representation of a SAM file is a BAM file, which is a compressed SAM file.[1] SAM files can be analysed and edited with the software SAMtools.  The SAM format has a really extensive and complex specification that you can find  here .  In brief it consists of a header section and reads (with other information) in tab delimited format.",
            "title": "the sam/bam format"
        },
        {
            "location": "/file_formats/#example-header-section",
            "text": "@HD VN:1.0                  SO:unsorted\n@SQ SN:O_volvulusOVOC_OM1a  LN:2816604\n@SQ SN:O_volvulusOVOC_OM1b  LN:28345163\n@SQ SN:O_volvulusOVOC_OM2   LN:25485961",
            "title": "Example header section"
        },
        {
            "location": "/file_formats/#example-read",
            "text": "M01137:130:00-A:17009:1352/14 * 0 0 * * 0 0 AGCAAAATACAACGATCTGGATGGTAGCATTAGCGATGCGACACTGCTTGAACCGTCAAAG FGGFGCFGFFGC8,,@D?E6EFCF,=AEFFGGDGGGADFGG@>FFEGGG:+<7D>AFCFGG YT:Z:UU",
            "title": "Example read"
        },
        {
            "location": "/file_formats/#the-vcf-format",
            "text": "The vcf format is also a text-based file format. VCF stands for Variant Call Format and is used to store gene sequence variations (SNVs, indels). The format has been developped for genotyping projects, and is the standard to represent variations in the genome of a species.  A vcf is a tab-delimited file, described  here .",
            "title": "the vcf format"
        },
        {
            "location": "/file_formats/#vcf-example",
            "text": "##fileformat=VCFv4.0\n##fileDate=20110705\n##reference=1000GenomesPilot-NCBI37\n##phasing=partial\n##INFO=<ID=NS,Number=1,Type=Integer,Description=\"Number of Samples With Data\">\n##INFO=<ID=DP,Number=1,Type=Integer,Description=\"Total Depth\">\n##INFO=<ID=AF,Number=.,Type=Float,Description=\"Allele Frequency\">\n##INFO=<ID=AA,Number=1,Type=String,Description=\"Ancestral Allele\">\n##INFO=<ID=DB,Number=0,Type=Flag,Description=\"dbSNP membership, build 129\">\n##INFO=<ID=H2,Number=0,Type=Flag,Description=\"HapMap2 membership\">\n##FILTER=<ID=q10,Description=\"Quality below 10\">\n##FILTER=<ID=s50,Description=\"Less than 50% of samples have data\">\n##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\"Genotype Quality\">\n##FORMAT=<ID=GT,Number=1,Type=String,Description=\"Genotype\">\n##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\"Read Depth\">\n##FORMAT=<ID=HQ,Number=2,Type=Integer,Description=\"Haplotype Quality\">\n#CHROM POS    ID        REF  ALT     QUAL FILTER INFO                              FORMAT      Sample1        Sample2        Sample3\n2      4370   rs6057    G    A       29   .      NS=2;DP=13;AF=0.5;DB;H2           GT:GQ:DP:HQ 0|0:48:1:52,51 1|0:48:8:51,51 1/1:43:5:.,.\n2      7330   .         T    A       3    q10    NS=5;DP=12;AF=0.017               GT:GQ:DP:HQ 0|0:46:3:58,50 0|1:3:5:65,3   0/0:41:3\n2      110696 rs6055    A    G,T     67   PASS   NS=2;DP=10;AF=0.333,0.667;AA=T;DB GT:GQ:DP:HQ 1|2:21:6:23,27 2|1:2:0:18,2   2/2:35:4\n2      130237 .         T    .       47   .      NS=2;DP=16;AA=T                   GT:GQ:DP:HQ 0|0:54:7:56,60 0|0:48:4:56,51 0/0:61:2\n2      134567 microsat1 GTCT G,GTACT 50   PASS   NS=2;DP=9;AA=G                    GT:GQ:DP    0/1:35:4       0/2:17:2       1/1:40:3\nchr1    45796269        .       G       C\nchr1    45797505        .       C       G\nchr1    45798555        .       T       C\nchr1    45798901        .       C       T\nchr1    45805566        .       G       C\nchr2    47703379        .       C       T\nchr2    48010488        .       G       A\nchr2    48030838        .       A       T\nchr2    48032875        .       CTAT    -\nchr2    48032937        .       T       C\nchr2    48033273        .       TTTTTGTTTTAATTCCT       -\nchr2    48033551        .       C       G\nchr2    48033910        .       A       T\nchr2    215632048       .       G       T\nchr2    215632125       .       TT      -\nchr2    215632155       .       T       C\nchr2    215632192       .       G       A\nchr2    215632255       .       CA      TG\nchr2    215634055       .       C       T",
            "title": "VCF Example"
        },
        {
            "location": "/file_formats/#the-gff-format",
            "text": "The general feature format (gff) is another text file format, used for describing genes and other features of DNA, RNA and protein sequences. It is the standard for annotation of genomes.  A gff file should contain 9 columns, described  here",
            "title": "the gff format"
        },
        {
            "location": "/file_formats/#example-gff",
            "text": "##description: evidence-based annotation of the human genome (GRCh38), version 25 (Ensembl 85)\n##provider: GENCODE\n##contact: gencode-help@sanger.ac.uk\n##format: gtf\n##date: 2016-07-15\nchr1    HAVANA  gene    11869   14409   .   +   .   gene_id \"ENSG00000223972.5\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; level 2; havana_gene \"OTTHUMG00000000961.2\";\nchr1    HAVANA  transcript  11869   14409   .   +   .   gene_id \"ENSG00000223972.5\"; transcript_id \"ENST00000456328.2\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; transcript_type \"processed_transcript\"; transcript_status \"KNOWN\"; transcript_name \"DDX11L1-002\"; level 2; transcript_support_level \"1\"; tag \"basic\"; havana_gene \"OTTHUMG00000000961.2\"; havana_transcript \"OTTHUMT00000362751.1\";\nchr1    HAVANA  exon    11869   12227   .   +   .   gene_id \"ENSG00000223972.5\"; transcript_id \"ENST00000456328.2\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; transcript_type \"processed_transcript\"; transcript_status \"KNOWN\"; transcript_name \"DDX11L1-002\"; exon_number 1; exon_id \"ENSE00002234944.1\"; level 2; transcript_support_level \"1\"; tag \"basic\"; havana_gene \"OTTHUMG00000000961.2\"; havana_transcript \"OTTHUMT00000362751.1\";\nchr1    HAVANA  exon    12613   12721   .   +   .   gene_id \"ENSG00000223972.5\"; transcript_id \"ENST00000456328.2\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; transcript_type \"processed_transcript\"; transcript_status \"KNOWN\"; transcript_name \"DDX11L1-002\"; exon_number 2; exon_id \"ENSE00003582793.1\"; level 2; transcript_support_level \"1\"; tag \"basic\"; havana_gene \"OTTHUMG00000000961.2\"; havana_transcript \"OTTHUMT00000362751.1\";\nchr1    HAVANA  exon    13221   14409   .   +   .   gene_id \"ENSG00000223972.5\"; transcript_id \"ENST00000456328.2\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; transcript_type \"processed_transcript\"; transcript_status \"KNOWN\"; transcript_name \"DDX11L1-002\"; exon_number 3; exon_id \"ENSE00002312635.1\"; level 2; transcript_support_level \"1\"; tag \"basic\"; havana_gene \"OTTHUMG00000000961.2\"; havana_transcript \"OTTHUMT00000362751.1\";",
            "title": "Example gff"
        },
        {
            "location": "/qc/",
            "text": "Quality Control and Trimming\n\u00b6\n\n\nIn this practical you will learn to import, view and check the quality of raw high thoughput sequencing sequencing data.\n\n\nThe first dataset you will be working with is from an Illumina MiSeq dataset.\nThe sequenced organism is an enterohaemorrhagic E. coli (EHEC) of the serotype O157, a potentially fatal gastrointestinal pathogen.\nThe sequenced bacterium was part of an outbreak investigation in the St. Louis area, USA in 2011.\nThe sequencing was done as paired-end 2x150bp.\n\n\nDownloading the data\n\u00b6\n\n\nThe raw data were deposited at the European Nucleotide Archive, under the accession number SRR957824.\nYou could go to the ENA \nwebsite\n and search for the run with the accession SRR957824.\n\n\nYou can also download the data associated with the run directly with the following command:\n\n\nwget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR957/SRR957824/SRR957824_1.fastq.gz\nwget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR957/SRR957824/SRR957824_2.fastq.gz\n\n\n\n\nFastQC\n\u00b6\n\n\nTo check the quality of the sequence data we will use a tool called FastQC. With this you can check things like read length distribution, quality distribution across the read length, sequencing artifacts and much more.\n\n\nFastQC has a graphical interface and can be downloaded and run on a Windows or Linux computer without installation. It is available \nhere\n.\n\n\nHowever, FastQC is also available as a command line utility on the training server you are using. You can load the module and execute the program as follows:\n\n\nfastqc SRR957824_1.fastq.gz SRR957824_2.fastq.gz\n\n\n\n\nwhich will produce both a .zip archive containing all the plots, and a html document for you to look at the result in your browser.\n\n\nOpen the html file with your favourite web browser, and try to interpret them.\n\n\nPay special attention to the per base sequence quality and sequence length distribution. Explanations for the various quality modules can be found \nhere\n. Also, have a look at examples of a \ngood\n and a \nbad\n illumina read set for comparison.\n\n\nYou will note that the reads in your uploaded dataset have fairly poor quality (<20) towards the end. There are also outlier reads that have very poor quality for most of the second half of the reads.\n\n\nThere are overrepresented sequences in the data. Where do they come from?\n\n\nScythe\n\u00b6\n\n\nScythe uses a Naive Bayesian approach to classify contaminant substrings in sequence reads. It considers quality information, which can make it robust in picking out 3'-end adapters, which often include poor quality bases.\n\n\nFirst, install scythe:\n\n\ngit clone https://github.com/vsbuffalo/scythe.git\ncd scythe\nmake all\n\n\n\n\nThen, copy or move \"scythe\" to a directory in your $PATH, for example like this:\n\n\ncp scythe $HOME/bin/\n\n\nScythe can be run minimally with:\n\n\nscythe -a adapter_file.fasta -o trimmed_sequences.fastq sequences.fastq\n\n\nTry to trim the adapters in both your read files!\n\n\nSickle\n\u00b6\n\n\nMost modern sequencing technologies produce reads that have deteriorating quality towards the 3'-end and some towards the 5'-end as well. Incorrectly called bases in both regions negatively impact assembles, mapping, and downstream bioinformatics analyses.\n\n\nWe will trim each read individually down to the good quality part to keep the bad part from interfering with downstream applications.\n\n\nTo do so, we will use sickle. Sickle is a tool that uses sliding windows along with quality and length thresholds to determine when quality is sufficiently low to trim the 3'-end of reads and also determines when the quality is sufficiently high enough to trim the 5'-end of reads. It will also discard reads based upon a length threshold.\n\n\nFirst, install sickle:\n\n\ngit clone https://github.com/najoshi/sickle.git\ncd sickle\nmake\n\n\n\n\nCopy sickle to a directory in your $PATH:\n\n\ncp sickle $HOME/bin/\n\n\nSickle has two modes to work with both paired-end and single-end reads: sickle se and sickle pe.\n\n\nRunning sickle by itself will print the help:\n\n\nsickle\n\n\nRunning sickle with either the \"se\" or \"pe\" commands will give help specific to those commands. Since we have paired end reads:\n\n\nsickle pe\n\n\nSet the quality score to 25. This means the trimmer will work its way from both ends of each read, cutting away any bases with a quality score < 25.\n\n\nsickle pe -f input_file1.fastq -r input_file2.fastq -t sanger \\\n-o trimmed_output_file1.fastq -p trimmed_output_file2.fastq \\\n-s trimmed_singles_file.fastq -q 25\n\n\n\n\nWhat did the trimming do to the per-base sequence quality, the per sequence quality scores and the sequence length distribution? Run FastQC again to find out.\n\n\nWhat is the sequence duplication levels graph about? Why should you care about a high level of duplication, and why is the level of duplication very low for this data?\n\n\nBased on the FastQC report, there seems to be a population of shorter reads that are technical artifacts. We will ignore them for now as they will not interfere with our analysis.\n\n\nExtra exercises\n\u00b6\n\n\nPerform quality control on the extra datasets given by your instructors.",
            "title": "Quality Control and Trimming"
        },
        {
            "location": "/qc/#quality-control-and-trimming",
            "text": "In this practical you will learn to import, view and check the quality of raw high thoughput sequencing sequencing data.  The first dataset you will be working with is from an Illumina MiSeq dataset.\nThe sequenced organism is an enterohaemorrhagic E. coli (EHEC) of the serotype O157, a potentially fatal gastrointestinal pathogen.\nThe sequenced bacterium was part of an outbreak investigation in the St. Louis area, USA in 2011.\nThe sequencing was done as paired-end 2x150bp.",
            "title": "Quality Control and Trimming"
        },
        {
            "location": "/qc/#downloading-the-data",
            "text": "The raw data were deposited at the European Nucleotide Archive, under the accession number SRR957824.\nYou could go to the ENA  website  and search for the run with the accession SRR957824.  You can also download the data associated with the run directly with the following command:  wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR957/SRR957824/SRR957824_1.fastq.gz\nwget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR957/SRR957824/SRR957824_2.fastq.gz",
            "title": "Downloading the data"
        },
        {
            "location": "/qc/#fastqc",
            "text": "To check the quality of the sequence data we will use a tool called FastQC. With this you can check things like read length distribution, quality distribution across the read length, sequencing artifacts and much more.  FastQC has a graphical interface and can be downloaded and run on a Windows or Linux computer without installation. It is available  here .  However, FastQC is also available as a command line utility on the training server you are using. You can load the module and execute the program as follows:  fastqc SRR957824_1.fastq.gz SRR957824_2.fastq.gz  which will produce both a .zip archive containing all the plots, and a html document for you to look at the result in your browser.  Open the html file with your favourite web browser, and try to interpret them.  Pay special attention to the per base sequence quality and sequence length distribution. Explanations for the various quality modules can be found  here . Also, have a look at examples of a  good  and a  bad  illumina read set for comparison.  You will note that the reads in your uploaded dataset have fairly poor quality (<20) towards the end. There are also outlier reads that have very poor quality for most of the second half of the reads.  There are overrepresented sequences in the data. Where do they come from?",
            "title": "FastQC"
        },
        {
            "location": "/qc/#scythe",
            "text": "Scythe uses a Naive Bayesian approach to classify contaminant substrings in sequence reads. It considers quality information, which can make it robust in picking out 3'-end adapters, which often include poor quality bases.  First, install scythe:  git clone https://github.com/vsbuffalo/scythe.git\ncd scythe\nmake all  Then, copy or move \"scythe\" to a directory in your $PATH, for example like this:  cp scythe $HOME/bin/  Scythe can be run minimally with:  scythe -a adapter_file.fasta -o trimmed_sequences.fastq sequences.fastq  Try to trim the adapters in both your read files!",
            "title": "Scythe"
        },
        {
            "location": "/qc/#sickle",
            "text": "Most modern sequencing technologies produce reads that have deteriorating quality towards the 3'-end and some towards the 5'-end as well. Incorrectly called bases in both regions negatively impact assembles, mapping, and downstream bioinformatics analyses.  We will trim each read individually down to the good quality part to keep the bad part from interfering with downstream applications.  To do so, we will use sickle. Sickle is a tool that uses sliding windows along with quality and length thresholds to determine when quality is sufficiently low to trim the 3'-end of reads and also determines when the quality is sufficiently high enough to trim the 5'-end of reads. It will also discard reads based upon a length threshold.  First, install sickle:  git clone https://github.com/najoshi/sickle.git\ncd sickle\nmake  Copy sickle to a directory in your $PATH:  cp sickle $HOME/bin/  Sickle has two modes to work with both paired-end and single-end reads: sickle se and sickle pe.  Running sickle by itself will print the help:  sickle  Running sickle with either the \"se\" or \"pe\" commands will give help specific to those commands. Since we have paired end reads:  sickle pe  Set the quality score to 25. This means the trimmer will work its way from both ends of each read, cutting away any bases with a quality score < 25.  sickle pe -f input_file1.fastq -r input_file2.fastq -t sanger \\\n-o trimmed_output_file1.fastq -p trimmed_output_file2.fastq \\\n-s trimmed_singles_file.fastq -q 25  What did the trimming do to the per-base sequence quality, the per sequence quality scores and the sequence length distribution? Run FastQC again to find out.  What is the sequence duplication levels graph about? Why should you care about a high level of duplication, and why is the level of duplication very low for this data?  Based on the FastQC report, there seems to be a population of shorter reads that are technical artifacts. We will ignore them for now as they will not interfere with our analysis.",
            "title": "Sickle"
        },
        {
            "location": "/qc/#extra-exercises",
            "text": "Perform quality control on the extra datasets given by your instructors.",
            "title": "Extra exercises"
        },
        {
            "location": "/mapping/",
            "text": "Mapping and Variant Calling\n\u00b6\n\n\nIn this practical you will learn to map NGS reads to a reference sequence, check the output using a viewer software and investigate some aspects of the results. You will be using the read data from the \nQuality Control\n practical.\n\n\nEHEC O157 strains generally carry a large virulence plasmid, pO157. Plasmids are circular genetic elements that many bacteria carry in addition to their chromosomes. This particular plasmid encodes a number of proteins which are known or suspected to be involved in the ability to cause severe disease in infected humans. Your task in this practical is to map your prepared read set to a reference sequence of the virulence plasmid, to determine if the pO157 plasmid is present in the St. Louis outbreak strain.\n\n\nRead mapping\n\u00b6\n\n\ndownloading a reference\n\u00b6\n\n\nYou will need a reference sequence to map your reads to. You can download the reference from \nhere\n\n\nor from the command-line with\n\n\nwget https://raw.githubusercontent.com/HadrienG/tutorials/master/data/pO157_Sakai.fasta\n\n\nThis file contains the sequence of the pO157 plasmid from the Sakai outbreak strain of E. coli O157. In contrast to the strain we are working on, this strain is available as a finished genome, i.e. the whole sequence of both the single chromosome and the large virulence plasmid are known.\n\n\nindexing the reference\n\u00b6\n\n\nBefore aligning the reads against a reference, it is necessary to build an index of that reference:\n\n\nmodule load bowtie2\nbowtie2-build pO157_Sakai.fasta pO157_Sakai\n\n\n\n\naligning reads\n\u00b6\n\n\nbowtie2 -x pO157_Sakai -1 reads_1.fastq -2 reads_2.fastq -S output.sam\n\n\nThe output of the mapping will be in SAM format. you can find a brief explanation of the SAM format \nhere\n\n\nvisualising the alignment\n\u00b6\n\n\nTo view the outcome of the read mapping, we will use a program called Tablet, that can be run without administrated privileges. Download it \nhere\n.\n\n\nStart the program and select Red Button \u2013 Open. Choose your SAM-file and pO157_Sakai.fasta as a reference.\n\n\n\n\nSelect the only contig to the left.\n\n\n\n\nNext, download pO157_Sakai.gff from \nhere\n. Select import features in Tablet and import pO157_Sakai.gff. This file contains annotations, i.e. what is encoded in each part of the DNA sequence. Two tracks (CDS + GENE) will be added.\n\n\nNavigate the mapping using the zoom and pan left/right etc. Under Colour schemes you can highlight bases that do not match the reference. Holding your pointer over a CDS will show you a description of the genetic region.\n\n\n\n\nDoes the mapping data confirm the presence of an intact pO157 plasmid in the St. Louis outbreak strain?\n\n\nYou will note that one region has extremely high coverage. This region corresponds to two adjacent CDSs\n\n\nCheck their identity, what do they have in common?\n\n\nExtract the sequence of the biggest of these two by right-clicking on it and selecting copy reference subsequence to clipboard\n\n\nYour read data was from the whole genome, but you are only mapping to a reference for the pO157 plasmid. Go to BLAST and use nucleotide BLAST to check if the problematic region is present on the chromosome of the reference strain Sakai (BA000007.2) using the Align two sequences option with your subsequence copied from Tablet and BA000007.2. You don\u2019t have to enter the whole genome sequence, just the code.\n\n\n\n\nHow many matches do you find on the chromosome?\n\n\nWhy do you think the coverage is so high for this region in the mapping to the pO157 reference?\n\n\nVariant Calling\n\u00b6\n\n\nA frequent application for mapping reads is variant calling, i.e. finding positions where the reference and your sequences differ. Single nucleotide polymorphism (SNP)-based typing is particularly popular and used for a broad range of applications. For an EHEC O157 outbreak you could use it to identify the source, for instance. Indels are insertions and deletions in the mapped data compared to the reference.\n\n\nYou could find SNPs by just staring at the alignment in Tablet. A variant caller is a program that automates this process, finding variable positions anywhere in the sequence and weighing the evidence for a true variant (number of reads agreeing, quality of those reads etc.) We will be using SAMtools for variant calling.\n\n\nSorting and indexing a bam file\n\u00b6\n\n\nFirst you need to sort the reads in the BAM-file so the variant caller can easily find the relevant ones for a given position in the reference sequence.\n\n\nYou may need to convert your sam file into a bam file first:\n\n\nsamtools view -bS -o $output.bam $input.sam\n\n\nthen sort and index the bam file:\n\n\nsamtools sort $output.bam > $output_sorted.bam\nsamtools index $output_sorted.bam\n\n\n\n\nNow we can perform the SNP and Indel calling:\n\n\nsamtools mpileup -uv -o output.vcf -f pO157_Sakai.fasta test_sorted.bam\n\n\nYou can read about the structure of vcf files here\n\n\nHow many SNPs did the variant caller find? Did you find any indels?\n\n\nEach variant in the vcf comes with a lot of different quality metrics from the variant calling process. The \u201cQUAL\u201d column tells you how confident the variant caller is feeling about that variant.\n\n\nUse the coordinates from the vcf-file and manually check a high quality SNP and a low quality SNP in your Tablet view of the sam file. (Hint: there is a \u201cJump to base\u201d button that can make it easier for you.)\n\n\nIf you have time:\nRead the original paper about the St. Louis outbreak investigation, in particular check the supplemental material and methods to see how the authors performed read mapping and variant calling.",
            "title": "Mapping and Variant Calling"
        },
        {
            "location": "/mapping/#mapping-and-variant-calling",
            "text": "In this practical you will learn to map NGS reads to a reference sequence, check the output using a viewer software and investigate some aspects of the results. You will be using the read data from the  Quality Control  practical.  EHEC O157 strains generally carry a large virulence plasmid, pO157. Plasmids are circular genetic elements that many bacteria carry in addition to their chromosomes. This particular plasmid encodes a number of proteins which are known or suspected to be involved in the ability to cause severe disease in infected humans. Your task in this practical is to map your prepared read set to a reference sequence of the virulence plasmid, to determine if the pO157 plasmid is present in the St. Louis outbreak strain.",
            "title": "Mapping and Variant Calling"
        },
        {
            "location": "/mapping/#read-mapping",
            "text": "",
            "title": "Read mapping"
        },
        {
            "location": "/mapping/#downloading-a-reference",
            "text": "You will need a reference sequence to map your reads to. You can download the reference from  here  or from the command-line with  wget https://raw.githubusercontent.com/HadrienG/tutorials/master/data/pO157_Sakai.fasta  This file contains the sequence of the pO157 plasmid from the Sakai outbreak strain of E. coli O157. In contrast to the strain we are working on, this strain is available as a finished genome, i.e. the whole sequence of both the single chromosome and the large virulence plasmid are known.",
            "title": "downloading a reference"
        },
        {
            "location": "/mapping/#indexing-the-reference",
            "text": "Before aligning the reads against a reference, it is necessary to build an index of that reference:  module load bowtie2\nbowtie2-build pO157_Sakai.fasta pO157_Sakai",
            "title": "indexing the reference"
        },
        {
            "location": "/mapping/#aligning-reads",
            "text": "bowtie2 -x pO157_Sakai -1 reads_1.fastq -2 reads_2.fastq -S output.sam  The output of the mapping will be in SAM format. you can find a brief explanation of the SAM format  here",
            "title": "aligning reads"
        },
        {
            "location": "/mapping/#visualising-the-alignment",
            "text": "To view the outcome of the read mapping, we will use a program called Tablet, that can be run without administrated privileges. Download it  here .  Start the program and select Red Button \u2013 Open. Choose your SAM-file and pO157_Sakai.fasta as a reference.   Select the only contig to the left.   Next, download pO157_Sakai.gff from  here . Select import features in Tablet and import pO157_Sakai.gff. This file contains annotations, i.e. what is encoded in each part of the DNA sequence. Two tracks (CDS + GENE) will be added.  Navigate the mapping using the zoom and pan left/right etc. Under Colour schemes you can highlight bases that do not match the reference. Holding your pointer over a CDS will show you a description of the genetic region.   Does the mapping data confirm the presence of an intact pO157 plasmid in the St. Louis outbreak strain?  You will note that one region has extremely high coverage. This region corresponds to two adjacent CDSs  Check their identity, what do they have in common?  Extract the sequence of the biggest of these two by right-clicking on it and selecting copy reference subsequence to clipboard  Your read data was from the whole genome, but you are only mapping to a reference for the pO157 plasmid. Go to BLAST and use nucleotide BLAST to check if the problematic region is present on the chromosome of the reference strain Sakai (BA000007.2) using the Align two sequences option with your subsequence copied from Tablet and BA000007.2. You don\u2019t have to enter the whole genome sequence, just the code.   How many matches do you find on the chromosome?  Why do you think the coverage is so high for this region in the mapping to the pO157 reference?",
            "title": "visualising the alignment"
        },
        {
            "location": "/mapping/#variant-calling",
            "text": "A frequent application for mapping reads is variant calling, i.e. finding positions where the reference and your sequences differ. Single nucleotide polymorphism (SNP)-based typing is particularly popular and used for a broad range of applications. For an EHEC O157 outbreak you could use it to identify the source, for instance. Indels are insertions and deletions in the mapped data compared to the reference.  You could find SNPs by just staring at the alignment in Tablet. A variant caller is a program that automates this process, finding variable positions anywhere in the sequence and weighing the evidence for a true variant (number of reads agreeing, quality of those reads etc.) We will be using SAMtools for variant calling.",
            "title": "Variant Calling"
        },
        {
            "location": "/mapping/#sorting-and-indexing-a-bam-file",
            "text": "First you need to sort the reads in the BAM-file so the variant caller can easily find the relevant ones for a given position in the reference sequence.  You may need to convert your sam file into a bam file first:  samtools view -bS -o $output.bam $input.sam  then sort and index the bam file:  samtools sort $output.bam > $output_sorted.bam\nsamtools index $output_sorted.bam  Now we can perform the SNP and Indel calling:  samtools mpileup -uv -o output.vcf -f pO157_Sakai.fasta test_sorted.bam  You can read about the structure of vcf files here  How many SNPs did the variant caller find? Did you find any indels?  Each variant in the vcf comes with a lot of different quality metrics from the variant calling process. The \u201cQUAL\u201d column tells you how confident the variant caller is feeling about that variant.  Use the coordinates from the vcf-file and manually check a high quality SNP and a low quality SNP in your Tablet view of the sam file. (Hint: there is a \u201cJump to base\u201d button that can make it easier for you.)  If you have time:\nRead the original paper about the St. Louis outbreak investigation, in particular check the supplemental material and methods to see how the authors performed read mapping and variant calling.",
            "title": "Sorting and indexing a bam file"
        },
        {
            "location": "/assembly/",
            "text": "De-novo Genome Assembly\n\u00b6\n\n\nIn this practical we will perform the assembly of M. genitalium, a bacterium published in 1995 by Fraser et al in Science.\n\n\nGetting the data\n\u00b6\n\n\nGo to ENA, and search for the run ERR486840.\n\n\nDownload the 2 fastq files associated with the run.\n\n\nQuality control\n\u00b6\n\n\nPerform quality control of the data as you did in the \nQC tutorial\n\n\nHow many reads are in the fastq file? What is the read length?\nDoes the data need trimming or other filtering? If so, do it.\n\n\nFind the genome size of M. genitalium in the Fraser paper abstract.\nBased on the expected genome size, the read length and the number of reads \u2013 what average coverage do you expect to get from this fastq read files?\n\n\nDe-novo assembly\n\u00b6\n\n\nWe will be using the SPAdes assembler to assemble our bacterium.\n\n\nmodule load spades\nspades.py -k 21,33,55,77,99 --careful --only-assembler -1 read_1.fastq -2 read_2.fastq -o output\n\n\n\n\nThis will produce a series of outputs. The scaffolds will be in fasta format.\n\n\nHow well does the assembly total consensus size and coverage correspond to your earlier estimation?\nWhat is the N50 of the assembly? What does this mean?\nHow many contigs in total did the assembly produce?\nHow many contigs longer than 500bp? What is the N50 of those contigs only?\n\n\nPerform another assembly with the following options:\n\n\nUse the raw reads (no trimming, but with adapters removed), wthout the --only-assembler option.\n\n\nIf you have time, try out the following genome assemblers:\n\n\n\n\nMaSurCa\n\n\nRay\n\n\nVelvet\n\n\n\n\nComparing assemblies\n\u00b6\n\n\nQUAST is a software evaluating the quality of genome assemblies by computing various metrics, including\n\n\n\n\nN50, length for which the collection of all contigs of that length or longer covers at least 50% of assembly length\n\n\nNG50, where length of the reference genome is being covered\n\n\nNA50 and NGA50, where aligned blocks instead of contigs are taken\n\n\nmisassemblies, misassembled and unaligned contigs or contigs bases\n\n\ngenes and operons covered\n\n\n\n\nTo run Quast:\n\n\nmodule load quast\nquast.py assembly1.fasta assembly2.fasta ... -R reference.fasta -G reference.gff\n\n\n\n\nQuast will produce a pdf in the \nquast_results\n directory. Download it on your computer and take a look. Which assembly is better?\n\n\nFixing misassemblies\n\u00b6\n\n\nPilon is a software tool which can be used to automatically improve draft assemblies. It attempts to make improvements to the input genome, including:\n\n\n\n\nSingle base differences\n\n\nSmall Indels\n\n\nLarger Indels or block substitution events\n\n\nGap filling\n\n\nIdentification of local misassemblies, including optional opening of new gaps\n\n\n\n\nPilon then outputs a FASTA file containing an improved representation of the genome from the read data and an optional VCF file detailing variation seen between the read data and the input genome.\n\n\nYou can read how Pilon works in detail \nhere\n\n\nBefore running Pilon itself, you have to map your reads back to the assembly!\n\n\nbowtie2-build assembly.fasta index_prefix\n(bowtie2 -x index_prefix -1 read_1.fastq -2 read_2.fastq | samtools view -bS -o output_to_sort.bam - ) 2> bowtie.err\nsamtools sort output_to_sort.bam alignment.bam\nsamtools index alignment.bam\n\n\n\n\nRun Pilon with the following command:\n\n\nmodule load pilon\npilon --genome assembly.fasta --frags alignment.bam --output pilon_output\n\n\n\n\nOnce Pilon is finished running, compare the new assembly with the old one using Quast!",
            "title": "De-novo Genome Assembly"
        },
        {
            "location": "/assembly/#de-novo-genome-assembly",
            "text": "In this practical we will perform the assembly of M. genitalium, a bacterium published in 1995 by Fraser et al in Science.",
            "title": "De-novo Genome Assembly"
        },
        {
            "location": "/assembly/#getting-the-data",
            "text": "Go to ENA, and search for the run ERR486840.  Download the 2 fastq files associated with the run.",
            "title": "Getting the data"
        },
        {
            "location": "/assembly/#quality-control",
            "text": "Perform quality control of the data as you did in the  QC tutorial  How many reads are in the fastq file? What is the read length?\nDoes the data need trimming or other filtering? If so, do it.  Find the genome size of M. genitalium in the Fraser paper abstract.\nBased on the expected genome size, the read length and the number of reads \u2013 what average coverage do you expect to get from this fastq read files?",
            "title": "Quality control"
        },
        {
            "location": "/assembly/#de-novo-assembly",
            "text": "We will be using the SPAdes assembler to assemble our bacterium.  module load spades\nspades.py -k 21,33,55,77,99 --careful --only-assembler -1 read_1.fastq -2 read_2.fastq -o output  This will produce a series of outputs. The scaffolds will be in fasta format.  How well does the assembly total consensus size and coverage correspond to your earlier estimation?\nWhat is the N50 of the assembly? What does this mean?\nHow many contigs in total did the assembly produce?\nHow many contigs longer than 500bp? What is the N50 of those contigs only?  Perform another assembly with the following options:  Use the raw reads (no trimming, but with adapters removed), wthout the --only-assembler option.  If you have time, try out the following genome assemblers:   MaSurCa  Ray  Velvet",
            "title": "De-novo assembly"
        },
        {
            "location": "/assembly/#comparing-assemblies",
            "text": "QUAST is a software evaluating the quality of genome assemblies by computing various metrics, including   N50, length for which the collection of all contigs of that length or longer covers at least 50% of assembly length  NG50, where length of the reference genome is being covered  NA50 and NGA50, where aligned blocks instead of contigs are taken  misassemblies, misassembled and unaligned contigs or contigs bases  genes and operons covered   To run Quast:  module load quast\nquast.py assembly1.fasta assembly2.fasta ... -R reference.fasta -G reference.gff  Quast will produce a pdf in the  quast_results  directory. Download it on your computer and take a look. Which assembly is better?",
            "title": "Comparing assemblies"
        },
        {
            "location": "/assembly/#fixing-misassemblies",
            "text": "Pilon is a software tool which can be used to automatically improve draft assemblies. It attempts to make improvements to the input genome, including:   Single base differences  Small Indels  Larger Indels or block substitution events  Gap filling  Identification of local misassemblies, including optional opening of new gaps   Pilon then outputs a FASTA file containing an improved representation of the genome from the read data and an optional VCF file detailing variation seen between the read data and the input genome.  You can read how Pilon works in detail  here  Before running Pilon itself, you have to map your reads back to the assembly!  bowtie2-build assembly.fasta index_prefix\n(bowtie2 -x index_prefix -1 read_1.fastq -2 read_2.fastq | samtools view -bS -o output_to_sort.bam - ) 2> bowtie.err\nsamtools sort output_to_sort.bam alignment.bam\nsamtools index alignment.bam  Run Pilon with the following command:  module load pilon\npilon --genome assembly.fasta --frags alignment.bam --output pilon_output  Once Pilon is finished running, compare the new assembly with the old one using Quast!",
            "title": "Fixing misassemblies"
        },
        {
            "location": "/annotation/",
            "text": "Genome Annotation\n\u00b6\n\n\nAfter you have de novo assembled your genome sequencing reads into contigs, it is useful to know what genomic features are on those contigs. The process of identifying and labelling those features is called genome annotation.\n\n\nProkka is a \u201cwrapper\u201d; it collects together several pieces of software (from various authors), and so avoids \u201cre-inventing the wheel\u201d.\n\n\nProkka finds and annotates features (both protein coding regions and RNA genes, i.e. tRNA, rRNA) present on on a sequence. Prokka uses a two-step process for the annotation of protein coding regions: first, protein coding regions on the genome are identified using \nProdigal\n; second, the function of the encoded protein is predicted by similarity to proteins in one of many protein or protein domain databases. Prokka is a software tool that can be used to annotate bacterial, archaeal and viral genomes quickly, generating standard output files in GenBank, EMBL and gff formats. More information about Prokka can be found \nhere\n.\n\n\nInput data\n\u00b6\n\n\nProkka requires assembled contigs. You will need your best assembly from the assembly tutorial.\n\n\nAlternatively, you can download an assembly \nhere\n\n\nRunning prokka\n\u00b6\n\n\nmodule load prokka\nawk '/^>/{print \">ctg\" ++i; next}{print}' < assembly.fasta > good_contigs.fasta\nprokka --outdir annotation --kingdom Bacteria \\\n--proteins m_genitalium.faa good_contigs.fasta\n\n\n\n\nOnce Prokka has finished, examine each of its output files.\n\n\n\n\nThe GFF and GBK files contain all of the information about the features annotated (in different formats.)\n\n\nThe .txt file contains a summary of the number of features annotated.\n\n\nThe .faa file contains the protein sequences of the genes annotated.\n\n\nThe .ffn file contains the nucleotide sequences of the genes annotated.\n\n\n\n\nVisualising the annotation\n\u00b6\n\n\nArtemis is a graphical Java program to browse annotated genomes. Download it \nhere\n and install it on your local computer.\n\n\nCopy the .gff file produced by prokka on your computer, and open it with artemis.\n\n\nYou will be overwhelmed and/or confused at first, and possibly permanently. Here are some tips:\n\n\n\n\nThere are 3 panels: feature map (top), sequence (middle), feature list (bottom)\n\n\nClick right-mouse-button on bottom panel and select Show products\n\n\nZooming is done via the verrtical scroll bars in the two top panels",
            "title": "Genome Annotation"
        },
        {
            "location": "/annotation/#genome-annotation",
            "text": "After you have de novo assembled your genome sequencing reads into contigs, it is useful to know what genomic features are on those contigs. The process of identifying and labelling those features is called genome annotation.  Prokka is a \u201cwrapper\u201d; it collects together several pieces of software (from various authors), and so avoids \u201cre-inventing the wheel\u201d.  Prokka finds and annotates features (both protein coding regions and RNA genes, i.e. tRNA, rRNA) present on on a sequence. Prokka uses a two-step process for the annotation of protein coding regions: first, protein coding regions on the genome are identified using  Prodigal ; second, the function of the encoded protein is predicted by similarity to proteins in one of many protein or protein domain databases. Prokka is a software tool that can be used to annotate bacterial, archaeal and viral genomes quickly, generating standard output files in GenBank, EMBL and gff formats. More information about Prokka can be found  here .",
            "title": "Genome Annotation"
        },
        {
            "location": "/annotation/#input-data",
            "text": "Prokka requires assembled contigs. You will need your best assembly from the assembly tutorial.  Alternatively, you can download an assembly  here",
            "title": "Input data"
        },
        {
            "location": "/annotation/#running-prokka",
            "text": "module load prokka\nawk '/^>/{print \">ctg\" ++i; next}{print}' < assembly.fasta > good_contigs.fasta\nprokka --outdir annotation --kingdom Bacteria \\\n--proteins m_genitalium.faa good_contigs.fasta  Once Prokka has finished, examine each of its output files.   The GFF and GBK files contain all of the information about the features annotated (in different formats.)  The .txt file contains a summary of the number of features annotated.  The .faa file contains the protein sequences of the genes annotated.  The .ffn file contains the nucleotide sequences of the genes annotated.",
            "title": "Running prokka"
        },
        {
            "location": "/annotation/#visualising-the-annotation",
            "text": "Artemis is a graphical Java program to browse annotated genomes. Download it  here  and install it on your local computer.  Copy the .gff file produced by prokka on your computer, and open it with artemis.  You will be overwhelmed and/or confused at first, and possibly permanently. Here are some tips:   There are 3 panels: feature map (top), sequence (middle), feature list (bottom)  Click right-mouse-button on bottom panel and select Show products  Zooming is done via the verrtical scroll bars in the two top panels",
            "title": "Visualising the annotation"
        },
        {
            "location": "/pan_genome/",
            "text": "Pan-Genome Analysis\n\u00b6\n\n\nIn this tutorial we will learn how to determine a pan-genome from a collection of isolate genomes.\n\n\nThis tutorial is inspired from \nGenome annotation and Pangenome Analysis\n from the CBIB in Santiago, Chile\n\n\nGetting the data\n\u00b6\n\n\nWe'll use six \nListeria monocytogenes\n genomes in this tutorial.\n\n\nwget https://github.com/HadrienG/tutorials/blob/master/data/pangenome.tar.gz\ntar xzf pangenome.tar.gz\ncd pangenome\n\n\n\n\nThese genomes correspond to isolates of \nL. monocytogenes\n reported in\n\n\n\n\nXiangyu Deng, Adam M Phillippy, Zengxin Li, Steven L Salzberg and Wei Zhang. (2010) Probing the pan-genome of Listeria monocytogenes: new insights into intraspecific niche expansion and genomic diversification. doi:10.1186/1471-2164-11-500\n\n\n\n\nThe six genomes you downloaded were selected based on their level of completeness (finished; contigs, etc) and their genotype (type I-III):\n\n\n\n\n\n\n\n\nGenome Assembly\n\n\nGenome Accession\n\n\nGenotype\n\n\nSequenced by\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\nGCA_000026705\n\n\nFM242711\n\n\ntype I\n\n\nInstitut_Pasteur\n\n\nFinished\n\n\n\n\n\n\nGCA_000008285\n\n\nAE017262\n\n\ntype I\n\n\nTIGR\n\n\nFinished\n\n\n\n\n\n\nGCA_000168815\n\n\nAATL00000000\n\n\ntype I\n\n\nBroad Institute\n\n\n79 contigs\n\n\n\n\n\n\nGCA_000196035\n\n\nAL591824\n\n\ntype II\n\n\nEuropean Consortium\n\n\nFinished\n\n\n\n\n\n\nGCA_000168635\n\n\nAARW00000000\n\n\ntype II\n\n\nBroad Institute\n\n\n25 contigs\n\n\n\n\n\n\nGCA_000021185\n\n\nCP001175\n\n\ntype III\n\n\nMSU\n\n\nFinished\n\n\n\n\n\n\n\n\nAnnotation of the genomes\n\u00b6\n\n\nBy annotating the genomes we mean to add information regarding genes, their location, strandedness, and features and attributes. Now that you have the genomes, we need to annotate them to determine the location and attributes of the genes contained in them. We will use Prokka for the annotation.\n\n\nprokka --kingdom Bacteria --outdir prokka_GCA_000008285 --genus Listeria --locustag GCA_000008285 GCA_000008285.1_ASM828v1_genomic.fna\n\n\n\n\nAnnotate the 6 genomes, by replacing the \n-outdir\n and \n-locustag\n and \nfasta file\n accordingly.\n\n\nPan-genome analysis\n\u00b6\n\n\nput all the .gff files in the same folder (e.g., ./gff) and run Roary\n\n\nroary -f results -e -n -v gff/*.gff\n\n\nRoary will get all the coding sequences, convert them into protein, and create pre-clusters. Then, using BLASTP and MCL, Roary will create clusters, and check for paralogs. Finally, Roary will take every isolate and order them by presence/absence of orthologs. The summary output is present in the \nsummary_statistics.txt\n file.\n\n\nAdditionally, Roary produces a \ngene_presence_absence.csv\n file that can be opened in any spreadsheet software to manually explore the results. In this file, you will find information such as gene name and gene annotation, and, of course, whether a gene is present in a genome or not.\n\n\nPlotting the result\n\u00b6\n\n\nRoary comes with a python script that allows you to generate a few plots to graphically assess your analysis output.\n\n\nFirst, we need to generate a tree file from the alignment generated by Roary:\n\n\nFastTree \u2013nt \u2013gtr core_gene_alignment.aln > my_tree.newick\n\n\n\n\nThen we can plot the Roary results:\n\n\nroary_plots.py my_tree.newick gene_presence_absence.csv",
            "title": "Pan-Genome Analysis"
        },
        {
            "location": "/pan_genome/#pan-genome-analysis",
            "text": "In this tutorial we will learn how to determine a pan-genome from a collection of isolate genomes.  This tutorial is inspired from  Genome annotation and Pangenome Analysis  from the CBIB in Santiago, Chile",
            "title": "Pan-Genome Analysis"
        },
        {
            "location": "/pan_genome/#getting-the-data",
            "text": "We'll use six  Listeria monocytogenes  genomes in this tutorial.  wget https://github.com/HadrienG/tutorials/blob/master/data/pangenome.tar.gz\ntar xzf pangenome.tar.gz\ncd pangenome  These genomes correspond to isolates of  L. monocytogenes  reported in   Xiangyu Deng, Adam M Phillippy, Zengxin Li, Steven L Salzberg and Wei Zhang. (2010) Probing the pan-genome of Listeria monocytogenes: new insights into intraspecific niche expansion and genomic diversification. doi:10.1186/1471-2164-11-500   The six genomes you downloaded were selected based on their level of completeness (finished; contigs, etc) and their genotype (type I-III):     Genome Assembly  Genome Accession  Genotype  Sequenced by  Status      GCA_000026705  FM242711  type I  Institut_Pasteur  Finished    GCA_000008285  AE017262  type I  TIGR  Finished    GCA_000168815  AATL00000000  type I  Broad Institute  79 contigs    GCA_000196035  AL591824  type II  European Consortium  Finished    GCA_000168635  AARW00000000  type II  Broad Institute  25 contigs    GCA_000021185  CP001175  type III  MSU  Finished",
            "title": "Getting the data"
        },
        {
            "location": "/pan_genome/#annotation-of-the-genomes",
            "text": "By annotating the genomes we mean to add information regarding genes, their location, strandedness, and features and attributes. Now that you have the genomes, we need to annotate them to determine the location and attributes of the genes contained in them. We will use Prokka for the annotation.  prokka --kingdom Bacteria --outdir prokka_GCA_000008285 --genus Listeria --locustag GCA_000008285 GCA_000008285.1_ASM828v1_genomic.fna  Annotate the 6 genomes, by replacing the  -outdir  and  -locustag  and  fasta file  accordingly.",
            "title": "Annotation of the genomes"
        },
        {
            "location": "/pan_genome/#pan-genome-analysis_1",
            "text": "put all the .gff files in the same folder (e.g., ./gff) and run Roary  roary -f results -e -n -v gff/*.gff  Roary will get all the coding sequences, convert them into protein, and create pre-clusters. Then, using BLASTP and MCL, Roary will create clusters, and check for paralogs. Finally, Roary will take every isolate and order them by presence/absence of orthologs. The summary output is present in the  summary_statistics.txt  file.  Additionally, Roary produces a  gene_presence_absence.csv  file that can be opened in any spreadsheet software to manually explore the results. In this file, you will find information such as gene name and gene annotation, and, of course, whether a gene is present in a genome or not.",
            "title": "Pan-genome analysis"
        },
        {
            "location": "/pan_genome/#plotting-the-result",
            "text": "Roary comes with a python script that allows you to generate a few plots to graphically assess your analysis output.  First, we need to generate a tree file from the alignment generated by Roary:  FastTree \u2013nt \u2013gtr core_gene_alignment.aln > my_tree.newick  Then we can plot the Roary results:  roary_plots.py my_tree.newick gene_presence_absence.csv",
            "title": "Plotting the result"
        },
        {
            "location": "/16S/",
            "text": "Metabarcoding\n\u00b6\n\n\nThis tutorial is largely inspired of the \nMiSeq SOP\n from the Schloss Lab.\n\n\n\n\nKozich JJ, Westcott SL, Baxter NT, Highlander SK, Schloss PD. (2013): Development of a dual-index sequencing strategy and curation pipeline for analyzing amplicon sequence data on the MiSeq Illumina sequencing platform. Applied and Environmental Microbiology. 79(17):5112-20.\n\n\n\n\nTable of Contents\n\u00b6\n\n\n\n\nIntroduction\n\n\nSoftwares Required for this Tutorial\n\n\nDownloading the Data and Start Mothur\n\n\nReducing Sequencing and PCR Errors\n\n\nProcessing Improved Sequences\n\n\nAnalysis\n\n\nOTUs\n\n\n\n\n\n\nBatch Mode\n\n\n\n\nIntroduction\n\u00b6\n\n\nThe 16S rRNA gene is a section of prokaryotic DNA found in all bacteria and archaea. This gene codes for an rRNA, and this rRNA in turn makes up part of the ribosome. The first 'r' in rRNA stands for ribosomal. The ribosome is composed of two subunits, the large subunit (LSU) and the small subunit (SSU).\n\n\nThe 16S rRNA gene is a commonly used tool for identifying bacteria for several reasons.  First, traditional characterization depended upon phenotypic traits like gram positive or gram negative, bacillus or coccus, etc. Taxonomists today consider analysis of an organism's DNA more reliable than classification based solely on phenotypes. Secondly, researchers may, for a number of reasons, want to identify or classify only the bacteria within a given environmental or medical sample.  While there is a homologous gene in eukaryotes, the 18S rRNA gene, it is distinct, thereby rendering the 16S rRNA gene a useful tool for extracting and identifying bacteria as separate from plant, animal, fungal, and protist DNA within the same sample.  Thirdly, the 16S rRNA gene is relatively short at 1.5 kb, making it faster and cheaper to sequence than many other unique bacterial genes.\n\n\nMothur is a command-line computer program for analyzing sequence data from microbial communities and namely 16s data. mothur is licensed under the GPL and is free to use.\n\n\nSoftwares Required for this Tutorial\n\u00b6\n\n\n\n\nmothur\n\n\nmothur_krona\n\n\n\n\nDownloading the Data and Start Mothur\n\u00b6\n\n\nFirstly, download and unzip the sample dataset:\n\n\nwget http://www.mothur.org/w/images/d/d6/MiSeqSOPData.zip\nunzip MiSeqSOPData.zip\n\n\n\n\nIn the \nMiSeq_SOP\n directory, you'll find the reads files in fastq format, as well as a file called \nstability.files\n\n\nThe first lines of \nstability.files\n look like this:\n\n\n\n\nF3D0  F3D0_S188_L001_R1_001.fastq F3D0_S188_L001_R2_001.fastq\nF3D141  F3D141_S207_L001_R1_001.fastq   F3D141_S207_L001_R2_001.fastq\nF3D142  F3D142_S208_L001_R1_001.fastq   F3D142_S208_L001_R2_001.fastq\nF3D143  F3D143_S209_L001_R1_001.fastq   F3D143_S209_L001_R2_001.fastq\n\n\n\n\nThe first column is the name of the sample. The second column is the name of the forward read for that sample and the third columns in the name of the reverse read for that sample.\n\n\nNow it's time to start mothur. Type \nmothur\n in your terminal. You should see your prompt changing to\n\n\n\n\nmothur >\n\n\n\n\nReducing Sequencing and PCR Errors\n\u00b6\n\n\nThe first thing we want to do is combine our two sets of reads for each sample and then to combine the data from all of the samples. This is done using the \nmake.contigs\n command, which requires \nstability.files\n as input. This command will extract the sequence and quality score data from your fastq files, create the reverse complement of the reverse read and then join the reads into contigs.\n\n\nmake.contigs(file=stability.files, processors=8)  \n\nIt took 30 secs to process 152360 sequences.  \n\nGroup count:\nF3D0    7793\nF3D1    5869\nF3D141  5958\nF3D142  3183\nF3D143  3178\nF3D144  4827\nF3D145  7377\nF3D146  5021\nF3D147  17070\nF3D148  12405\nF3D149  13083\nF3D150  5509\nF3D2    19620\nF3D3    6758\nF3D5    4448\nF3D6    7989\nF3D7    5129\nF3D8    5294\nF3D9    7070\nMock    4779\n\nTotal of all groups is 152360\n\nOutput File Names:\nstability.trim.contigs.fasta\nstability.trim.contigs.qual\nstability.contigs.report\nstability.scrap.contigs.fasta\nstability.scrap.contigs.qual\nstability.contigs.groups\n\n\n\n\nThe \nstability.contigs.report\n file will tell you something about the contig assembly for each read. Let's see what these sequences look like using the \nsummary.seqs\n command:\n\n\nsummary.seqs(fasta=stability.trim.contigs.fasta)\n\nStart   End NBases  Ambigs  Polymer NumSeqs\nMinimum:    1   248 248 0   3   1\n2.5%-tile:  1   252 252 0   3   3810\n25%-tile:   1   252 252 0   4   38091\nMedian:     1   252 252 0   4   76181\n75%-tile:   1   253 253 0   5   114271\n97.5%-tile: 1   253 253 6   6   148552\nMaximum:    1   502 502 249 243 152360\nMean:   1   252.811 252.811 0.70063 4.44854\n# of Seqs:  152360\n\n\n\n\nThis tells us that we have 152360 sequences that for the most part vary between 248 and 253 bases. Interestingly, the longest read in the dataset is 502 bp. Be suspicious of this, the reads are supposed to be 251 bp each. This read clearly didn't assemble well (or at all). Also, note that at least 2.5% of our sequences had some ambiguous base calls. We'll take care of these issues in the next step when we run \nscreen.seqs\n.\n\n\nscreen.seqs(fasta=stability.trim.contigs.fasta, group=stability.contigs.groups, maxambig=0, maxlength=275)\n\n\nYou'll notice that mothur remembered that we used 8 processors in \nmake.contigs\n. To see what else mothur knows about you, run the following:\n\n\nget.current()\n\nCurrent files saved by mothur:\nfasta=stability.trim.contigs.good.fasta\ngroup=stability.contigs.good.groups\nqfile=stability.trim.contigs.qual\nprocessors=8\nsummary=stability.trim.contigs.summary\n\n\n\n\nWhat this means is that mothur remembers your latest fasta file and group file as well as the number of processors you have. So you could run:\n\n\nmothur > summary.seqs(fasta=stability.trim.contigs.good.fasta)\nmothur > summary.seqs(fasta=current)\nmothur > summary.seqs()\n\n\n\n\nand get the same output for each command.\n\n\nBut, now that we have filtered the sequencing errors, let's move to the next step.\n\n\nProcessing Improved Sequences\n\u00b6\n\n\nWe anticipate that many of our sequences are duplicates of each other. Because it's computationally wasteful to align the same sequences several times, we'll make our sequences unique:\n\n\nunique.seqs(fasta=stability.trim.contigs.good.fasta)\n\n\n\n\nIf two sequences have the same identical sequence, then they're considered duplicates and will get merged. In the screen output there are two columns - the first is the number of sequences characterized and the second is the number of unique sequences remaining\n\n\nAnother thing to do to make our lives easier is to simplify the names and group files. If you look at the most recent versions of those files you'll see together they are 13 MB. This may not seem like much, but with a full MiSeq run those long sequence names can add up and make life tedious. So we'll run count.seqs to generate a table where the rows are the names of the unique seqeunces and the columns are the names of the groups. The table is then filled with the number of times each unique sequence shows up in each group.\n\n\nThis will generate a file called stability.trim.contigs.good.count_table. In subsequent commands we'll use it by using the count option:\n\n\ncount.seqs(name=stability.trim.contigs.good.names, group=stability.contigs.good.groups)\nsummary.seqs(count=stability.trim.contigs.good.count_table)\n\nUsing stability.trim.contigs.good.unique.fasta as input file for the fasta parameter.\n\nUsing 8 processors.\n\n        Start   End NBases  Ambigs  Polymer NumSeqs\nMinimum:    1   250 250 0   3   1\n2.5%-tile:  1   252 252 0   3   3227\n25%-tile:   1   252 252 0   4   32265\nMedian:     1   252 252 0   4   64530\n75%-tile:   1   253 253 0   5   96794\n97.5%-tile: 1   253 253 0   6   125832\nMaximum:    1   270 270 0   12  129058\nMean:   1   252.462 252.462 0   4.36663\n# of unique seqs:   16477\ntotal # of seqs:    129058\n\n\n\n\nNow we need to align our sequences to the reference alignment.\n\n\nFirst we need to download the SILVA database.\n\n\n# This step should be done outside mothur\nwget http://www.mothur.org/w/images/9/98/Silva.bacteria.zip\nunzip Silva.bacteria.zip\n\n\n\n\nIf you have quit mothur to download the database, rerun the \nmothur\n command, then take a look at the database you have downloaded:\n\n\nsummary.seqs(fasta=silva.bacteria/silva.bacteria.fasta, processors=8)\n\n\n\n\nNow do the alignment using \nalign.seqs\n:\n\n\nalign.seqs(fasta=stability.trim.contigs.good.unique.fasta, reference=silva.bacteria/silva.bacteria.fasta)\n\n\n\n\nWe can then run \nsummary.seqs\n again to get a summary of our alignment:\n\n\nsummary.seqs(fasta=stability.trim.contigs.good.unique.align, count=stability.trim.contigs.good.count_table)\n\n\n\n\nYou'll see that the bulk of the sequences start at position 13862 and end at position 23444. Some sequences start at position 13144 or 13876 and end at 22587 or 25294. These deviants from the mode positions are likely due to an insertion or deletion at the terminal ends of the aliignments. Sometimes you'll see sequences that start and end at the same position indicating a very poor alignment, which is generally due to non-specific amplification. To make sure that everything overlaps the same region we'll re-run screen.seqs to get sequences that start at or before position 1968 and end at or after position 11550. We'll also set the maximum homopolymer length to 8 since there's nothing in the database with a stretch of 9 or more of the same base in a row (this really could have been done in the first execution of screen.seqs above). Note that we need the count table so that we can update the table for the sequences we're removing and we're also using the summary file so we don't have to figure out again all the start and stop positions:\n\n\nscreen.seqs(fasta=stability.trim.contigs.good.unique.align, count=stability.trim.contigs.good.count_table, summary=stability.trim.contigs.good.unique.summary, start=13862, end=23444, maxhomop=8)\nsummary.seqs(fasta=current, count=current)\n\n\n\n\nNo we can trim both ends of the aligned reads to be sure the all overlap exactly the same region. We can do this with \nfliter.seqs\n\n\nfilter.seqs(fasta=stability.trim.contigs.good.unique.good.align, vertical=T, trump=.)\n\n\n\n\nWe may have introduced redundancy by trimming the ends of the sequences, so we will re-run \nunique.seqs\n\n\nunique.seqs(fasta=stability.trim.contigs.good.unique.good.filter.fasta, count=stability.trim.contigs.good.good.count_table)\n\n\n\n\nThis identified 3 duplicate sequences that we've now merged with previous unique sequences. The next thing we want to do to further de-noise our sequences is to pre-cluster the sequences using the \npre.cluster\n command allowing for up to 2 differences between sequences. This command will split the sequences by group and then sort them by abundance and go from most abundant to least and identify sequences that are within 2 nt of each other. If they are then they get merged. We generally favor allowing 1 difference for every 100 bp of sequence:\n\n\npre.cluster(fasta=stability.trim.contigs.good.unique.good.filter.unique.fasta, count=stability.trim.contigs.good.unique.good.filter.count_table, diffs=2)\n\n\n\n\nAt this point we have removed as much sequencing error as we can and it is time to turn our attention to removing chimeras. We'll do this using the UCHIME algorithm that is called within mothur using the \nchimera.uchime\n command. Again, this command will split the data by sample and check for chimeras. Our preferred way of doing this is to use the abundant sequences as our reference. In addition, if a sequence is flagged as chimeric in one sample, the the default (dereplicate=F) is to remove it from all samples. Our experience suggests that this is a bit aggressive since we've seen rare sequences get flagged as chimeric when they're the most abundant sequence in another sample. This is how we do it:\n\n\nchimera.uchime(fasta=stability.trim.contigs.good.unique.good.filter.unique.precluster.fasta, count=stability.trim.contigs.good.unique.good.filter.unique.precluster.count_table, dereplicate=t)\n\n\n\n\nRunning \nchimera.uchime\n with the count file will remove the chimeric sequences from the count file. But you still need to remove those sequences from the fasta file. We do this using \nremove.seqs\n:\n\n\nremove.seqs(fasta=stability.trim.contigs.good.unique.good.filter.unique.precluster.fasta, accnos=stability.trim.contigs.good.unique.good.filter.unique.precluster.denovo.uchime.accnos)\n\n\n\n\nAs a final quality control step, we need to see if there are any \"undesirables\" in our dataset. Sometimes when we pick a primer set they will amplify other stuff that gets to this point in the pipeline such as 18S rRNA gene fragments or 16S rRNA from Archaea, chloroplasts, and mitochondira. There's also just the random stuff that we want to get rid of.  Let's go ahead and classify those sequences using the Bayesian classifier with the \nclassify.seqs\n command:\n\n\nclassify.seqs(fasta=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.fasta, count=stability.trim.contigs.good.unique.good.filter.unique.precluster.denovo.uchime.pick.count_table, reference=silva.bacteria/silva.bacteria.fasta, taxonomy=silva.bacteria/silva.bacteria.rdp.tax, cutoff=80)\n\n\n\n\nNow that everything is classified we want to remove our undesirables. We do this with the \nremove.lineage\n command:\n\n\nremove.lineage(fasta=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.fasta, count=stability.trim.contigs.good.unique.good.filter.unique.precluster.denovo.uchime.pick.count_table, taxonomy=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.rdp.wang.taxonomy, taxon=Chloroplast-Mitochondria-unknown-Archaea-Eukaryota)\n\n\n\n\nAnalysis\n\u00b6\n\n\nOTUs\n\u00b6\n\n\nWe will use \ncluster.split\n for clustering sequences into OTUs\n\n\ncluster.split(fasta=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.fasta, count=stability.trim.contigs.good.unique.good.filter.unique.precluster.denovo.uchime.pick.pick.count_table, taxonomy=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.rdp.wang.pick.taxonomy, splitmethod=classify, taxlevel=4, cutoff=0.15)\n\n\n\n\nWe used \ntaxlevel=4\n, which corresponds to the level of \nOrder\n\n\nNext we want to know how many sequences are in each OTU from each group and we can do this using the \nmake.shared command\n. Here we tell mothur that we're really only interested in the 0.03 cutoff level:\n\n\nmake.shared(list=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.an.unique_list.list, count=stability.trim.contigs.good.unique.good.filter.unique.precluster.denovo.uchime.pick.pick.count_table, label=0.03)\n\n\n\n\nWe also want to know the taxonomy for each of our OTUs. We can get the consensus taxonomy for each OTU using the \nclassify.otu\n command\n\n\nclassify.otu(list=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.an.unique_list.list, count=stability.trim.contigs.good.unique.good.filter.unique.precluster.denovo.uchime.pick.pick.count_table, taxonomy=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.rdp.wang.pick.taxonomy, label=0.03)\n\n\n\n\nIf you open the file \nstability.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.an.unique_list.0.03.cons.taxonomy\n, you can get information about your OTUs.\n\n\nOTU Size    Taxonomy\nOtu0001 12328   Bacteria(100);Bacteroidetes(100);Bacteroidia(100);Bacteroidales(100);Porphyromonadaceae(100);Barnesiella(100);Barnesiella_unclassified(100);\nOtu0002 8918    Bacteria(100);Bacteroidetes(100);Bacteroidia(100);Bacteroidales(100);Porphyromonadaceae(100);Barnesiella(100);Barnesiella_unclassified(100);\nOtu0003 7850    Bacteria(100);Bacteroidetes(100);Bacteroidia(100);Bacteroidales(100);Porphyromonadaceae(100);Barnesiella(100);Barnesiella_unclassified(100);\nOtu0004 7478    Bacteria(100);Bacteroidetes(100);Bacteroidia(100);Bacteroidales(100);Porphyromonadaceae(100);Barnesiella(100);Barnesiella_unclassified(100);\nOtu0005 7478    Bacteria(100);Bacteroidetes(100);Bacteroidia(100);Bacteroidales(100);Porphyromonadaceae(100);Barnesiella(100);Barnesiella_unclassified(100);\nOtu0006 6650    Bacteria(100);Bacteroidetes(100);Bacteroidia(100);Bacteroidales(100);Porphyromonadaceae(100);Barnesiella(100);Barnesiella_unclassified(100);\nOtu0007 6341    Bacteria(100);Bacteroidetes(100);Bacteroidia(100);Bacteroidales(100);Bacteroidaceae(100);Bacteroides(100);Bacteroides_unclassified(100);\nOtu0008 5374    Bacteria(100);Bacteroidetes(100);Bacteroidia(100);Bacteroidales(100);Rikenellaceae(100);Alistipes(100);Alistipes_unclassified(100);\nOtu0009 3618    Bacteria(100);Bacteroidetes(100);Bacteroidia(100);Bacteroidales(100);Porphyromonadaceae(100);Barnesiella(100);Barnesiella_unclassified(100);\n\n\n\n\nThis is telling you that Otu0001 was observed 12328 times in your sample and that 100% of the sequences were from \nBarnesiella\n\n\n\n\n\nIn order to vizualise the composition of our datasets, we'll use phyloseq, a R package to work with microbiom data.\n\n\nMost of the phyloseq functionalities require aand a tree file. We need to generate it with mothur:\n\n\ndist.seqs(fasta=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.fasta, output=lt, processors=8)\nclearcut(phylip=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.phylip.dist)\n\n\n\n\nBatch Mode\n\u00b6\n\n\nIt is perfectly acceptable to enter the commands for your analysis from within mothur. We call this the interactive mode. If you are doing a lot these types of analysis or you want to use this SOP on your own data without thinking too much, you can run mothur in \nbatch mode\n using\n\n\n./mothur script.batch\n\n\nwhere script.batch (or whatever name you want, really) is a text file containing all the\ncommands that you previously entered in interactive mode.\n\n\nIf you have time, copy all the commands from this tutorial in a file, a try to make mothur work in batch mode!\n\n\nPhyloSeq Analysis\n\u00b6\n\n\nFirst, install and load the phyloseq package:\n\n\nsource('http://bioconductor.org/biocLite.R')\nbiocLite('phyloseq')\n\nlibrary(\"phyloseq\")\nlibrary(\"ggplot2\")\nlibrary(\"plyr\")\ntheme_set(theme_bw())  # set the ggplot theme\n\n\n\n\nThe PhyloSeq package has an \nimport_mothur\n function that you can use to import the files you generated with mothur. As an example, import the example mothur data provided by phyloseq as an example:\n\n\nmothlist <- system.file(\"extdata\", \"esophagus.fn.list.gz\", package=\"phyloseq\")\nmothgroup <- system.file(\"extdata\", \"esophagus.good.groups.gz\", package=\"phyloseq\")\nmothtree <- system.file(\"extdata\", \"esophagus.tree.gz\", package=\"phyloseq\")\n\nshow_mothur_cutoffs(mothlist)\ncutoff <- '0.10'\nx <- import_mothur(mothlist, mothgroup, mothtree, cutoff)\nx\n\n\n\n\nNote: If if you ever work with 16s data and decide to use QIIME instead of mothur, phyloseq also has an \nimport_qiime\n function. Also, newer version of qiime and mothur have the ability to produce a \n.biom\n file.\n\n\n\n\n\u201cThe biom file format (canonically pronounced \u2018biome\u2019) is designed to be a general-use format for representing counts of observations in one or more biological samples. BIOM is a recognized standard for the Earth Microbiome Project and is a Genomics Standards Consortium candidate project.\u201d\n\n\n\n\nMore info on \nhttp://biom-format.org/\n\n\nFor the rest of this tutorial, we will work with an example dataset provided by the phyloseq package. Load the data with the following command:\n\n\ndata(enterotype)\ndata(\"GlobalPatterns\")\n\n\n\n\nOrdination and distance-based analysis\n\u00b6\n\n\nLet's do some preliminary filtering. Remove the OTUs that included all unassigned sequences (\"-1\")\n\n\nenterotype <- subset_species(enterotype, Genus != \"-1\")\n\n\n\n\nThe available distance methods coded in the phyloseq package:\n\n\ndist_methods <- unlist(distanceMethodList)\nprint(dist_methods)\n\n##     UniFrac1     UniFrac2        DPCoA          JSD     vegdist1\n##    \"unifrac\"   \"wunifrac\"      \"dpcoa\"        \"jsd\"  \"manhattan\"\n##     vegdist2     vegdist3     vegdist4     vegdist5     vegdist6\n##  \"euclidean\"   \"canberra\"       \"bray\" \"kulczynski\"    \"jaccard\"\n##     vegdist7     vegdist8     vegdist9    vegdist10    vegdist11\n##      \"gower\"   \"altGower\"   \"morisita\"       \"horn\"  \"mountford\"\n##    vegdist12    vegdist13    vegdist14    vegdist15   betadiver1\n##       \"raup\"   \"binomial\"       \"chao\"        \"cao\"          \"w\"\n##   betadiver2   betadiver3   betadiver4   betadiver5   betadiver6\n##         \"-1\"          \"c\"         \"wb\"          \"r\"          \"I\"\n##   betadiver7   betadiver8   betadiver9  betadiver10  betadiver11\n##          \"e\"          \"t\"         \"me\"          \"j\"        \"sor\"\n##  betadiver12  betadiver13  betadiver14  betadiver15  betadiver16\n##          \"m\"         \"-2\"         \"co\"         \"cc\"          \"g\"\n##  betadiver17  betadiver18  betadiver19  betadiver20  betadiver21\n##         \"-3\"          \"l\"         \"19\"         \"hk\"        \"rlb\"\n##  betadiver22  betadiver23  betadiver24        dist1        dist2\n##        \"sim\"         \"gl\"          \"z\"    \"maximum\"     \"binary\"\n##        dist3   designdist\n##  \"minkowski\"        \"ANY\"\n\n\n\n\nRemove the two distance-methods that require a tree, and the generic custom method that requires user-defined distance arguments.\n\n\n# These require tree\ndist_methods[(1:3)]\n\n# Remove them from the vector\ndist_methods <- dist_methods[-(1:3)]\n# This is the user-defined method:\ndist_methods[\"designdist\"]\n\n# Remove the user-defined distance\ndist_methods = dist_methods[-which(dist_methods==\"ANY\")]\n\n\n\n\nLoop through each distance method, save each plot to a list, called plist.\n\n\nplist <- vector(\"list\", length(dist_methods))\nnames(plist) = dist_methods\nfor( i in dist_methods ){\n    # Calculate distance matrix\n    iDist <- distance(enterotype, method=i)\n    # Calculate ordination\n    iMDS  <- ordinate(enterotype, \"MDS\", distance=iDist)\n    ## Make plot\n    # Don't carry over previous plot (if error, p will be blank)\n    p <- NULL\n    # Create plot, store as temp variable, p\n    p <- plot_ordination(enterotype, iMDS, color=\"SeqTech\", shape=\"Enterotype\")\n    # Add title to each plot\n    p <- p + ggtitle(paste(\"MDS using distance method \", i, sep=\"\"))\n    # Save the graphic to file.\n    plist[[i]] = p\n}\n\n\n\n\nCombine results and shade according to Sequencing technology:\n\n\ndf = ldply(plist, function(x) x$data)\nnames(df)[1] <- \"distance\"\np = ggplot(df, aes(Axis.1, Axis.2, color=SeqTech, shape=Enterotype))\np = p + geom_point(size=3, alpha=0.5)\np = p + facet_wrap(~distance, scales=\"free\")\np = p + ggtitle(\"MDS on various distance metrics for Enterotype dataset\")\np\n\n\n\n\nPrint individual plots:\n\n\nprint(plist[[\"jsd\"]])\nprint(plist[[\"jaccard\"]])\nprint(plist[[\"bray\"]])\nprint(plist[[\"euclidean\"]])\n\n\n\n\nAlpha diversity graphics\n\u00b6\n\n\nHere is the default graphic produced by the plot_richness function on the GP example dataset:\n\n\nGP <- prune_species(speciesSums(GlobalPatterns) > 0, GlobalPatterns)\nplot_richness(GP)\n\n\n\n\nNote that in this case, the Fisher calculation results in a warning (but still plots). We can avoid this by specifying a measures argument to plot_richness, which will include just the alpha-diversity measures that we want.\n\n\nplot_richness(GP, measures=c(\"Chao1\", \"Shannon\"))\n\n\n\n\nWe can specify a sample variable on which to group/organize samples along the horizontal (x) axis. An experimentally meaningful categorical variable is usually a good choice \u2013 in this case, the \"SampleType\" variable works much better than attempting to interpret the sample names directly (as in the previous plot):\n\n\nplot_richness(GP, x=\"SampleType\", measures=c(\"Chao1\", \"Shannon\"))\n\n\n\n\nNow suppose we wanted to use an external variable in the plot that isn\u2019t in the GP dataset already \u2013 for example, a logical that indicated whether or not the samples are human-associated. First, define this new variable, human, as a factor (other vectors could also work; or other data you might have describing the samples).\n\n\nsampleData(GP)$human <- getVariable(GP, \"SampleType\") %in% c(\"Feces\", \"Mock\", \"Skin\", \"Tongue\")\n\n\n\n\nNow tell plot_richness to map the new human variable on the horizontal axis, and shade the points in different color groups, according to which \"SampleType\" they belong.\n\n\nplot_richness(GP, x=\"human\", color=\"SampleType\", measures=c(\"Chao1\", \"Shannon\"))\n\n\n\n\nWe can merge samples that are from the environment (SampleType), and make the points bigger with a ggplot2 layer. First, merge the samples.\n\n\nGPst = merge_samples(GP, \"SampleType\")\n# repair variables that were damaged during merge (coerced to numeric)\nsample_data(GPst)$SampleType <- factor(sample_names(GPst))\nsample_data(GPst)$human <- as.logical(sample_data(GPst)$human)\n\np = plot_richness(GPst, x=\"human\", color=\"SampleType\", measures=c(\"Chao1\", \"Shannon\"))\np + geom_point(size=5, alpha=0.7)\n\n\n\n\nTrees\n\u00b6\n\n\nhead(phy_tree(GlobalPatterns)$node.label, 10)\n\n\n\n\nThe node data from the \nGlobalPatterns\n dataset are strange. They look like they might be bootstrap values, but they sometimes have two decimals.\n\n\nphy_tree(GlobalPatterns)$node.label = substr(phy_tree(GlobalPatterns)$node.label, 1, 4)\n\n\n\n\nAdditionally, the dataset has many OTUs, too many to fit them all on a tree. Let's take the 50 more abundant and plot a basic tree:\n\n\nphyseq = prune_taxa(taxa_names(GlobalPatterns)[1:50], GlobalPatterns)\nplot_tree(physeq)\n\n\n\n\ndots are annotated next to tips (OTUs) in the tree, one for each sample in which that OTU was observed. Let's color the dots by taxonomic ranks, and sample covariates:\n\n\nplot_tree(physeq, nodelabf=nodeplotboot(), ladderize=\"left\", color=\"SampleType\")\n\n\n\n\nby taxonomic class:\n\n\nplot_tree(physeq, nodelabf=nodeplotboot(), ladderize=\"left\", color=\"Class\")\n\n\n\n\nIt can be useful to label the tips:\n\n\nplot_tree(physeq, color=\"SampleType\", label.tips=\"Genus\")\n\n\n\n\nMaking a radial tree is easy with ggplot2, simply recognizing that our vertically-oriented tree is a cartesian mapping of the data to a graphic \u2013 and that a radial tree is the same mapping, but with polar coordinates instead.\n\n\nplot_tree(physeq, nodelabf=nodeplotboot(60,60,3), color=\"SampleType\", shape=\"Class\", ladderize=\"left\") + coord_polar(theta=\"y\")\n\n\n\n\nBar plots\n\u00b6\n\n\nBar plots are one of the easiest way to vizualize your data. But be careful, they can be misleading if grouping sample!\n\n\nLet's take a subset of the GlobalPatterns dataset, and produce a basic bar plot:\n\n\ngp.ch = subset_taxa(GlobalPatterns, Phylum == \"Chlamydiae\")\nplot_bar(gp.ch)\n\n\n\n\nThe dataset is plotted with every sample mapped individually to the horizontal (x) axis, and abundance values mapped to the veritcal (y) axis. At each sample\u2019s horizontal position, the abundance values for each OTU are stacked in order from greatest to least, separate by a thin horizontal line. As long as the parameters you choose to separate the data result in more than one OTU abundance value at the respective position in the plot, the values will be stacked in order as a means of displaying both the sum total value while still representing the individual OTU abundances.\n\n\nThe bar plot will be clearer with color to represent the Genus to which each OTU belongs.\n\n\nplot_bar(gp.ch, fill=\"Genus\")\n\n\n\n\nNow keep the same fill color, and group the samples together by the SampleType variable; essentially, the environment from which the sample was taken and sequenced.\n\n\nplot_bar(gp.ch, x=\"SampleType\", fill=\"Genus\")\n\n\n\n\nA more complex example using facets:\n\n\nplot_bar(gp.ch, \"Family\", fill=\"Genus\", facet_grid=~SampleType)\n\n\n\n\nHeatmaps\n\u00b6\n\n\nThe following two lines subset the dataset to just the top 300 most abundant Bacteria taxa across all samples (in this case, with no prior preprocessing. Not recommended, but quick).\n\n\ndata(\"GlobalPatterns\")\ngpt <- subset_taxa(GlobalPatterns, Kingdom==\"Bacteria\")\ngpt <- prune_taxa(names(sort(taxa_sums(gpt),TRUE)[1:300]), gpt)\nplot_heatmap(gpt, sample.label=\"SampleType\")\n\n\n\n\nsubset a smaller dataset based on an Archaeal phylum\n\n\ngpac <- subset_taxa(GlobalPatterns, Phylum==\"Crenarchaeota\")\nplot_heatmap(gpac)\n\n\n\n\nPlot microbiome network\n\u00b6\n\n\nThere is a random aspect to some of the network layout methods. For complete reproducibility of the images produced later in this tutorial, it is possible to set the random number generator seed explicitly:\n\n\nset.seed(711L)\n\n\nBecause we want to use the enterotype designations as a plot feature in these plots, we need to remove the 9 samples for which no enterotype designation was assigned (this will save us the hassle of some pesky warning messages, but everything still works; the offending samples are anyway omitted).\n\n\nenterotype = subset_samples(enterotype, !is.na(Enterotype))\n\n\n\n\nCreate an igraph-based network based on the default distance method, \u201cJaccard\u201d, and a maximum distance between connected nodes of 0.3.\n\n\nig <- make_network(enterotype, max.dist=0.3)\nplot_network(ig, enterotype)\n\n\n\n\nThe previous graphic displayed some interesting structure, with one or two major subgraphs comprising a majority of samples. Furthermore, there seemed to be a correlation in the sample naming scheme and position within the network. Instead of trying to read all of the sample names to understand the pattern, let\u2019s map some of the sample variables onto this graphic as color and shape:\n\n\nplot_network(ig, enterotype, color=\"SeqTech\", shape=\"Enterotype\", line_weight=0.4, label=NULL)\n\n\n\n\nIn the previous examples, the choice of maximum-distance and distance method were informed, but arbitrary. Let\u2019s see what happens when the maximum distance is lowered, decreasing the number of edges in the network\n\n\nig <- make_network(enterotype, max.dist=0.2)\nplot_network(ig, enterotype, color=\"SeqTech\", shape=\"Enterotype\", line_weight=0.4, label=NULL)\n\n\n\n\nLet\u2019s repeat the previous exercise, but replace the Jaccard (default) distance method with Bray-Curtis\n\n\nig <- make_network(enterotype, dist.fun=\"bray\", max.dist=0.3)\nplot_network(ig, enterotype, color=\"SeqTech\", shape=\"Enterotype\", line_weight=0.4, label=NULL)",
            "title": "Metabarcoding"
        },
        {
            "location": "/16S/#metabarcoding",
            "text": "This tutorial is largely inspired of the  MiSeq SOP  from the Schloss Lab.   Kozich JJ, Westcott SL, Baxter NT, Highlander SK, Schloss PD. (2013): Development of a dual-index sequencing strategy and curation pipeline for analyzing amplicon sequence data on the MiSeq Illumina sequencing platform. Applied and Environmental Microbiology. 79(17):5112-20.",
            "title": "Metabarcoding"
        },
        {
            "location": "/16S/#table-of-contents",
            "text": "Introduction  Softwares Required for this Tutorial  Downloading the Data and Start Mothur  Reducing Sequencing and PCR Errors  Processing Improved Sequences  Analysis  OTUs    Batch Mode",
            "title": "Table of Contents"
        },
        {
            "location": "/16S/#introduction",
            "text": "The 16S rRNA gene is a section of prokaryotic DNA found in all bacteria and archaea. This gene codes for an rRNA, and this rRNA in turn makes up part of the ribosome. The first 'r' in rRNA stands for ribosomal. The ribosome is composed of two subunits, the large subunit (LSU) and the small subunit (SSU).  The 16S rRNA gene is a commonly used tool for identifying bacteria for several reasons.  First, traditional characterization depended upon phenotypic traits like gram positive or gram negative, bacillus or coccus, etc. Taxonomists today consider analysis of an organism's DNA more reliable than classification based solely on phenotypes. Secondly, researchers may, for a number of reasons, want to identify or classify only the bacteria within a given environmental or medical sample.  While there is a homologous gene in eukaryotes, the 18S rRNA gene, it is distinct, thereby rendering the 16S rRNA gene a useful tool for extracting and identifying bacteria as separate from plant, animal, fungal, and protist DNA within the same sample.  Thirdly, the 16S rRNA gene is relatively short at 1.5 kb, making it faster and cheaper to sequence than many other unique bacterial genes.  Mothur is a command-line computer program for analyzing sequence data from microbial communities and namely 16s data. mothur is licensed under the GPL and is free to use.",
            "title": "Introduction"
        },
        {
            "location": "/16S/#softwares-required-for-this-tutorial",
            "text": "mothur  mothur_krona",
            "title": "Softwares Required for this Tutorial"
        },
        {
            "location": "/16S/#downloading-the-data-and-start-mothur",
            "text": "Firstly, download and unzip the sample dataset:  wget http://www.mothur.org/w/images/d/d6/MiSeqSOPData.zip\nunzip MiSeqSOPData.zip  In the  MiSeq_SOP  directory, you'll find the reads files in fastq format, as well as a file called  stability.files  The first lines of  stability.files  look like this:   F3D0  F3D0_S188_L001_R1_001.fastq F3D0_S188_L001_R2_001.fastq\nF3D141  F3D141_S207_L001_R1_001.fastq   F3D141_S207_L001_R2_001.fastq\nF3D142  F3D142_S208_L001_R1_001.fastq   F3D142_S208_L001_R2_001.fastq\nF3D143  F3D143_S209_L001_R1_001.fastq   F3D143_S209_L001_R2_001.fastq   The first column is the name of the sample. The second column is the name of the forward read for that sample and the third columns in the name of the reverse read for that sample.  Now it's time to start mothur. Type  mothur  in your terminal. You should see your prompt changing to   mothur >",
            "title": "Downloading the Data and Start Mothur"
        },
        {
            "location": "/16S/#reducing-sequencing-and-pcr-errors",
            "text": "The first thing we want to do is combine our two sets of reads for each sample and then to combine the data from all of the samples. This is done using the  make.contigs  command, which requires  stability.files  as input. This command will extract the sequence and quality score data from your fastq files, create the reverse complement of the reverse read and then join the reads into contigs.  make.contigs(file=stability.files, processors=8)  \n\nIt took 30 secs to process 152360 sequences.  \n\nGroup count:\nF3D0    7793\nF3D1    5869\nF3D141  5958\nF3D142  3183\nF3D143  3178\nF3D144  4827\nF3D145  7377\nF3D146  5021\nF3D147  17070\nF3D148  12405\nF3D149  13083\nF3D150  5509\nF3D2    19620\nF3D3    6758\nF3D5    4448\nF3D6    7989\nF3D7    5129\nF3D8    5294\nF3D9    7070\nMock    4779\n\nTotal of all groups is 152360\n\nOutput File Names:\nstability.trim.contigs.fasta\nstability.trim.contigs.qual\nstability.contigs.report\nstability.scrap.contigs.fasta\nstability.scrap.contigs.qual\nstability.contigs.groups  The  stability.contigs.report  file will tell you something about the contig assembly for each read. Let's see what these sequences look like using the  summary.seqs  command:  summary.seqs(fasta=stability.trim.contigs.fasta)\n\nStart   End NBases  Ambigs  Polymer NumSeqs\nMinimum:    1   248 248 0   3   1\n2.5%-tile:  1   252 252 0   3   3810\n25%-tile:   1   252 252 0   4   38091\nMedian:     1   252 252 0   4   76181\n75%-tile:   1   253 253 0   5   114271\n97.5%-tile: 1   253 253 6   6   148552\nMaximum:    1   502 502 249 243 152360\nMean:   1   252.811 252.811 0.70063 4.44854\n# of Seqs:  152360  This tells us that we have 152360 sequences that for the most part vary between 248 and 253 bases. Interestingly, the longest read in the dataset is 502 bp. Be suspicious of this, the reads are supposed to be 251 bp each. This read clearly didn't assemble well (or at all). Also, note that at least 2.5% of our sequences had some ambiguous base calls. We'll take care of these issues in the next step when we run  screen.seqs .  screen.seqs(fasta=stability.trim.contigs.fasta, group=stability.contigs.groups, maxambig=0, maxlength=275)  You'll notice that mothur remembered that we used 8 processors in  make.contigs . To see what else mothur knows about you, run the following:  get.current()\n\nCurrent files saved by mothur:\nfasta=stability.trim.contigs.good.fasta\ngroup=stability.contigs.good.groups\nqfile=stability.trim.contigs.qual\nprocessors=8\nsummary=stability.trim.contigs.summary  What this means is that mothur remembers your latest fasta file and group file as well as the number of processors you have. So you could run:  mothur > summary.seqs(fasta=stability.trim.contigs.good.fasta)\nmothur > summary.seqs(fasta=current)\nmothur > summary.seqs()  and get the same output for each command.  But, now that we have filtered the sequencing errors, let's move to the next step.",
            "title": "Reducing Sequencing and PCR Errors"
        },
        {
            "location": "/16S/#processing-improved-sequences",
            "text": "We anticipate that many of our sequences are duplicates of each other. Because it's computationally wasteful to align the same sequences several times, we'll make our sequences unique:  unique.seqs(fasta=stability.trim.contigs.good.fasta)  If two sequences have the same identical sequence, then they're considered duplicates and will get merged. In the screen output there are two columns - the first is the number of sequences characterized and the second is the number of unique sequences remaining  Another thing to do to make our lives easier is to simplify the names and group files. If you look at the most recent versions of those files you'll see together they are 13 MB. This may not seem like much, but with a full MiSeq run those long sequence names can add up and make life tedious. So we'll run count.seqs to generate a table where the rows are the names of the unique seqeunces and the columns are the names of the groups. The table is then filled with the number of times each unique sequence shows up in each group.  This will generate a file called stability.trim.contigs.good.count_table. In subsequent commands we'll use it by using the count option:  count.seqs(name=stability.trim.contigs.good.names, group=stability.contigs.good.groups)\nsummary.seqs(count=stability.trim.contigs.good.count_table)\n\nUsing stability.trim.contigs.good.unique.fasta as input file for the fasta parameter.\n\nUsing 8 processors.\n\n        Start   End NBases  Ambigs  Polymer NumSeqs\nMinimum:    1   250 250 0   3   1\n2.5%-tile:  1   252 252 0   3   3227\n25%-tile:   1   252 252 0   4   32265\nMedian:     1   252 252 0   4   64530\n75%-tile:   1   253 253 0   5   96794\n97.5%-tile: 1   253 253 0   6   125832\nMaximum:    1   270 270 0   12  129058\nMean:   1   252.462 252.462 0   4.36663\n# of unique seqs:   16477\ntotal # of seqs:    129058  Now we need to align our sequences to the reference alignment.  First we need to download the SILVA database.  # This step should be done outside mothur\nwget http://www.mothur.org/w/images/9/98/Silva.bacteria.zip\nunzip Silva.bacteria.zip  If you have quit mothur to download the database, rerun the  mothur  command, then take a look at the database you have downloaded:  summary.seqs(fasta=silva.bacteria/silva.bacteria.fasta, processors=8)  Now do the alignment using  align.seqs :  align.seqs(fasta=stability.trim.contigs.good.unique.fasta, reference=silva.bacteria/silva.bacteria.fasta)  We can then run  summary.seqs  again to get a summary of our alignment:  summary.seqs(fasta=stability.trim.contigs.good.unique.align, count=stability.trim.contigs.good.count_table)  You'll see that the bulk of the sequences start at position 13862 and end at position 23444. Some sequences start at position 13144 or 13876 and end at 22587 or 25294. These deviants from the mode positions are likely due to an insertion or deletion at the terminal ends of the aliignments. Sometimes you'll see sequences that start and end at the same position indicating a very poor alignment, which is generally due to non-specific amplification. To make sure that everything overlaps the same region we'll re-run screen.seqs to get sequences that start at or before position 1968 and end at or after position 11550. We'll also set the maximum homopolymer length to 8 since there's nothing in the database with a stretch of 9 or more of the same base in a row (this really could have been done in the first execution of screen.seqs above). Note that we need the count table so that we can update the table for the sequences we're removing and we're also using the summary file so we don't have to figure out again all the start and stop positions:  screen.seqs(fasta=stability.trim.contigs.good.unique.align, count=stability.trim.contigs.good.count_table, summary=stability.trim.contigs.good.unique.summary, start=13862, end=23444, maxhomop=8)\nsummary.seqs(fasta=current, count=current)  No we can trim both ends of the aligned reads to be sure the all overlap exactly the same region. We can do this with  fliter.seqs  filter.seqs(fasta=stability.trim.contigs.good.unique.good.align, vertical=T, trump=.)  We may have introduced redundancy by trimming the ends of the sequences, so we will re-run  unique.seqs  unique.seqs(fasta=stability.trim.contigs.good.unique.good.filter.fasta, count=stability.trim.contigs.good.good.count_table)  This identified 3 duplicate sequences that we've now merged with previous unique sequences. The next thing we want to do to further de-noise our sequences is to pre-cluster the sequences using the  pre.cluster  command allowing for up to 2 differences between sequences. This command will split the sequences by group and then sort them by abundance and go from most abundant to least and identify sequences that are within 2 nt of each other. If they are then they get merged. We generally favor allowing 1 difference for every 100 bp of sequence:  pre.cluster(fasta=stability.trim.contigs.good.unique.good.filter.unique.fasta, count=stability.trim.contigs.good.unique.good.filter.count_table, diffs=2)  At this point we have removed as much sequencing error as we can and it is time to turn our attention to removing chimeras. We'll do this using the UCHIME algorithm that is called within mothur using the  chimera.uchime  command. Again, this command will split the data by sample and check for chimeras. Our preferred way of doing this is to use the abundant sequences as our reference. In addition, if a sequence is flagged as chimeric in one sample, the the default (dereplicate=F) is to remove it from all samples. Our experience suggests that this is a bit aggressive since we've seen rare sequences get flagged as chimeric when they're the most abundant sequence in another sample. This is how we do it:  chimera.uchime(fasta=stability.trim.contigs.good.unique.good.filter.unique.precluster.fasta, count=stability.trim.contigs.good.unique.good.filter.unique.precluster.count_table, dereplicate=t)  Running  chimera.uchime  with the count file will remove the chimeric sequences from the count file. But you still need to remove those sequences from the fasta file. We do this using  remove.seqs :  remove.seqs(fasta=stability.trim.contigs.good.unique.good.filter.unique.precluster.fasta, accnos=stability.trim.contigs.good.unique.good.filter.unique.precluster.denovo.uchime.accnos)  As a final quality control step, we need to see if there are any \"undesirables\" in our dataset. Sometimes when we pick a primer set they will amplify other stuff that gets to this point in the pipeline such as 18S rRNA gene fragments or 16S rRNA from Archaea, chloroplasts, and mitochondira. There's also just the random stuff that we want to get rid of.  Let's go ahead and classify those sequences using the Bayesian classifier with the  classify.seqs  command:  classify.seqs(fasta=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.fasta, count=stability.trim.contigs.good.unique.good.filter.unique.precluster.denovo.uchime.pick.count_table, reference=silva.bacteria/silva.bacteria.fasta, taxonomy=silva.bacteria/silva.bacteria.rdp.tax, cutoff=80)  Now that everything is classified we want to remove our undesirables. We do this with the  remove.lineage  command:  remove.lineage(fasta=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.fasta, count=stability.trim.contigs.good.unique.good.filter.unique.precluster.denovo.uchime.pick.count_table, taxonomy=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.rdp.wang.taxonomy, taxon=Chloroplast-Mitochondria-unknown-Archaea-Eukaryota)",
            "title": "Processing Improved Sequences"
        },
        {
            "location": "/16S/#analysis",
            "text": "",
            "title": "Analysis"
        },
        {
            "location": "/16S/#otus",
            "text": "We will use  cluster.split  for clustering sequences into OTUs  cluster.split(fasta=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.fasta, count=stability.trim.contigs.good.unique.good.filter.unique.precluster.denovo.uchime.pick.pick.count_table, taxonomy=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.rdp.wang.pick.taxonomy, splitmethod=classify, taxlevel=4, cutoff=0.15)  We used  taxlevel=4 , which corresponds to the level of  Order  Next we want to know how many sequences are in each OTU from each group and we can do this using the  make.shared command . Here we tell mothur that we're really only interested in the 0.03 cutoff level:  make.shared(list=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.an.unique_list.list, count=stability.trim.contigs.good.unique.good.filter.unique.precluster.denovo.uchime.pick.pick.count_table, label=0.03)  We also want to know the taxonomy for each of our OTUs. We can get the consensus taxonomy for each OTU using the  classify.otu  command  classify.otu(list=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.an.unique_list.list, count=stability.trim.contigs.good.unique.good.filter.unique.precluster.denovo.uchime.pick.pick.count_table, taxonomy=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.rdp.wang.pick.taxonomy, label=0.03)  If you open the file  stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.an.unique_list.0.03.cons.taxonomy , you can get information about your OTUs.  OTU Size    Taxonomy\nOtu0001 12328   Bacteria(100);Bacteroidetes(100);Bacteroidia(100);Bacteroidales(100);Porphyromonadaceae(100);Barnesiella(100);Barnesiella_unclassified(100);\nOtu0002 8918    Bacteria(100);Bacteroidetes(100);Bacteroidia(100);Bacteroidales(100);Porphyromonadaceae(100);Barnesiella(100);Barnesiella_unclassified(100);\nOtu0003 7850    Bacteria(100);Bacteroidetes(100);Bacteroidia(100);Bacteroidales(100);Porphyromonadaceae(100);Barnesiella(100);Barnesiella_unclassified(100);\nOtu0004 7478    Bacteria(100);Bacteroidetes(100);Bacteroidia(100);Bacteroidales(100);Porphyromonadaceae(100);Barnesiella(100);Barnesiella_unclassified(100);\nOtu0005 7478    Bacteria(100);Bacteroidetes(100);Bacteroidia(100);Bacteroidales(100);Porphyromonadaceae(100);Barnesiella(100);Barnesiella_unclassified(100);\nOtu0006 6650    Bacteria(100);Bacteroidetes(100);Bacteroidia(100);Bacteroidales(100);Porphyromonadaceae(100);Barnesiella(100);Barnesiella_unclassified(100);\nOtu0007 6341    Bacteria(100);Bacteroidetes(100);Bacteroidia(100);Bacteroidales(100);Bacteroidaceae(100);Bacteroides(100);Bacteroides_unclassified(100);\nOtu0008 5374    Bacteria(100);Bacteroidetes(100);Bacteroidia(100);Bacteroidales(100);Rikenellaceae(100);Alistipes(100);Alistipes_unclassified(100);\nOtu0009 3618    Bacteria(100);Bacteroidetes(100);Bacteroidia(100);Bacteroidales(100);Porphyromonadaceae(100);Barnesiella(100);Barnesiella_unclassified(100);  This is telling you that Otu0001 was observed 12328 times in your sample and that 100% of the sequences were from  Barnesiella   In order to vizualise the composition of our datasets, we'll use phyloseq, a R package to work with microbiom data.  Most of the phyloseq functionalities require aand a tree file. We need to generate it with mothur:  dist.seqs(fasta=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.fasta, output=lt, processors=8)\nclearcut(phylip=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.phylip.dist)",
            "title": "OTUs"
        },
        {
            "location": "/16S/#batch-mode",
            "text": "It is perfectly acceptable to enter the commands for your analysis from within mothur. We call this the interactive mode. If you are doing a lot these types of analysis or you want to use this SOP on your own data without thinking too much, you can run mothur in  batch mode  using  ./mothur script.batch  where script.batch (or whatever name you want, really) is a text file containing all the\ncommands that you previously entered in interactive mode.  If you have time, copy all the commands from this tutorial in a file, a try to make mothur work in batch mode!",
            "title": "Batch Mode"
        },
        {
            "location": "/16S/#phyloseq-analysis",
            "text": "First, install and load the phyloseq package:  source('http://bioconductor.org/biocLite.R')\nbiocLite('phyloseq')\n\nlibrary(\"phyloseq\")\nlibrary(\"ggplot2\")\nlibrary(\"plyr\")\ntheme_set(theme_bw())  # set the ggplot theme  The PhyloSeq package has an  import_mothur  function that you can use to import the files you generated with mothur. As an example, import the example mothur data provided by phyloseq as an example:  mothlist <- system.file(\"extdata\", \"esophagus.fn.list.gz\", package=\"phyloseq\")\nmothgroup <- system.file(\"extdata\", \"esophagus.good.groups.gz\", package=\"phyloseq\")\nmothtree <- system.file(\"extdata\", \"esophagus.tree.gz\", package=\"phyloseq\")\n\nshow_mothur_cutoffs(mothlist)\ncutoff <- '0.10'\nx <- import_mothur(mothlist, mothgroup, mothtree, cutoff)\nx  Note: If if you ever work with 16s data and decide to use QIIME instead of mothur, phyloseq also has an  import_qiime  function. Also, newer version of qiime and mothur have the ability to produce a  .biom  file.   \u201cThe biom file format (canonically pronounced \u2018biome\u2019) is designed to be a general-use format for representing counts of observations in one or more biological samples. BIOM is a recognized standard for the Earth Microbiome Project and is a Genomics Standards Consortium candidate project.\u201d   More info on  http://biom-format.org/  For the rest of this tutorial, we will work with an example dataset provided by the phyloseq package. Load the data with the following command:  data(enterotype)\ndata(\"GlobalPatterns\")",
            "title": "PhyloSeq Analysis"
        },
        {
            "location": "/16S/#ordination-and-distance-based-analysis",
            "text": "Let's do some preliminary filtering. Remove the OTUs that included all unassigned sequences (\"-1\")  enterotype <- subset_species(enterotype, Genus != \"-1\")  The available distance methods coded in the phyloseq package:  dist_methods <- unlist(distanceMethodList)\nprint(dist_methods)\n\n##     UniFrac1     UniFrac2        DPCoA          JSD     vegdist1\n##    \"unifrac\"   \"wunifrac\"      \"dpcoa\"        \"jsd\"  \"manhattan\"\n##     vegdist2     vegdist3     vegdist4     vegdist5     vegdist6\n##  \"euclidean\"   \"canberra\"       \"bray\" \"kulczynski\"    \"jaccard\"\n##     vegdist7     vegdist8     vegdist9    vegdist10    vegdist11\n##      \"gower\"   \"altGower\"   \"morisita\"       \"horn\"  \"mountford\"\n##    vegdist12    vegdist13    vegdist14    vegdist15   betadiver1\n##       \"raup\"   \"binomial\"       \"chao\"        \"cao\"          \"w\"\n##   betadiver2   betadiver3   betadiver4   betadiver5   betadiver6\n##         \"-1\"          \"c\"         \"wb\"          \"r\"          \"I\"\n##   betadiver7   betadiver8   betadiver9  betadiver10  betadiver11\n##          \"e\"          \"t\"         \"me\"          \"j\"        \"sor\"\n##  betadiver12  betadiver13  betadiver14  betadiver15  betadiver16\n##          \"m\"         \"-2\"         \"co\"         \"cc\"          \"g\"\n##  betadiver17  betadiver18  betadiver19  betadiver20  betadiver21\n##         \"-3\"          \"l\"         \"19\"         \"hk\"        \"rlb\"\n##  betadiver22  betadiver23  betadiver24        dist1        dist2\n##        \"sim\"         \"gl\"          \"z\"    \"maximum\"     \"binary\"\n##        dist3   designdist\n##  \"minkowski\"        \"ANY\"  Remove the two distance-methods that require a tree, and the generic custom method that requires user-defined distance arguments.  # These require tree\ndist_methods[(1:3)]\n\n# Remove them from the vector\ndist_methods <- dist_methods[-(1:3)]\n# This is the user-defined method:\ndist_methods[\"designdist\"]\n\n# Remove the user-defined distance\ndist_methods = dist_methods[-which(dist_methods==\"ANY\")]  Loop through each distance method, save each plot to a list, called plist.  plist <- vector(\"list\", length(dist_methods))\nnames(plist) = dist_methods\nfor( i in dist_methods ){\n    # Calculate distance matrix\n    iDist <- distance(enterotype, method=i)\n    # Calculate ordination\n    iMDS  <- ordinate(enterotype, \"MDS\", distance=iDist)\n    ## Make plot\n    # Don't carry over previous plot (if error, p will be blank)\n    p <- NULL\n    # Create plot, store as temp variable, p\n    p <- plot_ordination(enterotype, iMDS, color=\"SeqTech\", shape=\"Enterotype\")\n    # Add title to each plot\n    p <- p + ggtitle(paste(\"MDS using distance method \", i, sep=\"\"))\n    # Save the graphic to file.\n    plist[[i]] = p\n}  Combine results and shade according to Sequencing technology:  df = ldply(plist, function(x) x$data)\nnames(df)[1] <- \"distance\"\np = ggplot(df, aes(Axis.1, Axis.2, color=SeqTech, shape=Enterotype))\np = p + geom_point(size=3, alpha=0.5)\np = p + facet_wrap(~distance, scales=\"free\")\np = p + ggtitle(\"MDS on various distance metrics for Enterotype dataset\")\np  Print individual plots:  print(plist[[\"jsd\"]])\nprint(plist[[\"jaccard\"]])\nprint(plist[[\"bray\"]])\nprint(plist[[\"euclidean\"]])",
            "title": "Ordination and distance-based analysis"
        },
        {
            "location": "/16S/#alpha-diversity-graphics",
            "text": "Here is the default graphic produced by the plot_richness function on the GP example dataset:  GP <- prune_species(speciesSums(GlobalPatterns) > 0, GlobalPatterns)\nplot_richness(GP)  Note that in this case, the Fisher calculation results in a warning (but still plots). We can avoid this by specifying a measures argument to plot_richness, which will include just the alpha-diversity measures that we want.  plot_richness(GP, measures=c(\"Chao1\", \"Shannon\"))  We can specify a sample variable on which to group/organize samples along the horizontal (x) axis. An experimentally meaningful categorical variable is usually a good choice \u2013 in this case, the \"SampleType\" variable works much better than attempting to interpret the sample names directly (as in the previous plot):  plot_richness(GP, x=\"SampleType\", measures=c(\"Chao1\", \"Shannon\"))  Now suppose we wanted to use an external variable in the plot that isn\u2019t in the GP dataset already \u2013 for example, a logical that indicated whether or not the samples are human-associated. First, define this new variable, human, as a factor (other vectors could also work; or other data you might have describing the samples).  sampleData(GP)$human <- getVariable(GP, \"SampleType\") %in% c(\"Feces\", \"Mock\", \"Skin\", \"Tongue\")  Now tell plot_richness to map the new human variable on the horizontal axis, and shade the points in different color groups, according to which \"SampleType\" they belong.  plot_richness(GP, x=\"human\", color=\"SampleType\", measures=c(\"Chao1\", \"Shannon\"))  We can merge samples that are from the environment (SampleType), and make the points bigger with a ggplot2 layer. First, merge the samples.  GPst = merge_samples(GP, \"SampleType\")\n# repair variables that were damaged during merge (coerced to numeric)\nsample_data(GPst)$SampleType <- factor(sample_names(GPst))\nsample_data(GPst)$human <- as.logical(sample_data(GPst)$human)\n\np = plot_richness(GPst, x=\"human\", color=\"SampleType\", measures=c(\"Chao1\", \"Shannon\"))\np + geom_point(size=5, alpha=0.7)",
            "title": "Alpha diversity graphics"
        },
        {
            "location": "/16S/#trees",
            "text": "head(phy_tree(GlobalPatterns)$node.label, 10)  The node data from the  GlobalPatterns  dataset are strange. They look like they might be bootstrap values, but they sometimes have two decimals.  phy_tree(GlobalPatterns)$node.label = substr(phy_tree(GlobalPatterns)$node.label, 1, 4)  Additionally, the dataset has many OTUs, too many to fit them all on a tree. Let's take the 50 more abundant and plot a basic tree:  physeq = prune_taxa(taxa_names(GlobalPatterns)[1:50], GlobalPatterns)\nplot_tree(physeq)  dots are annotated next to tips (OTUs) in the tree, one for each sample in which that OTU was observed. Let's color the dots by taxonomic ranks, and sample covariates:  plot_tree(physeq, nodelabf=nodeplotboot(), ladderize=\"left\", color=\"SampleType\")  by taxonomic class:  plot_tree(physeq, nodelabf=nodeplotboot(), ladderize=\"left\", color=\"Class\")  It can be useful to label the tips:  plot_tree(physeq, color=\"SampleType\", label.tips=\"Genus\")  Making a radial tree is easy with ggplot2, simply recognizing that our vertically-oriented tree is a cartesian mapping of the data to a graphic \u2013 and that a radial tree is the same mapping, but with polar coordinates instead.  plot_tree(physeq, nodelabf=nodeplotboot(60,60,3), color=\"SampleType\", shape=\"Class\", ladderize=\"left\") + coord_polar(theta=\"y\")",
            "title": "Trees"
        },
        {
            "location": "/16S/#bar-plots",
            "text": "Bar plots are one of the easiest way to vizualize your data. But be careful, they can be misleading if grouping sample!  Let's take a subset of the GlobalPatterns dataset, and produce a basic bar plot:  gp.ch = subset_taxa(GlobalPatterns, Phylum == \"Chlamydiae\")\nplot_bar(gp.ch)  The dataset is plotted with every sample mapped individually to the horizontal (x) axis, and abundance values mapped to the veritcal (y) axis. At each sample\u2019s horizontal position, the abundance values for each OTU are stacked in order from greatest to least, separate by a thin horizontal line. As long as the parameters you choose to separate the data result in more than one OTU abundance value at the respective position in the plot, the values will be stacked in order as a means of displaying both the sum total value while still representing the individual OTU abundances.  The bar plot will be clearer with color to represent the Genus to which each OTU belongs.  plot_bar(gp.ch, fill=\"Genus\")  Now keep the same fill color, and group the samples together by the SampleType variable; essentially, the environment from which the sample was taken and sequenced.  plot_bar(gp.ch, x=\"SampleType\", fill=\"Genus\")  A more complex example using facets:  plot_bar(gp.ch, \"Family\", fill=\"Genus\", facet_grid=~SampleType)",
            "title": "Bar plots"
        },
        {
            "location": "/16S/#heatmaps",
            "text": "The following two lines subset the dataset to just the top 300 most abundant Bacteria taxa across all samples (in this case, with no prior preprocessing. Not recommended, but quick).  data(\"GlobalPatterns\")\ngpt <- subset_taxa(GlobalPatterns, Kingdom==\"Bacteria\")\ngpt <- prune_taxa(names(sort(taxa_sums(gpt),TRUE)[1:300]), gpt)\nplot_heatmap(gpt, sample.label=\"SampleType\")  subset a smaller dataset based on an Archaeal phylum  gpac <- subset_taxa(GlobalPatterns, Phylum==\"Crenarchaeota\")\nplot_heatmap(gpac)",
            "title": "Heatmaps"
        },
        {
            "location": "/16S/#plot-microbiome-network",
            "text": "There is a random aspect to some of the network layout methods. For complete reproducibility of the images produced later in this tutorial, it is possible to set the random number generator seed explicitly:  set.seed(711L)  Because we want to use the enterotype designations as a plot feature in these plots, we need to remove the 9 samples for which no enterotype designation was assigned (this will save us the hassle of some pesky warning messages, but everything still works; the offending samples are anyway omitted).  enterotype = subset_samples(enterotype, !is.na(Enterotype))  Create an igraph-based network based on the default distance method, \u201cJaccard\u201d, and a maximum distance between connected nodes of 0.3.  ig <- make_network(enterotype, max.dist=0.3)\nplot_network(ig, enterotype)  The previous graphic displayed some interesting structure, with one or two major subgraphs comprising a majority of samples. Furthermore, there seemed to be a correlation in the sample naming scheme and position within the network. Instead of trying to read all of the sample names to understand the pattern, let\u2019s map some of the sample variables onto this graphic as color and shape:  plot_network(ig, enterotype, color=\"SeqTech\", shape=\"Enterotype\", line_weight=0.4, label=NULL)  In the previous examples, the choice of maximum-distance and distance method were informed, but arbitrary. Let\u2019s see what happens when the maximum distance is lowered, decreasing the number of edges in the network  ig <- make_network(enterotype, max.dist=0.2)\nplot_network(ig, enterotype, color=\"SeqTech\", shape=\"Enterotype\", line_weight=0.4, label=NULL)  Let\u2019s repeat the previous exercise, but replace the Jaccard (default) distance method with Bray-Curtis  ig <- make_network(enterotype, dist.fun=\"bray\", max.dist=0.3)\nplot_network(ig, enterotype, color=\"SeqTech\", shape=\"Enterotype\", line_weight=0.4, label=NULL)",
            "title": "Plot microbiome network"
        },
        {
            "location": "/wms/",
            "text": "Whole Metagenome Sequencin\n\u00b6\n\n\nTable of Contents\n\u00b6\n\n\n\n\nIntroduction\n\n\nThe Pig Microbiome\n\n\nWhole Metagenome Sequencing\n\n\n\n\n\n\nSoftwares Required for this Tutorial\n\n\nGetting the Data and Checking their Quality\n\n\nTaxonomic Classification\n\n\nVisualization\n\n\n\n\nIntroduction\n\u00b6\n\n\nMicrobiome used\n\u00b6\n\n\nIn this tutorial we will compare samples from the Pig Gut Microbiome to samples from the Human Gut Microbiome. Below you'll find a brief description of the two projects:\n\n\nThe Pig Microbiome:\n\n\n\n\nPig is a main species for livestock and biomedicine. The pig genome sequence was recently reported. To boost research, we established a catalogue of the genes of the gut microbiome based on faecal samples of 287 pigs from France, Denmark and China. More than 7.6 million non-redundant genes representing 719 metagenomic species were identified by deep metagenome sequencing, highlighting more similarities with the human than with the mouse catalogue. The pig and human catalogues share only 12.6 and 9.3 % of their genes, respectively, but 70 and 95% of their functional pathways. The pig gut microbiota is influenced by gender, age and breed. Analysis of the prevalence of antibiotics resistance genes (ARGs) reflected antibiotics supplementation in each farm system, and revealed that non-antibiotics-fed animals still harbour ARGs. The pig catalogue creates a resource for whole metagenomics-based studies, highly valuable for research in biomedicine and for sustainable knowledge-based pig farming\n\n\n\n\nThe Human Microbiome:\n\n\n\n\nWe are facing a global metabolic health crisis provoked by an obesity epidemic. Here we report the human gut microbial composition in a population sample of 123 non-obese and 169 obese Danish individuals. We find two groups of individuals that differ by the number of gut microbial genes and thus gut bacterial richness. They harbour known and previously unknown bacterial species at different proportions; individuals with a low bacterial richness (23% of the population) are characterized by more marked overall adiposity, insulin resistance and dyslipidaemia and a more pronounced inflammatory phenotype when compared with high bacterial richness individuals. The obese individuals among the former also gain more weight over time. Only a few bacterial species are sufficient to distinguish between individuals with high and low bacterial richness, and even between lean and obese. Our classifications based on variation in the gut microbiome identify subsets of individuals in the general white adult population who may be at increased risk of progressing to adiposity-associated co-morbidities\n\n\n\n\nWhole Metagenome Sequencing\n\u00b6\n\n\nWhole Metagenome sequencing (WMS), or shotgun metagenome sequencing, is a relatively new and powerful sequencing approach that provides insight into community biodiversity and function. On the contrary of Metabarcoding, where only a specific region of the bacterial community (the 16s rRNA) is sequenced, WMS aims at sequencing all the genomic material present in the environment.\n\n\nThe choice of shotgun or 16S approaches is usually dictated by the nature of the studies being conducted. For instance, 16S is well suited for analysis of large number of samples, i.e., multiple patients, longitudinal studies, etc. but offers limited taxonomical and functional resolution. WMS is generally more expensive but offers increased resolution, and allows the discovery of viruses as well as other mobile genetic elements.\n\n\nSoftwares Required for this Tutorial\n\u00b6\n\n\n\n\nFastQC\n\n\nKraken\n\n\nBracken\n\n\nR\n\n\nPavian\n\n\n\n\nGetting the Data and Checking their Quality\n\u00b6\n\n\nIf you are reading this tutorial online and haven't cloned the directory, first download and unpack the data:\n\n\nwget http://77.235.253.14/metlab/wms.tar\ntar xvf wms.tar\ncd wms\n\n\n\n\nWe'll use FastQC to check the quality of our data. FastQC can be downloaded and\nrun on a Windows or Linux computer without installation. It is available \nhere\n\n\nStart FastQC and select the fastq file you just downloaded with \nfile -> open\n\nWhat do you think about the quality of the reads? Do they need trimming? Are there still adapters\npresent? Overrepresented sequences?\n\n\nAlternatively, run fastqc on the command-line:\n\n\nfastqc *.fastq.gz\n\n\nIf the quality appears to be good, it's because it was probably the cleaned reads that were deposited into SRA.\nWe can directly move to the classification step.\n\n\nTaxonomic Classification\n\u00b6\n\n\nKraken\n is a system for assigning taxonomic labels to short DNA sequences (i.e. reads)\n\nKraken aims to achieve high sensitivity and high speed by utilizing exact alignments of k-mers and a novel classification algorithm (sic).\n\n\nIn short, kraken uses a new approach with exact k-mer matching to assign taxonomy to short reads. It is \nextremely\n fast compared to traditional\napproaches (i.e. BLAST).\n\n\nBy default, the authors of kraken built their database based on RefSeq Bacteria, Archea and Viruses. We'll use it for the purpose of this tutorial.\n\n\nNOTE: The database may have been installed already! Ask your instructor!\n\n\n# You might not need this step (for example if you're working on Uppmax!)\nwget https://ccb.jhu.edu/software/kraken/dl/minikraken.tgz\ntar xzf minikraken.tgz\n$KRAKEN_DB=minikraken_20141208\n\n\n\n\nNow run kraken on the reads\n\n\nmkdir kraken_results\nfor i in *_1.fastq.gz\ndo\n    prefix=$(basename $i _1.fastq.gz)\n    # set number of threads to number of cores if running under SLURM, otherwise use 2 threads\n    nthreads=${SLURM_NPROCS:=2}\n    kraken --db $KRAKEN_DB --threads ${nthreads} --fastq-input --gzip-compressed \\\n        ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz > kraken_results/${prefix}.tab\n    kraken-report --db $KRAKEN_DB \\\n        kraken_results/${prefix}.tab > kraken_results/${prefix}_tax.txt\ndone\n\n\n\n\nwhich produces a tab-delimited file with an assigned TaxID for each read.\n\n\nKraken includes a script called \nkraken-report\n to transform this file into a \"tree\" view with the percentage of reads assigned to each taxa. We've run this script at each step in the loop. Take a look at the \n_tax.txt\n files!\n\n\nAbundance estimation using Bracken\n\u00b6\n\n\nBracken (Bayesian Reestimation of Abundance with KrakEN) is a highly accurate statistical method that computes the abundance of species in DNA sequences from a metagenomics sample\n\n\nBefore starting, you need to install Bracken:\n\n\ncd\ngit clone https://github.com/jenniferlu717/Bracken.git\nchmod 755 Bracken/*.py\nchmod 755 Bracken/*.pl\nexport PATH=$PATH:$HOME/Bracken\n\n\n\n\nUnfortunately, Uppmax lacks some perl packages necessary for Bracken to work:\n\n\nFollow the tutorial \nhere\n to install \ncpanm\n\n\nthen install the two perl libraries that are missing:\n\n\ncpanm Parallel::ForkManager\ncpanm List::MoreUtils\n\n\n\n\nThree steps are necessary to set up Kraken abundance estimation.\n\n\n\n\n\n\nClassify all reads using Kraken and Generate a Kraken report file. We've done this!\n\n\n\n\n\n\nSearch all library input sequences against the database and compute the classifications for each perfect read of ${READ_LENGTH} base pairs from one of the input sequences.\n\n\n\n\n\n\nfind -L $KRAKEN_DB/library -name \"*.fna\" -o -name \"*.fa\" -o -name \"*.fasta\" > genomes.list\ncat $(grep -v '^#' genomes.list) > genomes.fasta\nkraken --db=${KRAKEN_DB} --fasta-input --threads=${SLURM_NPROCS:=10} kraken.fasta > database.kraken\ncount-kmer-abundances.pl --db=${KRAKEN_DB} --read-length=100 database.kraken > database100mers.kraken_cnts\n\n\n\n\n\n\nGenerate the kmer distribution file\n\n\n\n\npython generate_kmer_distribution.py -i database100mers.kraken_cnts -o KMER_DISTR.TXT\n\n\n\n\nNow, given the expected kmer distribution for genomes in a kraken database along\nwith a kraken report file, the number of reads belonging to each species (or\ngenus) is estimated using the estimate_abundance.py file, run with the\nfollowing command line:\n\n\npython estimate_abundance.py -i KRAKEN.REPORT -k KMER_DISTR.TXT -o OUTPUT_FILE.TXT\n\n\nRun this command for the six \n_tax.txt\n files that you generated with kraken!\n\n\nThe following required parameters must be specified:\n- KRAKEN.REPORT     :: the kraken report generated for a given dataset\n- KMER_DISTR.TXT    :: the file generated by generate_kmer_distribution.py\n- OUTPUT_FILE.TXT   :: the desired name of the output file to be generated by the code\n\n\nVisualization\n\u00b6\n\n\nAlternative 1: Pavian\n\u00b6\n\n\nPavian is a web application for exploring metagenomics classification results.\n\n\nInstall and run Pavian:\n\n\n(In R or Rstudio)\n\n\n## Installs required packages from CRAN and Bioconductor\nsource(\"https://raw.githubusercontent.com/fbreitwieser/pavian/master/inst/shinyapp/install-pavian.R\")\npavian::runApp(port=5000)\n\n\n\n\nPavian will be available at http://127.0.0.1:5000",
            "title": "Whole Metagenome Sequencing"
        },
        {
            "location": "/wms/#whole-metagenome-sequencin",
            "text": "",
            "title": "Whole Metagenome Sequencin"
        },
        {
            "location": "/wms/#table-of-contents",
            "text": "Introduction  The Pig Microbiome  Whole Metagenome Sequencing    Softwares Required for this Tutorial  Getting the Data and Checking their Quality  Taxonomic Classification  Visualization",
            "title": "Table of Contents"
        },
        {
            "location": "/wms/#introduction",
            "text": "",
            "title": "Introduction"
        },
        {
            "location": "/wms/#microbiome-used",
            "text": "In this tutorial we will compare samples from the Pig Gut Microbiome to samples from the Human Gut Microbiome. Below you'll find a brief description of the two projects:  The Pig Microbiome:   Pig is a main species for livestock and biomedicine. The pig genome sequence was recently reported. To boost research, we established a catalogue of the genes of the gut microbiome based on faecal samples of 287 pigs from France, Denmark and China. More than 7.6 million non-redundant genes representing 719 metagenomic species were identified by deep metagenome sequencing, highlighting more similarities with the human than with the mouse catalogue. The pig and human catalogues share only 12.6 and 9.3 % of their genes, respectively, but 70 and 95% of their functional pathways. The pig gut microbiota is influenced by gender, age and breed. Analysis of the prevalence of antibiotics resistance genes (ARGs) reflected antibiotics supplementation in each farm system, and revealed that non-antibiotics-fed animals still harbour ARGs. The pig catalogue creates a resource for whole metagenomics-based studies, highly valuable for research in biomedicine and for sustainable knowledge-based pig farming   The Human Microbiome:   We are facing a global metabolic health crisis provoked by an obesity epidemic. Here we report the human gut microbial composition in a population sample of 123 non-obese and 169 obese Danish individuals. We find two groups of individuals that differ by the number of gut microbial genes and thus gut bacterial richness. They harbour known and previously unknown bacterial species at different proportions; individuals with a low bacterial richness (23% of the population) are characterized by more marked overall adiposity, insulin resistance and dyslipidaemia and a more pronounced inflammatory phenotype when compared with high bacterial richness individuals. The obese individuals among the former also gain more weight over time. Only a few bacterial species are sufficient to distinguish between individuals with high and low bacterial richness, and even between lean and obese. Our classifications based on variation in the gut microbiome identify subsets of individuals in the general white adult population who may be at increased risk of progressing to adiposity-associated co-morbidities",
            "title": "Microbiome used"
        },
        {
            "location": "/wms/#whole-metagenome-sequencing",
            "text": "Whole Metagenome sequencing (WMS), or shotgun metagenome sequencing, is a relatively new and powerful sequencing approach that provides insight into community biodiversity and function. On the contrary of Metabarcoding, where only a specific region of the bacterial community (the 16s rRNA) is sequenced, WMS aims at sequencing all the genomic material present in the environment.  The choice of shotgun or 16S approaches is usually dictated by the nature of the studies being conducted. For instance, 16S is well suited for analysis of large number of samples, i.e., multiple patients, longitudinal studies, etc. but offers limited taxonomical and functional resolution. WMS is generally more expensive but offers increased resolution, and allows the discovery of viruses as well as other mobile genetic elements.",
            "title": "Whole Metagenome Sequencing"
        },
        {
            "location": "/wms/#softwares-required-for-this-tutorial",
            "text": "FastQC  Kraken  Bracken  R  Pavian",
            "title": "Softwares Required for this Tutorial"
        },
        {
            "location": "/wms/#getting-the-data-and-checking-their-quality",
            "text": "If you are reading this tutorial online and haven't cloned the directory, first download and unpack the data:  wget http://77.235.253.14/metlab/wms.tar\ntar xvf wms.tar\ncd wms  We'll use FastQC to check the quality of our data. FastQC can be downloaded and\nrun on a Windows or Linux computer without installation. It is available  here  Start FastQC and select the fastq file you just downloaded with  file -> open \nWhat do you think about the quality of the reads? Do they need trimming? Are there still adapters\npresent? Overrepresented sequences?  Alternatively, run fastqc on the command-line:  fastqc *.fastq.gz  If the quality appears to be good, it's because it was probably the cleaned reads that were deposited into SRA.\nWe can directly move to the classification step.",
            "title": "Getting the Data and Checking their Quality"
        },
        {
            "location": "/wms/#taxonomic-classification",
            "text": "Kraken  is a system for assigning taxonomic labels to short DNA sequences (i.e. reads) \nKraken aims to achieve high sensitivity and high speed by utilizing exact alignments of k-mers and a novel classification algorithm (sic).  In short, kraken uses a new approach with exact k-mer matching to assign taxonomy to short reads. It is  extremely  fast compared to traditional\napproaches (i.e. BLAST).  By default, the authors of kraken built their database based on RefSeq Bacteria, Archea and Viruses. We'll use it for the purpose of this tutorial.  NOTE: The database may have been installed already! Ask your instructor!  # You might not need this step (for example if you're working on Uppmax!)\nwget https://ccb.jhu.edu/software/kraken/dl/minikraken.tgz\ntar xzf minikraken.tgz\n$KRAKEN_DB=minikraken_20141208  Now run kraken on the reads  mkdir kraken_results\nfor i in *_1.fastq.gz\ndo\n    prefix=$(basename $i _1.fastq.gz)\n    # set number of threads to number of cores if running under SLURM, otherwise use 2 threads\n    nthreads=${SLURM_NPROCS:=2}\n    kraken --db $KRAKEN_DB --threads ${nthreads} --fastq-input --gzip-compressed \\\n        ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz > kraken_results/${prefix}.tab\n    kraken-report --db $KRAKEN_DB \\\n        kraken_results/${prefix}.tab > kraken_results/${prefix}_tax.txt\ndone  which produces a tab-delimited file with an assigned TaxID for each read.  Kraken includes a script called  kraken-report  to transform this file into a \"tree\" view with the percentage of reads assigned to each taxa. We've run this script at each step in the loop. Take a look at the  _tax.txt  files!",
            "title": "Taxonomic Classification"
        },
        {
            "location": "/wms/#abundance-estimation-using-bracken",
            "text": "Bracken (Bayesian Reestimation of Abundance with KrakEN) is a highly accurate statistical method that computes the abundance of species in DNA sequences from a metagenomics sample  Before starting, you need to install Bracken:  cd\ngit clone https://github.com/jenniferlu717/Bracken.git\nchmod 755 Bracken/*.py\nchmod 755 Bracken/*.pl\nexport PATH=$PATH:$HOME/Bracken  Unfortunately, Uppmax lacks some perl packages necessary for Bracken to work:  Follow the tutorial  here  to install  cpanm  then install the two perl libraries that are missing:  cpanm Parallel::ForkManager\ncpanm List::MoreUtils  Three steps are necessary to set up Kraken abundance estimation.    Classify all reads using Kraken and Generate a Kraken report file. We've done this!    Search all library input sequences against the database and compute the classifications for each perfect read of ${READ_LENGTH} base pairs from one of the input sequences.    find -L $KRAKEN_DB/library -name \"*.fna\" -o -name \"*.fa\" -o -name \"*.fasta\" > genomes.list\ncat $(grep -v '^#' genomes.list) > genomes.fasta\nkraken --db=${KRAKEN_DB} --fasta-input --threads=${SLURM_NPROCS:=10} kraken.fasta > database.kraken\ncount-kmer-abundances.pl --db=${KRAKEN_DB} --read-length=100 database.kraken > database100mers.kraken_cnts   Generate the kmer distribution file   python generate_kmer_distribution.py -i database100mers.kraken_cnts -o KMER_DISTR.TXT  Now, given the expected kmer distribution for genomes in a kraken database along\nwith a kraken report file, the number of reads belonging to each species (or\ngenus) is estimated using the estimate_abundance.py file, run with the\nfollowing command line:  python estimate_abundance.py -i KRAKEN.REPORT -k KMER_DISTR.TXT -o OUTPUT_FILE.TXT  Run this command for the six  _tax.txt  files that you generated with kraken!  The following required parameters must be specified:\n- KRAKEN.REPORT     :: the kraken report generated for a given dataset\n- KMER_DISTR.TXT    :: the file generated by generate_kmer_distribution.py\n- OUTPUT_FILE.TXT   :: the desired name of the output file to be generated by the code",
            "title": "Abundance estimation using Bracken"
        },
        {
            "location": "/wms/#visualization",
            "text": "",
            "title": "Visualization"
        },
        {
            "location": "/wms/#alternative-1-pavian",
            "text": "Pavian is a web application for exploring metagenomics classification results.  Install and run Pavian:  (In R or Rstudio)  ## Installs required packages from CRAN and Bioconductor\nsource(\"https://raw.githubusercontent.com/fbreitwieser/pavian/master/inst/shinyapp/install-pavian.R\")\npavian::runApp(port=5000)  Pavian will be available at http://127.0.0.1:5000",
            "title": "Alternative 1: Pavian"
        },
        {
            "location": "/meta_assembly/",
            "text": "Metagenome assembly\n\u00b6\n\n\nIn this tutorial you'll learn how to inspect the quality of High-throughput sequencing and\nperform a metagenomic assembly.\n\n\nWe will use data under the accession SRS018585 in the Sequence Read Archive. this sample is\n\"a Human Metagenome sample from G_DNA_Anterior nares of a male participant in the dbGaP study\nHMP Core Microbiome Sampling Protocol A (HMP-A)\"\n\n\nTable of Contents\n\u00b6\n\n\n\n\nSoftwares Required for this Tutorial\n\n\nGetting the Data\n\n\nQuality Control\n\n\nAssembly\n\n\nTaxonomic Classification and Visualization\n\n\n\n\nSoftwares Required for this Tutorial\n\u00b6\n\n\n\n\nFastQC\n\n\nsickle\n\n\nSPAdes\n\n\nBlast\n\n\nblobtools\n\n\n\n\nGetting the Data\n\u00b6\n\n\nwget http://downloads.hmpdacc.org/data/Illumina/anterior_nares/SRS018585.tar.bz2\ntar xjf SRS018585.tar.bz2\ncd SRS018585\n\n\n\n\nQuality Control\n\u00b6\n\n\nwe'll use FastQC to check the quality of our data. FastQC can be downloaded and\nran on a Windows or LINUX computer without installation. It is available \nhere\n\n\nStart FastQC and select the fastq files you just downloaded with \nfile -> open\n\n\nWhat is the average read length? The average quality?\n\n\nNow we'll trim the reads using sickle\n\n\nsickle pe -f SRS018585.denovo_duplicates_marked.trimmed.1.fastq \\\n-r SRS018585.denovo_duplicates_marked.trimmed.2.fastq -t sanger \\\n-o SRS018585_trimmed_1.fastq -p SRS018585_trimmed_2.fastq -s unpaired.fastq\n\n\n\n\nsickle normally gives you a summary of how many reads were trimmed.\n\n\nAssembly\n\u00b6\n\n\nSPAdes will be used for the assembly. Since version 3.7, SPAdes includes a metagenomic version of its algorithm, callable\nwith the option --meta\n\n\nspades.py --meta -1 SRS018585_trimmed_1.fastq -2 SRS018585_trimmed_2.fastq -t 8 -o assembly\n\n\n\n\nthe resulting assenmbly can be found under assembly/scaffolds.fasta. How many contigs does this assembly contain?\nHow long is the longest contig and to what organism does it belong to?\n\n\nTaxonomic Classification and Visualization\n\u00b6\n\n\nFor the vizualisation of the assembly we will use a tool called blobtools.\nBlobtools produces \"Taxon annotated GC-coverage plots\" (TAGC) and was orignially made for\nthe visualisation of (draft) genome assemblies.  \n\n\nmkdir blobtools && cd $_\nblastn -num_threads 8 -db nt -query ../assembly/scaffolds.fasta -out blastresults.txt -outfmt '6 qseqid staxids bitscore'\n\n\n\n\nThis blast step is necessary to obtain the taxonomic information of your contigs.\nIt might take a while. Be patient!\n\n\nblobtools create -i ../assembly/scaffolds.fasta -y spades -t blastresults.txt \\\n    --nodes /export/databases/taxonomy/nodes.dmp \\\n    --names /export/databases/taxonomy/names.dmp \\\n    -o scaffolds --title SRS018585\nblobtools plot -i scaffolds.blob.BlobDB.json -o scaffolds --title -r family\n\n\n\n\nInspect the plot, what is the most abundant families? try to play with the parameters\n(especially \n-r\n)",
            "title": "Metagenome assembly"
        },
        {
            "location": "/meta_assembly/#metagenome-assembly",
            "text": "In this tutorial you'll learn how to inspect the quality of High-throughput sequencing and\nperform a metagenomic assembly.  We will use data under the accession SRS018585 in the Sequence Read Archive. this sample is\n\"a Human Metagenome sample from G_DNA_Anterior nares of a male participant in the dbGaP study\nHMP Core Microbiome Sampling Protocol A (HMP-A)\"",
            "title": "Metagenome assembly"
        },
        {
            "location": "/meta_assembly/#table-of-contents",
            "text": "Softwares Required for this Tutorial  Getting the Data  Quality Control  Assembly  Taxonomic Classification and Visualization",
            "title": "Table of Contents"
        },
        {
            "location": "/meta_assembly/#softwares-required-for-this-tutorial",
            "text": "FastQC  sickle  SPAdes  Blast  blobtools",
            "title": "Softwares Required for this Tutorial"
        },
        {
            "location": "/meta_assembly/#getting-the-data",
            "text": "wget http://downloads.hmpdacc.org/data/Illumina/anterior_nares/SRS018585.tar.bz2\ntar xjf SRS018585.tar.bz2\ncd SRS018585",
            "title": "Getting the Data"
        },
        {
            "location": "/meta_assembly/#quality-control",
            "text": "we'll use FastQC to check the quality of our data. FastQC can be downloaded and\nran on a Windows or LINUX computer without installation. It is available  here  Start FastQC and select the fastq files you just downloaded with  file -> open  What is the average read length? The average quality?  Now we'll trim the reads using sickle  sickle pe -f SRS018585.denovo_duplicates_marked.trimmed.1.fastq \\\n-r SRS018585.denovo_duplicates_marked.trimmed.2.fastq -t sanger \\\n-o SRS018585_trimmed_1.fastq -p SRS018585_trimmed_2.fastq -s unpaired.fastq  sickle normally gives you a summary of how many reads were trimmed.",
            "title": "Quality Control"
        },
        {
            "location": "/meta_assembly/#assembly",
            "text": "SPAdes will be used for the assembly. Since version 3.7, SPAdes includes a metagenomic version of its algorithm, callable\nwith the option --meta  spades.py --meta -1 SRS018585_trimmed_1.fastq -2 SRS018585_trimmed_2.fastq -t 8 -o assembly  the resulting assenmbly can be found under assembly/scaffolds.fasta. How many contigs does this assembly contain?\nHow long is the longest contig and to what organism does it belong to?",
            "title": "Assembly"
        },
        {
            "location": "/meta_assembly/#taxonomic-classification-and-visualization",
            "text": "For the vizualisation of the assembly we will use a tool called blobtools.\nBlobtools produces \"Taxon annotated GC-coverage plots\" (TAGC) and was orignially made for\nthe visualisation of (draft) genome assemblies.    mkdir blobtools && cd $_\nblastn -num_threads 8 -db nt -query ../assembly/scaffolds.fasta -out blastresults.txt -outfmt '6 qseqid staxids bitscore'  This blast step is necessary to obtain the taxonomic information of your contigs.\nIt might take a while. Be patient!  blobtools create -i ../assembly/scaffolds.fasta -y spades -t blastresults.txt \\\n    --nodes /export/databases/taxonomy/nodes.dmp \\\n    --names /export/databases/taxonomy/names.dmp \\\n    -o scaffolds --title SRS018585\nblobtools plot -i scaffolds.blob.BlobDB.json -o scaffolds --title -r family  Inspect the plot, what is the most abundant families? try to play with the parameters\n(especially  -r )",
            "title": "Taxonomic Classification and Visualization"
        },
        {
            "location": "/rna/",
            "text": "RNA-Seq\n\u00b6\n\n\nLoad salmon\n\u00b6\n\n\nmodule load Salmon\n\n\n\n\nDownloading the data\n\u00b6\n\n\nFor this tutorial we will use the test data from \nthis\n paper:\n\n\n\n\nMalachi Griffith\n, Jason R. Walker, Nicholas C. Spies, Benjamin J. Ainscough, Obi L. Griffith\n. 2015. Informatics for RNA-seq: A web resource for analysis on the cloud. PLoS Comp Biol. 11(8):e1004393.\n\n\n\n\nThe test data consists of two commercially available RNA samples: Universal Human Reference (UHR) and Human Brain Reference (HBR). The UHR is total RNA isolated from a diverse set of 10 cancer cell lines. The HBR is total RNA isolated from the brains of 23 Caucasians, male and female, of varying age but mostly 60-80 years old.\n\n\nIn addition, a spike-in control was used. Specifically we added an aliquot of the ERCC ExFold RNA Spike-In Control Mixes to each sample. The spike-in consists of 92 transcripts that are present in known concentrations across a wide abundance range (from very few copies to many copies). This range allows us to test the degree to which the RNA-seq assay (including all laboratory and analysis steps) accurately reflects the relative abundance of transcript species within a sample. There are two 'mixes' of these transcripts to allow an assessment of differential expression output between samples if you put one mix in each of your two comparisons. In our case, Mix1 was added to the UHR sample, and Mix2 was added to the HBR sample. We also have 3 complete experimental replicates for each sample. This allows us to assess the technical variability of our overall process of producing RNA-seq data in the lab.\n\n\nFor all libraries we prepared low-throughput (Set A) TruSeq Stranded Total RNA Sample Prep Kit libraries with Ribo-Zero Gold to remove both cytoplasmic and mitochondrial rRNA. Triplicate, indexed libraries were made starting with 100ng Agilent/Strategene Universal Human Reference total RNA and 100ng Ambion Human Brain Reference total RNA. The Universal Human Reference replicates received 2 ul of 1:1000 ERCC Mix 1. The Human Brain Reference replicates received 1:1000 ERCC Mix 2. The libraries were quantified with KAPA Library Quantification qPCR and adjusted to the appropriate concentration for sequencing. The triplicate, indexed libraries were then pooled prior to sequencing. Each pool of three replicate libraries were sequenced across 2 lanes of a HiSeq 2000 using paired-end sequence chemistry with 100bp read lengths.\n\n\nSo to summarize we have:\n\n\n\n\nUHR + ERCC Spike-In Mix1, Replicate 1\n\n\nUHR + ERCC Spike-In Mix1, Replicate 2\n\n\nUHR + ERCC Spike-In Mix1, Replicate 3\n\n\nHBR + ERCC Spike-In Mix2, Replicate 1\n\n\nHBR + ERCC Spike-In Mix2, Replicate 2\n\n\nHBR + ERCC Spike-In Mix2, Replicate 3\n\n\n\n\nYou can download the data from \nhere\n.\n\n\nUnpack the data and go into the toy_rna directory\n\n\ntar xzf toy_rna.tar.gz\ncd toy_rna\n\n\n\n\nIndexing transcriptome\n\u00b6\n\n\nsalmon index -t chr22_transcripts.fa -i chr22_index\n\n\n\n\nQuantify reads using salmon\n\u00b6\n\n\nfor i in *_R1.fastq.gz\ndo\n   prefix=$(basename $i _R1.fastq.gz)\n   salmon quant -i chr22_index --libType A \\\n          -1 ${prefix}_R1.fastq.gz -2 ${prefix}_R2.fastq.gz -o quant/${prefix};\ndone\n\n\n\n\nThis loop simply goes through each sample and invokes salmon using fairly basic options:\n\n\n\n\nThe -i argument tells salmon where to find the index\n\n\n--libType A tells salmon that it should automatically determine the library type of the sequencing reads (e.g. stranded vs. unstranded etc.)\n\n\nThe -1 and -2 arguments tell salmon where to find the left and right reads for this sample (notice, salmon will accept gzipped FASTQ files directly).\n\n\nthe -o argument specifies the directory where salmon\u2019s quantification results sould be written.\n\n\n\n\nSalmon exposes many different options to the user that enable extra features or modify default behavior. However, the purpose and behavior of all of those options is beyond the scope of this introductory tutorial. You can read about salmon\u2019s many options in the \ndocumentation\n.\n\n\nAfter the salmon commands finish running, you should have a directory named \nquant\n, which will have a sub-directory for each sample. These sub-directories contain the quantification results of salmon, as well as a lot of other information salmon records about the sample and the run. The main output file (called quant.sf) is rather self-explanatory. For example, take a peek at the quantification file for sample \nHBR_Rep1\n in \nquant/HBR_Rep1/quant.sf\n and you\u2019ll see a simple TSV format file listing the name (Name) of each transcript, its length (Length), effective length (EffectiveLength) (more details on this in the documentation), and its abundance in terms of Transcripts Per Million (TPM) and estimated number of reads (NumReads) originating from this transcript.\n\n\nImport read counts using tximport\n\u00b6\n\n\nUsing the tximport R package, you can import salmon\u2019s transcript-level quantifications and optionally aggregate them to the gene level for gene-level differential expression analysis.\n\n\nFirst, open up your favourite R IDE and install the necessary packages:\n\n\nsource(\"https://bioconductor.org/biocLite.R\")\nbiocLite(\"tximport\")\nbiocLite(\"GenomicFeatures\")\n\ninstall.packages(\"readr\")\n\n\n\n\nThen load the modules:\n\n\nlibrary(tximport)\nlibrary(GenomicFeatures)\nlibrary(readr)\n\n\n\n\nSalmon did the quantifiation of the transcript level. We want to see which genes are differentially expressed, so we need to link the transcript names to the gene names. We can use our .gtf annotation for that, and the GenomicFeatures package:\n\n\ntxdb <- makeTxDbFromGFF(\"chr22_genes.gtf\")\nk <- keys(txdb, keytype = \"GENEID\")\ntx2gene <- select(txdb, keys = k, keytype = \"GENEID\", columns = \"TXNAME\")\nhead(tx2gene)\n\n\n\n\nNow we can import the salmon quantification. First, download the file with sample descriptions from \nhere\n and put it in the toy_rna directory. Then, use that file to load the corresponding quantification data.\n\n\nsamples <- read.table(\"samples.txt\", header = TRUE)\nfiles <- file.path(\"quant\", samples$sample, \"quant.sf\")\nnames(files) <- paste0(samples$sample)\ntxi.salmon <- tximport(files, type = \"salmon\", tx2gene = tx2gene, reader = read_tsv)\n\n\n\n\nTake a look at the data:\n\n\nhead(txi.salmon$counts)\n\n\n\n\nDifferential expression using DESeq2\n\u00b6\n\n\nInstall the necessary package:\n\n\nbiocLite('DESeq2')\n\n\n\n\nThen load it:\n\n\nlibrary(DESeq2)\n\n\n\n\nInstantiate the DESeqDataSet and generate result table. See \n?DESeqDataSetFromTximport\n and \n?DESeq\n for more information about the steps performed by the program.\n\n\ndds <- DESeqDataSetFromTximport(txi.salmon, samples, ~condition)\ndds <- DESeq(dds)\nres <- results(dds)\n\n\n\n\nRun the \nsummary\n command to get an idea of how many genes are up- and downregulated between the two conditions:\n\n\nsummary(res)\n\n\nDESeq uses a negative binomial distribution. Such distributions have two parameters: mean and dispersion. The dispersion is a parameter describing how much the variance deviates from the mean.\n\n\nYou can read more about the methods used by DESeq2 in the \npaper\n or the \nvignette\n\n\nPlot dispersions:\n\n\nplotDispEsts(dds, main=\"Dispersion plot\")\n\n\n\n\nFor clustering and heatmaps, we need to log transform our data:\n\n\nrld <- rlogTransformation(dds)\nhead(assay(rld))\n\n\n\n\nThen, we create a sample distance heatmap:\n\n\nlibrary(RColorBrewer)\nlibrary(gplots) # you may need to install this package\n\n(mycols <- brewer.pal(8, \"Dark2\")[1:length(unique(samples$condition))])\nsampleDists <- as.matrix(dist(t(assay(rld))))\nheatmap.2(as.matrix(sampleDists), key=F, trace=\"none\",\n          col=colorpanel(100, \"black\", \"white\"),\n          ColSideColors=mycols[samples$condition],\n          RowSideColors=mycols[samples$condition],\n          margin=c(10, 10), main=\"Sample Distance Matrix\")\n\n\n\n\nWe can also plot a PCA:\n\n\nDESeq2::plotPCA(rld, intgroup=\"condition\")\n\n\n\n\nIt is time to look at some p-values:\n\n\ntable(res$padj<0.05)\nres <- res[order(res$padj), ]\nresdata <- merge(as.data.frame(res), as.data.frame(counts(dds, normalized=TRUE)), by=\"row.names\", sort=FALSE)\nnames(resdata)[1] <- \"Gene\"\nhead(resdata)\n\n\n\n\nExamine plot of p-values, the MA plot and the Volcano Plot:\n\n\nhist(res$pvalue, breaks=50, col=\"grey\")\nDESeq2::plotMA(dds, ylim=c(-1,1), cex=1)\n\n# Volcano plot\nwith(res, plot(log2FoldChange, -log10(pvalue), pch=20, main=\"Volcano plot\", xlim=c(-2.5,2)))\nwith(subset(res, padj<.05 ), points(log2FoldChange, -log10(pvalue), pch=20, col=\"red\"))\n\n\n\n\nKEGG pathway analysis\n\u00b6\n\n\nAs always, install and load the necessary packages:\n\n\nbiocLite(\"AnnotationDbi\")\nbiocLite(\"org.Hs.eg.db\")\nbiocLite(\"pathview\")\nbiocLite(\"gage\")\nbiocLite(\"gageData\")\n\nlibrary(AnnotationDbi)\nlibrary(org.Hs.eg.db)\nlibrary(pathview)\nlibrary(gage)\nlibrary(gageData)\n\n\n\n\nLet\u2019s use the \nmapIds\n function to add more columns to the results. The row.names of our results table has the Ensembl gene ID (our key), so we need to specify  \nkeytype=ENSEMBL\n. The column argument tells the \nmapIds\n function which information we want, and the \nmultiVals\n argument tells the function what to do if there are multiple possible values for a single input value. Here we ask to just give us back the first one that occurs in the database. Let\u2019s get the Entrez IDs, gene symbols, and full gene names.\n\n\nres$symbol <- mapIds(org.Hs.eg.db,\n                     keys=row.names(res),\n                     column=\"SYMBOL\",\n                     keytype=\"ENSEMBL\",\n                     multiVals=\"first\")\nres$entrez <- mapIds(org.Hs.eg.db,\n                     keys=row.names(res),\n                     column=\"ENTREZID\",\n                     keytype=\"ENSEMBL\",\n                     multiVals=\"first\")\nres$name <- mapIds(org.Hs.eg.db,\n                     keys=row.names(res),\n                     column=\"GENENAME\",\n                     keytype=\"ENSEMBL\",\n                     multiVals=\"first\")\n\nhead(res)\n\n\n\n\nWe\u2019re going to use the \ngage\n package for pathway analysis, and the \npathview\n package to draw a pathway diagram.\n\n\nThe gageData package has pre-compiled databases mapping genes to KEGG pathways and GO terms for common organisms:\n\n\ndata(kegg.sets.hs)\ndata(sigmet.idx.hs)\nkegg.sets.hs <- kegg.sets.hs[sigmet.idx.hs]\nhead(kegg.sets.hs, 3)\n\n\n\n\nRun the pathway analysis. See help on the gage function with \n?gage\n. Specifically, you might want to try changing the value of same.dir.\n\n\nfoldchanges <- res$log2FoldChange\nnames(foldchanges) <- res$entrez\nkeggres <- gage(foldchanges, gsets=kegg.sets.hs, same.dir=TRUE)\nlapply(keggres, head)\n\n\n\n\nPull out the top 5 upregulated pathways, then further process that just to get the IDs. We\u2019ll use these KEGG pathway IDs downstream for plotting. The \ndplyr\n package is required to use the pipe (\n%>%\n) construct.\n\n\nlibrary(dplyr)\n\n# Get the pathways\nkeggrespathways <- data.frame(id=rownames(keggres$greater), keggres$greater) %>%\n  tbl_df() %>%\n  filter(row_number()<=5) %>%\n  .$id %>%\n  as.character()\nkeggrespathways\n\n# Get the IDs.\nkeggresids <- substr(keggrespathways, start=1, stop=8)\nkeggresids\n\n\n\n\nFinally, the \npathview()\n function in the pathview package makes the plots. Let\u2019s write a function so we can loop through and draw plots for the top 5 pathways we created above.\n\n\n# Define plotting function for applying later\nplot_pathway <- function(pid) pathview(gene.data=foldchanges, pathway.id=pid, species=\"hsa\", new.signature=FALSE)\n\n# Unload dplyr since it conflicts with the next line\ndetach(\"package:dplyr\", unload=T)\n\n# plot multiple pathways (plots saved to disk and returns a throwaway list object)\ntmp <- sapply(keggresids, function(pid) pathview(gene.data=foldchanges, pathway.id=pid, species=\"hsa\"))\n\n\n\n\nThanks\n\u00b6\n\n\nThis material was inspired by Stephen Turner's blog post:\n\n\n\n\nTutorial: RNA-seq differential expression & pathway analysis with Sailfish, DESeq2, GAGE, and Pathview: http://www.gettinggeneticsdone.com/2015/12/tutorial-rna-seq-differential.html",
            "title": "RNA-Seq"
        },
        {
            "location": "/rna/#rna-seq",
            "text": "",
            "title": "RNA-Seq"
        },
        {
            "location": "/rna/#load-salmon",
            "text": "module load Salmon",
            "title": "Load salmon"
        },
        {
            "location": "/rna/#downloading-the-data",
            "text": "For this tutorial we will use the test data from  this  paper:   Malachi Griffith , Jason R. Walker, Nicholas C. Spies, Benjamin J. Ainscough, Obi L. Griffith . 2015. Informatics for RNA-seq: A web resource for analysis on the cloud. PLoS Comp Biol. 11(8):e1004393.   The test data consists of two commercially available RNA samples: Universal Human Reference (UHR) and Human Brain Reference (HBR). The UHR is total RNA isolated from a diverse set of 10 cancer cell lines. The HBR is total RNA isolated from the brains of 23 Caucasians, male and female, of varying age but mostly 60-80 years old.  In addition, a spike-in control was used. Specifically we added an aliquot of the ERCC ExFold RNA Spike-In Control Mixes to each sample. The spike-in consists of 92 transcripts that are present in known concentrations across a wide abundance range (from very few copies to many copies). This range allows us to test the degree to which the RNA-seq assay (including all laboratory and analysis steps) accurately reflects the relative abundance of transcript species within a sample. There are two 'mixes' of these transcripts to allow an assessment of differential expression output between samples if you put one mix in each of your two comparisons. In our case, Mix1 was added to the UHR sample, and Mix2 was added to the HBR sample. We also have 3 complete experimental replicates for each sample. This allows us to assess the technical variability of our overall process of producing RNA-seq data in the lab.  For all libraries we prepared low-throughput (Set A) TruSeq Stranded Total RNA Sample Prep Kit libraries with Ribo-Zero Gold to remove both cytoplasmic and mitochondrial rRNA. Triplicate, indexed libraries were made starting with 100ng Agilent/Strategene Universal Human Reference total RNA and 100ng Ambion Human Brain Reference total RNA. The Universal Human Reference replicates received 2 ul of 1:1000 ERCC Mix 1. The Human Brain Reference replicates received 1:1000 ERCC Mix 2. The libraries were quantified with KAPA Library Quantification qPCR and adjusted to the appropriate concentration for sequencing. The triplicate, indexed libraries were then pooled prior to sequencing. Each pool of three replicate libraries were sequenced across 2 lanes of a HiSeq 2000 using paired-end sequence chemistry with 100bp read lengths.  So to summarize we have:   UHR + ERCC Spike-In Mix1, Replicate 1  UHR + ERCC Spike-In Mix1, Replicate 2  UHR + ERCC Spike-In Mix1, Replicate 3  HBR + ERCC Spike-In Mix2, Replicate 1  HBR + ERCC Spike-In Mix2, Replicate 2  HBR + ERCC Spike-In Mix2, Replicate 3   You can download the data from  here .  Unpack the data and go into the toy_rna directory  tar xzf toy_rna.tar.gz\ncd toy_rna",
            "title": "Downloading the data"
        },
        {
            "location": "/rna/#indexing-transcriptome",
            "text": "salmon index -t chr22_transcripts.fa -i chr22_index",
            "title": "Indexing transcriptome"
        },
        {
            "location": "/rna/#quantify-reads-using-salmon",
            "text": "for i in *_R1.fastq.gz\ndo\n   prefix=$(basename $i _R1.fastq.gz)\n   salmon quant -i chr22_index --libType A \\\n          -1 ${prefix}_R1.fastq.gz -2 ${prefix}_R2.fastq.gz -o quant/${prefix};\ndone  This loop simply goes through each sample and invokes salmon using fairly basic options:   The -i argument tells salmon where to find the index  --libType A tells salmon that it should automatically determine the library type of the sequencing reads (e.g. stranded vs. unstranded etc.)  The -1 and -2 arguments tell salmon where to find the left and right reads for this sample (notice, salmon will accept gzipped FASTQ files directly).  the -o argument specifies the directory where salmon\u2019s quantification results sould be written.   Salmon exposes many different options to the user that enable extra features or modify default behavior. However, the purpose and behavior of all of those options is beyond the scope of this introductory tutorial. You can read about salmon\u2019s many options in the  documentation .  After the salmon commands finish running, you should have a directory named  quant , which will have a sub-directory for each sample. These sub-directories contain the quantification results of salmon, as well as a lot of other information salmon records about the sample and the run. The main output file (called quant.sf) is rather self-explanatory. For example, take a peek at the quantification file for sample  HBR_Rep1  in  quant/HBR_Rep1/quant.sf  and you\u2019ll see a simple TSV format file listing the name (Name) of each transcript, its length (Length), effective length (EffectiveLength) (more details on this in the documentation), and its abundance in terms of Transcripts Per Million (TPM) and estimated number of reads (NumReads) originating from this transcript.",
            "title": "Quantify reads using salmon"
        },
        {
            "location": "/rna/#import-read-counts-using-tximport",
            "text": "Using the tximport R package, you can import salmon\u2019s transcript-level quantifications and optionally aggregate them to the gene level for gene-level differential expression analysis.  First, open up your favourite R IDE and install the necessary packages:  source(\"https://bioconductor.org/biocLite.R\")\nbiocLite(\"tximport\")\nbiocLite(\"GenomicFeatures\")\n\ninstall.packages(\"readr\")  Then load the modules:  library(tximport)\nlibrary(GenomicFeatures)\nlibrary(readr)  Salmon did the quantifiation of the transcript level. We want to see which genes are differentially expressed, so we need to link the transcript names to the gene names. We can use our .gtf annotation for that, and the GenomicFeatures package:  txdb <- makeTxDbFromGFF(\"chr22_genes.gtf\")\nk <- keys(txdb, keytype = \"GENEID\")\ntx2gene <- select(txdb, keys = k, keytype = \"GENEID\", columns = \"TXNAME\")\nhead(tx2gene)  Now we can import the salmon quantification. First, download the file with sample descriptions from  here  and put it in the toy_rna directory. Then, use that file to load the corresponding quantification data.  samples <- read.table(\"samples.txt\", header = TRUE)\nfiles <- file.path(\"quant\", samples$sample, \"quant.sf\")\nnames(files) <- paste0(samples$sample)\ntxi.salmon <- tximport(files, type = \"salmon\", tx2gene = tx2gene, reader = read_tsv)  Take a look at the data:  head(txi.salmon$counts)",
            "title": "Import read counts using tximport"
        },
        {
            "location": "/rna/#differential-expression-using-deseq2",
            "text": "Install the necessary package:  biocLite('DESeq2')  Then load it:  library(DESeq2)  Instantiate the DESeqDataSet and generate result table. See  ?DESeqDataSetFromTximport  and  ?DESeq  for more information about the steps performed by the program.  dds <- DESeqDataSetFromTximport(txi.salmon, samples, ~condition)\ndds <- DESeq(dds)\nres <- results(dds)  Run the  summary  command to get an idea of how many genes are up- and downregulated between the two conditions:  summary(res)  DESeq uses a negative binomial distribution. Such distributions have two parameters: mean and dispersion. The dispersion is a parameter describing how much the variance deviates from the mean.  You can read more about the methods used by DESeq2 in the  paper  or the  vignette  Plot dispersions:  plotDispEsts(dds, main=\"Dispersion plot\")  For clustering and heatmaps, we need to log transform our data:  rld <- rlogTransformation(dds)\nhead(assay(rld))  Then, we create a sample distance heatmap:  library(RColorBrewer)\nlibrary(gplots) # you may need to install this package\n\n(mycols <- brewer.pal(8, \"Dark2\")[1:length(unique(samples$condition))])\nsampleDists <- as.matrix(dist(t(assay(rld))))\nheatmap.2(as.matrix(sampleDists), key=F, trace=\"none\",\n          col=colorpanel(100, \"black\", \"white\"),\n          ColSideColors=mycols[samples$condition],\n          RowSideColors=mycols[samples$condition],\n          margin=c(10, 10), main=\"Sample Distance Matrix\")  We can also plot a PCA:  DESeq2::plotPCA(rld, intgroup=\"condition\")  It is time to look at some p-values:  table(res$padj<0.05)\nres <- res[order(res$padj), ]\nresdata <- merge(as.data.frame(res), as.data.frame(counts(dds, normalized=TRUE)), by=\"row.names\", sort=FALSE)\nnames(resdata)[1] <- \"Gene\"\nhead(resdata)  Examine plot of p-values, the MA plot and the Volcano Plot:  hist(res$pvalue, breaks=50, col=\"grey\")\nDESeq2::plotMA(dds, ylim=c(-1,1), cex=1)\n\n# Volcano plot\nwith(res, plot(log2FoldChange, -log10(pvalue), pch=20, main=\"Volcano plot\", xlim=c(-2.5,2)))\nwith(subset(res, padj<.05 ), points(log2FoldChange, -log10(pvalue), pch=20, col=\"red\"))",
            "title": "Differential expression using DESeq2"
        },
        {
            "location": "/rna/#kegg-pathway-analysis",
            "text": "As always, install and load the necessary packages:  biocLite(\"AnnotationDbi\")\nbiocLite(\"org.Hs.eg.db\")\nbiocLite(\"pathview\")\nbiocLite(\"gage\")\nbiocLite(\"gageData\")\n\nlibrary(AnnotationDbi)\nlibrary(org.Hs.eg.db)\nlibrary(pathview)\nlibrary(gage)\nlibrary(gageData)  Let\u2019s use the  mapIds  function to add more columns to the results. The row.names of our results table has the Ensembl gene ID (our key), so we need to specify   keytype=ENSEMBL . The column argument tells the  mapIds  function which information we want, and the  multiVals  argument tells the function what to do if there are multiple possible values for a single input value. Here we ask to just give us back the first one that occurs in the database. Let\u2019s get the Entrez IDs, gene symbols, and full gene names.  res$symbol <- mapIds(org.Hs.eg.db,\n                     keys=row.names(res),\n                     column=\"SYMBOL\",\n                     keytype=\"ENSEMBL\",\n                     multiVals=\"first\")\nres$entrez <- mapIds(org.Hs.eg.db,\n                     keys=row.names(res),\n                     column=\"ENTREZID\",\n                     keytype=\"ENSEMBL\",\n                     multiVals=\"first\")\nres$name <- mapIds(org.Hs.eg.db,\n                     keys=row.names(res),\n                     column=\"GENENAME\",\n                     keytype=\"ENSEMBL\",\n                     multiVals=\"first\")\n\nhead(res)  We\u2019re going to use the  gage  package for pathway analysis, and the  pathview  package to draw a pathway diagram.  The gageData package has pre-compiled databases mapping genes to KEGG pathways and GO terms for common organisms:  data(kegg.sets.hs)\ndata(sigmet.idx.hs)\nkegg.sets.hs <- kegg.sets.hs[sigmet.idx.hs]\nhead(kegg.sets.hs, 3)  Run the pathway analysis. See help on the gage function with  ?gage . Specifically, you might want to try changing the value of same.dir.  foldchanges <- res$log2FoldChange\nnames(foldchanges) <- res$entrez\nkeggres <- gage(foldchanges, gsets=kegg.sets.hs, same.dir=TRUE)\nlapply(keggres, head)  Pull out the top 5 upregulated pathways, then further process that just to get the IDs. We\u2019ll use these KEGG pathway IDs downstream for plotting. The  dplyr  package is required to use the pipe ( %>% ) construct.  library(dplyr)\n\n# Get the pathways\nkeggrespathways <- data.frame(id=rownames(keggres$greater), keggres$greater) %>%\n  tbl_df() %>%\n  filter(row_number()<=5) %>%\n  .$id %>%\n  as.character()\nkeggrespathways\n\n# Get the IDs.\nkeggresids <- substr(keggrespathways, start=1, stop=8)\nkeggresids  Finally, the  pathview()  function in the pathview package makes the plots. Let\u2019s write a function so we can loop through and draw plots for the top 5 pathways we created above.  # Define plotting function for applying later\nplot_pathway <- function(pid) pathview(gene.data=foldchanges, pathway.id=pid, species=\"hsa\", new.signature=FALSE)\n\n# Unload dplyr since it conflicts with the next line\ndetach(\"package:dplyr\", unload=T)\n\n# plot multiple pathways (plots saved to disk and returns a throwaway list object)\ntmp <- sapply(keggresids, function(pid) pathview(gene.data=foldchanges, pathway.id=pid, species=\"hsa\"))",
            "title": "KEGG pathway analysis"
        },
        {
            "location": "/rna/#thanks",
            "text": "This material was inspired by Stephen Turner's blog post:   Tutorial: RNA-seq differential expression & pathway analysis with Sailfish, DESeq2, GAGE, and Pathview: http://www.gettinggeneticsdone.com/2015/12/tutorial-rna-seq-differential.html",
            "title": "Thanks"
        }
    ]
}